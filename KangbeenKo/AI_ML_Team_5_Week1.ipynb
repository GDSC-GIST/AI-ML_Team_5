{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-ML_Team_5_Week1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VxO0rE1eQFy"
      },
      "source": [
        "# Natural Language Processing: Week 1 - Sentiment in text\n",
        "\n",
        "NLP는 어려운 문제 중 하나이다. 수치형 형태의 픽셀 이미지를 인풋으로 받는 이미지 처리와 달리, 자연어 처리는 인풋이 수치형이 아님은 물론 문장의 길이, 순서 등 고려해야 할 것이 많다.\n",
        "\n",
        "텍스트 처리 방법을 배우고, 감성분석 모델을 만들어 레이블이 달린 텍스트에 대한 Sentiment Analysis를 수행해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exs0k05yeXbi"
      },
      "source": [
        "## Character based encodings\n",
        "먼저, 우리는 문장 집합 내 각 글자에 대한 Character Encoding을 생각해 볼 수 있다. 이러한 방법에는 대표적으로 ASCII 코드가 있는데, 여기서 각 알파벳에는 숫자가 대응된다.\n",
        "\n",
        "다만 이러한 방식을 사용했을 때, LISTEN과 SILENT는 서로 같은 문자들의 조합으로 이루어져있기 때문에 뜻이 완전히 다름에도 불구하고 비슷한 값으로 Encoding 될 우려가 있다.\n",
        "\n",
        "따라서 Neural Network의 입력으로 사용하기에는 부적합하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsc4JtIlmkDx"
      },
      "source": [
        "## Word based encodings\n",
        "그렇다면 단어 기반의 Word based Encoding은 어떨까?\n",
        "\n",
        "I love my dog 이란 문장이 있다고 가정하자. 이 문장을 단어에 기반하여 임의로 인코딩을 수행하면 다음과 같을 것이다.\n",
        "\n",
        "$\\{I:001,LOVE:002, MY:003, DOG:004\\}$\n",
        "\n",
        "그럼 I love my cat 이란 문장은 어떻게 인코딩 될까? 단어 기반 인코딩에서, 같은 단어는 항상 같은 숫자값으로 인코딩 되므로, 해당 문장의 인코딩 결과는 다음과 같을 것이다.\n",
        "\n",
        "$\\{I:001, LOVE:002, MY:003, CAT:005\\}$\n",
        "\n",
        "Cat은 I, Love, My와 달리 첫 번째 문장에서는 찾을 수 없는 단어이므로 첫 번째 문장을 구성하는 4개의 단어 중 어느 것과도 겹치지 않는 수로 인코딩 되어야 한다. 따라서, 005로 인코딩 할 수 있다.\n",
        "\n",
        "이와 같은 단어 기반 인코딩을 적용하는 동시에, 해당 문장을 이루는 Sequence를 고려한다면, 두 문장 간의 유사도를 구하는 등의 계산을 수행할 수 있다.\n",
        "\n",
        "단어 인코딩은 TensorFlow에서 제공하는 `Tokenizer`로 수행할 수 있으므로, 해당 API를 이용해서 진행해보도록 하자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6VU1Hroehiz"
      },
      "source": [
        "## Using APIs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ycoi8X3dd6F1",
        "outputId": "e774a187-c146-43d9-9cbf-cdb398e78aa4"
      },
      "source": [
        "# Import Dependencies\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Example Sentences\n",
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "             'You love my dog!'\n",
        "]\n",
        "\n",
        "# Call Tokenizer & Create Instance\n",
        "tokenizer = Tokenizer(num_words = 100) # 가장 자주 사용되는 100-1개의 단어를 고려\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer.fit_on_texts(sentences) # 문자 데이터를 입력받아 인코딩을 수행한 후 리스트 형태로 반환하는 메서드\n",
        "\n",
        "# View word index\n",
        "word_index = tokenizer.word_index # 단어:토큰(숫자) 쌍 딕셔너리 반환 - 단어는 모두 소문자로 변환되며 구두점은 인코딩에 영향을 주지 않는다\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIGuWcSBoXRP"
      },
      "source": [
        "## Text to sequence\n",
        "\n",
        "Tokenizer를 이용해 문장을 토큰화해봤다.\n",
        "\n",
        "하지만 이러한 방식만으로는 부족한 점이 있다. 바로 문장의 순서이다. 문장은 단어로 이루어져있지만, 그것만으로는 부족하다. 위 예시에서 사용한 'I love my dog'은 말이 되지만, 'Love I dog my'는 말이 되지 않는다.\n",
        "\n",
        "문장에는 단어의 순서가 있기 때문인데, 이를 우리는 Sequence라고 한다.\n",
        "\n",
        "`texts_to_sequences()`를 이용해서 단어들을 시퀀스 형태로 변환해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj8Fn2D6kYaJ",
        "outputId": "c8f31de0-1ac9-49d4-cab7-bd1b423c560b"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'You love my dog!',\n",
        "  'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Sentences to Sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # sentence를 token으로 이루어진 list 형태의 sequence로 변환 - 각 list를 원소로 갖는 list 형성\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiKJ-zfruMmn"
      },
      "source": [
        "위 경우, fit에 사용된 sentences 내 포함되지 않은 단어를 가진 새로운 문장이 주어졌을 때, 해당 단어는 Sequence로의 변환 과정에서 제외된다. 이를 해결할 수 있는 방법에는 두 가지가 있다.\n",
        "\n",
        "첫 번째는 굉장히 많은 문장을 학습에 사용하는 것이고, 두 번째 방법은 **Out of Vocabulary (OOV)**를 사용하는 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIZCXLgWvzFD"
      },
      "source": [
        "## Looking more at the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_KNGQjypyXt",
        "outputId": "246f8bfa-221e-4c2b-befd-80c237a9750e"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "  'I love my dog',\n",
        "  'I love my cat',\n",
        "  'You love my dog!',\n",
        "  'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token='<OOV>') # OOV Token을 추가 - 1번 인덱스 / 반드시 <OOV>로 사용할 필요는 없지만, 실제 단어와 겹치지 않을 unique한 값으로 설정하는 것이 중요\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "test_sentences = [\n",
        "                  'I really love my dog',\n",
        "                  'My dog loves my manatee'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_sentences)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEqEBJWywxmH"
      },
      "source": [
        "이미지 처리에서, 우리는 다양한 사이즈의 인풋 이미지를 하나의 사이즈로 맞춰주는 과정이 필요했다. 이것은 NLP에서도 마찬가지이며, 이를 가능하게 하는 것이 padding과 truncating이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mdv5AX0xBFf"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARLk2xs-wXob",
        "outputId": "cdbada1c-0673-4ae9-ce6f-9adca70e21d4"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "print(sequences)\n",
        "\n",
        "# Padding\n",
        "padded = pad_sequences(sequences) # maxlen 파라미터로 sequence의 길이를 설정해 줄 수 있다. - default값은 sentences 중 최대 길이\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vJ8ha9Ex2rL"
      },
      "source": [
        "만약 sentences 내 특정 sentence의 길이보다 작은 maxlen을 설정한다면, sentence의 일부는 잘리게 되고, 이것을 truncating이라고 부른다.\n",
        "\n",
        "또한 padding/truncating 파라미터를 이용해 해당 과정의 시작 위치를 정할 수 있으며, defult 값은 pre(<-> post)이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v2Tt0e1xl_Y",
        "outputId": "e3f5d174-d7d6-454a-c084-511ca91cf40c"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', maxlen=5)\n",
        "print(padded) # 첫 번째, 두 번째 문장의 뒤에 padding이 적용되며, 세 번째 문장의 앞 쪽이 truncating 된다"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5  3  2  4  0]\n",
            " [ 5  3  2  7  0]\n",
            " [ 6  3  2  4  0]\n",
            " [ 9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t2-K9Sg5iza"
      },
      "source": [
        "## Sarcasm, really?\n",
        "\n",
        "- [Sarcasm in News Headlines Dataset by Rishabh Misra](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KEEJghe3g85",
        "outputId": "68d56ad8-d0c1-4c78-b52e-a04adb3f2b20"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "bXDgFomk3g3h",
        "outputId": "1358ce61-f507-450d-88d8-46ee4e826634"
      },
      "source": [
        "# kaggle.com > Profile: Account > API > Create New Token > kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5eaf45e9-952b-427e-9f3d-6324b0744b53\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5eaf45e9-952b-427e-9f3d-6324b0744b53\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"kangbeenko\",\"key\":\"72505225fa15a6a66bbdb1c061521ca1\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0UBr5GK3gyg"
      },
      "source": [
        "# Create folder name '.kaggle'\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Copy kaggle.json into .kaggle folder\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWlJ3qp844l_",
        "outputId": "44148a98-7b8f-4778-cfd7-0412a74690fe"
      },
      "source": [
        "# Search for the Kaggle Datasets\n",
        "!kaggle datasets list -s sarcasm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                     title                                           size  lastUpdated          downloadCount  \n",
            "------------------------------------------------------  ---------------------------------------------  -----  -------------------  -------------  \n",
            "danofer/sarcasm                                         Sarcasm on Reddit                              216MB  2018-05-27 08:19:04           6906  \n",
            "rmisra/news-headlines-dataset-for-sarcasm-detection     News Headlines Dataset For Sarcasm Detection     3MB  2019-07-03 23:52:57          27278  \n",
            "saurabhbagchi/sarcasm-detection-through-nlp             Sarcasm detection                                2MB  2021-03-20 08:31:15            153  \n",
            "nikhiljohnk/tweets-with-sarcasm-and-irony               Tweets with Sarcasm and Irony                    4MB  2020-11-04 10:14:04            425  \n",
            "rmisra/news-category-dataset                            News Category Dataset                           25MB  2018-12-02 04:09:45          25614  \n",
            "sherinclaudia/sarcastic-comments-on-reddit              Sarcastic Comments - REDDIT                    106MB  2019-01-30 14:41:53           2265  \n",
            "mikahama/the-best-sarcasm-annotated-dataset-in-spanish  The Best Sarcasm Annotated Dataset in Spanish   26KB  2020-06-21 16:50:44            191  \n",
            "khotijahs1/data-train                                   English Sarcasm                                190KB  2020-11-29 02:02:05             74  \n",
            "rmisra/clothing-fit-dataset-for-size-recommendation     Clothing Fit Dataset for Size Recommendation    40MB  2018-08-21 19:00:16           5376  \n",
            "rmisra/imdb-spoiler-dataset                             IMDB Spoiler Dataset                           331MB  2019-05-22 04:44:43           1622  \n",
            "kashnitsky/mlcourse                                     mlcourse.ai                                     51MB  2018-12-09 16:45:09          26723  \n",
            "gpreda/i-dont-work-here-lady                            I Don't Work Here Lady                           1MB  2021-10-01 17:35:55             14  \n",
            "wenxindong/sarcasm                                      sarcasm                                         26MB  2020-11-06 05:09:28             43  \n",
            "harrotuin/dutch-news-headlines                          Dutch news headlines for sarcasm detection     751KB  2020-12-14 09:07:44             24  \n",
            "sachinichake/detect-sarcasm-in-comments                 Detect Sarcasm in Comments                       2MB  2020-08-05 08:25:37             47  \n",
            "negiaditya/sarcasm                                      sarcasm                                          2MB  2020-05-18 20:17:56             28  \n",
            "harriken/bias-media-cat                                 Bias Media CAT                                  17MB  2017-10-27 22:46:25            373  \n",
            "akshit3050/sarcasm                                      sarcasm                                         26MB  2018-08-04 04:44:21            130  \n",
            "rmsharks4/sarcasmania-dataset                           Sarcasmania Dataset                              2MB  2019-05-28 09:07:23            105  \n",
            "williamscott701/memotion-dataset-7k                     Memotion Dataset 7k                            695MB  2020-02-27 13:06:01           1337  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-htLsgBN5FEH"
      },
      "source": [
        "We will use the second dataset, **'rmisra/news-headlines-dataset-for-sarcasm-detection'**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW2nIXo54YOH",
        "outputId": "e22ca77a-2abe-43fd-f9b4-72ce4c49e049"
      },
      "source": [
        "# Kaggle Dataset you want > Three dots right of the 'New Notebook' > Copy API Command\n",
        "!kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading news-headlines-dataset-for-sarcasm-detection.zip to /content\n",
            "\r  0% 0.00/3.30M [00:00<?, ?B/s]\n",
            "\r100% 3.30M/3.30M [00:00<00:00, 110MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRX2Iwzw5nal",
        "outputId": "032f1242-3436-4be6-fdbe-34e0989b070f"
      },
      "source": [
        "# Unzip the files\n",
        "!unzip news-headlines-dataset-for-sarcasm-detection.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  news-headlines-dataset-for-sarcasm-detection.zip\n",
            "  inflating: Sarcasm_Headlines_Dataset.json  \n",
            "  inflating: Sarcasm_Headlines_Dataset_v2.json  \n",
            "kaggle.json\n",
            "news-headlines-dataset-for-sarcasm-detection.zip\n",
            "sample_data\n",
            "Sarcasm_Headlines_Dataset.json\n",
            "Sarcasm_Headlines_Dataset_v2.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StWjQ3Qp9izZ",
        "outputId": "fd00f1ee-cf01-43d2-cd28-a60e50224c5e"
      },
      "source": [
        "!head Sarcasm_Headlines_Dataset_v2.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"is_sarcastic\": 1, \"headline\": \"thirtysomething scientists unveil doomsday clock of hair loss\", \"article_link\": \"https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205\"}\r\n",
            "{\"is_sarcastic\": 0, \"headline\": \"dem rep. totally nails why congress is falling short on gender, racial equality\", \"article_link\": \"https://www.huffingtonpost.com/entry/donna-edwards-inequality_us_57455f7fe4b055bb1170b207\"}\r\n",
            "{\"is_sarcastic\": 0, \"headline\": \"eat your veggies: 9 deliciously different recipes\", \"article_link\": \"https://www.huffingtonpost.com/entry/eat-your-veggies-9-delici_b_8899742.html\"}\r\n",
            "{\"is_sarcastic\": 1, \"headline\": \"inclement weather prevents liar from getting to work\", \"article_link\": \"https://local.theonion.com/inclement-weather-prevents-liar-from-getting-to-work-1819576031\"}\r\n",
            "{\"is_sarcastic\": 1, \"headline\": \"mother comes pretty close to using word 'streaming' correctly\", \"article_link\": \"https://www.theonion.com/mother-comes-pretty-close-to-using-word-streaming-cor-1819575546\"}\r\n",
            "{\"is_sarcastic\": 0, \"headline\": \"my white inheritance\", \"article_link\": \"https://www.huffingtonpost.com/entry/my-white-inheritance_us_59230747e4b07617ae4cbe1a\"}\r\n",
            "{\"is_sarcastic\": 0, \"headline\": \"5 ways to file your taxes with less stress\", \"article_link\": \"https://www.huffingtonpost.com/entry/5-ways-to-file-your-taxes_b_6957316.html\"}\r\n",
            "{\"is_sarcastic\": 1, \"headline\": \"richard branson's global-warming donation nearly as much as cost of failed balloon trips\", \"article_link\": \"https://www.theonion.com/richard-bransons-global-warming-donation-nearly-as-much-1819568749\"}\r\n",
            "{\"is_sarcastic\": 1, \"headline\": \"shadow government getting too large to meet in marriott conference room b\", \"article_link\": \"https://politics.theonion.com/shadow-government-getting-too-large-to-meet-in-marriott-1819570731\"}\r\n",
            "{\"is_sarcastic\": 0, \"headline\": \"lots of parents know this scenario\", \"article_link\": \"https://www.huffingtonpost.comhttp://pubx.co/6IXxhm\"}\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6-PnfX0ASlP"
      },
      "source": [
        "This JSON File is not seperated by comma. So the Error has occured in `json.load()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yn_ADGsAJRh",
        "outputId": "ad7b6226-636e-4ab1-c82d-c8de29e1d1bf"
      },
      "source": [
        "!gdown --id 1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n",
            "To: /content/sarcasm.json\n",
            "\r  0% 0.00/5.64M [00:00<?, ?B/s]\r100% 5.64M/5.64M [00:00<00:00, 88.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9fSHzBcAOGa",
        "outputId": "8c7a33ef-3122-4145-c246-44179b592e63"
      },
      "source": [
        "!head sarcasm.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\r\n",
            "{\"article_link\": \"https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5\", \"headline\": \"former versace store clerk sues over secret 'black code' for minority shoppers\", \"is_sarcastic\": 0},\r\n",
            "{\"article_link\": \"https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365\", \"headline\": \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\", \"is_sarcastic\": 0},\r\n",
            "{\"article_link\": \"https://local.theonion.com/mom-starting-to-fear-son-s-web-series-closest-thing-she-1819576697\", \"headline\": \"mom starting to fear son's web series closest thing she will have to grandchild\", \"is_sarcastic\": 1},\r\n",
            "{\"article_link\": \"https://politics.theonion.com/boehner-just-wants-wife-to-listen-not-come-up-with-alt-1819574302\", \"headline\": \"boehner just wants wife to listen, not come up with alternative debt-reduction ideas\", \"is_sarcastic\": 1},\r\n",
            "{\"article_link\": \"https://www.huffingtonpost.com/entry/jk-rowling-wishes-snape-happy-birthday_us_569117c4e4b0cad15e64fdcb\", \"headline\": \"j.k. rowling wishes snape happy birthday in the most magical way\", \"is_sarcastic\": 0},\r\n",
            "{\"article_link\": \"https://www.huffingtonpost.com/entry/advancing-the-worlds-women_b_6810038.html\", \"headline\": \"advancing the world's women\", \"is_sarcastic\": 0},\r\n",
            "{\"article_link\": \"https://www.huffingtonpost.com/entry/how-meat-is-grown-in-a-lab_us_561d1189e4b0c5a1ce607e86\", \"headline\": \"the fascinating case for eating lab-grown meat\", \"is_sarcastic\": 0},\r\n",
            "{\"article_link\": \"https://www.huffingtonpost.com/entry/boxed-college-tuition-ben_n_7445644.html\", \"headline\": \"this ceo will send your kids to school, if you work for his company\", \"is_sarcastic\": 0},\r\n",
            "{\"article_link\": \"https://politics.theonion.com/top-snake-handler-leaves-sinking-huckabee-campaign-1819578231\", \"headline\": \"top snake handler leaves sinking huckabee campaign\", \"is_sarcastic\": 1},\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyFb9-kR40M-"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('sarcasm.json', 'r') as f:\n",
        "  datastore = json.load(f)\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "urls = []\n",
        "\n",
        "for item in datastore:\n",
        "  sentences.append(item['headline'])\n",
        "  labels.append(item['is_sarcastic'])\n",
        "  urls.append(item['article_link'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddQ2dS8aA8lj"
      },
      "source": [
        "Sneak-peak of the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGnRok4YA78j",
        "outputId": "3f18c77e-0023-4339-d95a-3979c2d8ca4d"
      },
      "source": [
        "print(sentences[0:2])\n",
        "print(labels[0:2])\n",
        "print(urls[0:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"former versace store clerk sues over secret 'black code' for minority shoppers\", \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\"]\n",
            "[0, 0]\n",
            "['https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5', 'https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLEFNtYe6USr",
        "outputId": "2480387b-1bfc-4710-9f22-da9e6cdd458b"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "\n",
        "print(sentences[0])\n",
        "print(padded[0])\n",
        "print(labels[0])\n",
        "print(padded.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "former versace store clerk sues over secret 'black code' for minority shoppers\n",
            "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "0\n",
            "(26709, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9nmF9IQDCrd"
      },
      "source": [
        "Week 1 Quiz\n",
        "총점 8점 / 결과 8점\n",
        "\n",
        "###1. What is the name of the object used to tokenize sentences?\n",
        "- Tokenizer\n",
        "- WordTokenizer\n",
        "- TextTokenizer\n",
        "- CharacterTokenizer\n",
        "\n",
        "\n",
        "###2. What is the name of the method used to tokenize a list of sentences?\n",
        "- fit_to_text(sentences)\n",
        "- tokenize_on_text(sentences)\n",
        "- fit_on_texts(sentences)\n",
        "- tokenize(sentences)\n",
        "\n",
        "\n",
        "\n",
        "###3. Once you have the corpus tokenized, what’s the method used to encode a list of sentences to use those tokens?\n",
        "- texts_to_tokens(sentences)\n",
        "- text_to_sequences(sentences)\n",
        "- texts_to_sequences(sentences)\n",
        "- text_to_tokens(sentences)\n",
        "\n",
        "\n",
        "\n",
        "###4. When initializing the tokenizer, how to you specify a token to use for unknown words?\n",
        "- unknown_token=<Token>\n",
        "- oov_token=<Token>\n",
        "- unknown_word=<Token>\n",
        "- out_of_vocab=<Token>\n",
        "\n",
        "\n",
        "\n",
        "###5. If you don’t use a token for out of vocabulary words, what happens at encoding?\n",
        "- The word isn’t encoded, and is replaced by a zero in the sequence\n",
        "- The word isn’t encoded, and the sequencing ends\n",
        "- The word is replaced by the most common token\n",
        "- The word isn’t encoded, and is skipped in the sequence\n",
        "\n",
        "\n",
        "\n",
        "###6. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?\n",
        "- Make sure that they are all the same length using the pad_sequences method of the tokenizer\n",
        "- Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace\n",
        "- Process them on the input layer of the Neural Netword using the pad_sequences property\n",
        "- Specify the input layer of the Neural Network to expect different sizes with dynamic_length\n",
        "\n",
        "\n",
        "\n",
        "###7. If you have a number of sequences of different length, and call pad_sequences on them, what’s the default result?\n",
        "- They’ll get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones\n",
        "- They’ll get cropped to the length of the shortest sequence\n",
        "- Nothing, they’ll remain unchanged\n",
        "- They’ll get padded to the length of the longest sequence by adding zeros to the end of shorter ones\n",
        "\n",
        "\n",
        "###8. When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?\n",
        "- Call the padding method of the pad_sequences object, passing it ‘after’\n",
        "- Pass padding=’post’ to pad_sequences when initializing it\n",
        "- Pass padding=’after’ to pad_sequences when initializing it\n",
        "- Call the padding method of the pad_sequences object, passing it ‘post’"
      ]
    }
  ]
}