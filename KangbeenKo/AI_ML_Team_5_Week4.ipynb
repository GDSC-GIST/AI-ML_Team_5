{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI-ML_Team_5_Week4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIe26yXdGw25"
      },
      "source": [
        "# Natural Language Processing: Week 4 - Sequence models and literature\n",
        "\n",
        "앞서 배운 Sequence Model을 이용하여 새로운 text를 생성하는 것을 생각해보자.\n",
        "\n",
        "이는 굉장히 어렵고 새로운 것처럼 보이지만, 사실 이에 필요한 모든 것을 우리는 이미 배웠다.\n",
        "\n",
        "새로운 것을 생성하는 것이 아니라, '다음에 올 단어를 예측'하는 예측 문제라고 생각해보자. 전체 phrase로부터 단어들을 추출해 dataset을 형성한 후 X라는 만들어진 phrase와 그 뒤에 올 Y를 예측해나가면 된다.\n",
        "\n",
        "많은 단어들을 이용하여 신경망을 학습시킨다면 꽤나 복잡한 문장 또한 생성할 수 있을 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsRdEmrZr1Vw"
      },
      "source": [
        "## Lab01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJt8zChnJzL8"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeKgOUdxGtYa"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNEP3G0_J5HI"
      },
      "source": [
        "### Tokenizer and data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gz6JAJZJ27Y"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\nBattered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXYDlKb6J_Td",
        "outputId": "0ef0f6b9-bc23-4891-c90c-9c1425a8cbc2"
      },
      "source": [
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
            "263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STQkoHGeJ__q"
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_XfPYSfKStB",
        "outputId": "93352bd7-ecbe-4d5f-f58a-98296263c8c3"
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LAAZRv_KUX1",
        "outputId": "00ba9c92-acc7-45aa-b316-794ac1a52b29"
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsHP6aNXKW5Z",
        "outputId": "64638e85-2306-4cdc-d9ca-8251cc831c60"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsIuQFL4KZ7F",
        "outputId": "763d4f01-8af3-4684-be93-d60753f33c09"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(20))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=500, verbose=1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "15/15 [==============================] - 7s 8ms/step - loss: 5.5686 - accuracy: 0.0155\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 5.5481 - accuracy: 0.0530\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.5006 - accuracy: 0.0309\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.3891 - accuracy: 0.0199\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.2637 - accuracy: 0.0199\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.1751 - accuracy: 0.0486\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.1195 - accuracy: 0.0486\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.0792 - accuracy: 0.0486\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.0504 - accuracy: 0.0486\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 5.0201 - accuracy: 0.0486\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.9972 - accuracy: 0.0508\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.9701 - accuracy: 0.0662\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.9433 - accuracy: 0.0618\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.9179 - accuracy: 0.0574\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.8915 - accuracy: 0.0662\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.8648 - accuracy: 0.0706\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.8410 - accuracy: 0.0640\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.8160 - accuracy: 0.0684\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.7902 - accuracy: 0.0773\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.7699 - accuracy: 0.0795\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.7460 - accuracy: 0.0795\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.7236 - accuracy: 0.0773\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.7000 - accuracy: 0.0773\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.6769 - accuracy: 0.0795\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.6557 - accuracy: 0.0795\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.6320 - accuracy: 0.0751\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.6125 - accuracy: 0.0817\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.5860 - accuracy: 0.0817\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.5654 - accuracy: 0.0883\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.5418 - accuracy: 0.0883\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.5215 - accuracy: 0.0817\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.4985 - accuracy: 0.0773\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.4770 - accuracy: 0.0795\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.4541 - accuracy: 0.0861\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.4313 - accuracy: 0.0861\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.4076 - accuracy: 0.0883\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.3851 - accuracy: 0.0883\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.3592 - accuracy: 0.0883\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.3379 - accuracy: 0.0817\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.3167 - accuracy: 0.0949\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.2910 - accuracy: 0.0949\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.2648 - accuracy: 0.1038\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.2408 - accuracy: 0.1104\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.2166 - accuracy: 0.1104\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.1903 - accuracy: 0.1192\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.1667 - accuracy: 0.1236\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.1444 - accuracy: 0.1192\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.1173 - accuracy: 0.1302\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.0931 - accuracy: 0.1325\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.0658 - accuracy: 0.1479\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.0395 - accuracy: 0.1391\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.0128 - accuracy: 0.1545\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.9874 - accuracy: 0.1545\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.9641 - accuracy: 0.1567\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.9365 - accuracy: 0.1678\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.9109 - accuracy: 0.1766\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 3.8846 - accuracy: 0.1810\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.8586 - accuracy: 0.1854\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.8316 - accuracy: 0.1832\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.8060 - accuracy: 0.1898\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.7801 - accuracy: 0.1921\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.7538 - accuracy: 0.2009\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.7274 - accuracy: 0.2031\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.7018 - accuracy: 0.2097\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.6763 - accuracy: 0.2075\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.6493 - accuracy: 0.2119\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.6269 - accuracy: 0.2141\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.6010 - accuracy: 0.2274\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.5757 - accuracy: 0.2318\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.5522 - accuracy: 0.2362\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.5266 - accuracy: 0.2450\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4995 - accuracy: 0.2517\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4748 - accuracy: 0.2472\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4510 - accuracy: 0.2494\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4261 - accuracy: 0.2605\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.4024 - accuracy: 0.2671\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.3772 - accuracy: 0.2693\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.3513 - accuracy: 0.2870\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.3290 - accuracy: 0.2914\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.3044 - accuracy: 0.2936\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.2797 - accuracy: 0.3068\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.2566 - accuracy: 0.3135\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.2364 - accuracy: 0.3289\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 3.2096 - accuracy: 0.3245\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 3.1880 - accuracy: 0.3333\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.1631 - accuracy: 0.3289\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.1394 - accuracy: 0.3466\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.1183 - accuracy: 0.3422\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.0965 - accuracy: 0.3466\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.0738 - accuracy: 0.3576\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.0516 - accuracy: 0.3510\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.0289 - accuracy: 0.3620\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.0063 - accuracy: 0.3731\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.9837 - accuracy: 0.3841\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.9632 - accuracy: 0.3996\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.9412 - accuracy: 0.3996\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.9195 - accuracy: 0.4084\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.8992 - accuracy: 0.4128\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.8788 - accuracy: 0.4172\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.8568 - accuracy: 0.4283\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.8377 - accuracy: 0.4371\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.8162 - accuracy: 0.4415\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.7960 - accuracy: 0.4437\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.7755 - accuracy: 0.4481\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.7548 - accuracy: 0.4592\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.7366 - accuracy: 0.4636\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.7142 - accuracy: 0.4702\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.6943 - accuracy: 0.4768\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.6749 - accuracy: 0.4901\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.6547 - accuracy: 0.5011\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.6368 - accuracy: 0.4923\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.6172 - accuracy: 0.5033\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.5987 - accuracy: 0.5033\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.5769 - accuracy: 0.5121\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.5588 - accuracy: 0.5188\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.5396 - accuracy: 0.5210\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.5203 - accuracy: 0.5188\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.5031 - accuracy: 0.5364\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.4853 - accuracy: 0.5364\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4675 - accuracy: 0.5364\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4505 - accuracy: 0.5386\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4326 - accuracy: 0.5408\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4146 - accuracy: 0.5541\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.3951 - accuracy: 0.5475\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.3762 - accuracy: 0.5497\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.3591 - accuracy: 0.5607\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.3433 - accuracy: 0.5563\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.3230 - accuracy: 0.5651\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.3068 - accuracy: 0.5717\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2890 - accuracy: 0.5695\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.2719 - accuracy: 0.5717\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.2550 - accuracy: 0.5806\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2389 - accuracy: 0.5806\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.2231 - accuracy: 0.5850\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.2091 - accuracy: 0.5916\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1933 - accuracy: 0.5982\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1747 - accuracy: 0.6071\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.1598 - accuracy: 0.6026\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1453 - accuracy: 0.6093\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.1292 - accuracy: 0.6137\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1115 - accuracy: 0.6247\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.0967 - accuracy: 0.6247\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.0824 - accuracy: 0.6269\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.0644 - accuracy: 0.6313\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.0485 - accuracy: 0.6402\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.0327 - accuracy: 0.6424\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.0178 - accuracy: 0.6468\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.0022 - accuracy: 0.6556\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9876 - accuracy: 0.6578\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9719 - accuracy: 0.6600\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9593 - accuracy: 0.6600\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9461 - accuracy: 0.6755\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9300 - accuracy: 0.6733\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9144 - accuracy: 0.6755\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.9000 - accuracy: 0.6755\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.8855 - accuracy: 0.6843\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.8713 - accuracy: 0.6932\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.8555 - accuracy: 0.7020\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.8417 - accuracy: 0.7020\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.8275 - accuracy: 0.7020\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.8131 - accuracy: 0.6998\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.8006 - accuracy: 0.7042\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7874 - accuracy: 0.7020\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7745 - accuracy: 0.7064\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.7588 - accuracy: 0.7086\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.7451 - accuracy: 0.7108\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7337 - accuracy: 0.7130\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.7207 - accuracy: 0.7152\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.7052 - accuracy: 0.7174\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.6917 - accuracy: 0.7196\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6790 - accuracy: 0.7241\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6672 - accuracy: 0.7307\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6544 - accuracy: 0.7307\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.6411 - accuracy: 0.7307\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6290 - accuracy: 0.7351\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.6169 - accuracy: 0.7417\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.6042 - accuracy: 0.7439\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5926 - accuracy: 0.7439\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5818 - accuracy: 0.7439\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.5692 - accuracy: 0.7439\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.5570 - accuracy: 0.7506\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5445 - accuracy: 0.7572\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5319 - accuracy: 0.7594\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5200 - accuracy: 0.7550\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5076 - accuracy: 0.7572\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4970 - accuracy: 0.7572\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4851 - accuracy: 0.7660\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4723 - accuracy: 0.7638\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4571 - accuracy: 0.7660\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4473 - accuracy: 0.7660\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4365 - accuracy: 0.7704\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4232 - accuracy: 0.7704\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4122 - accuracy: 0.7726\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.4017 - accuracy: 0.7770\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.3915 - accuracy: 0.7859\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.3790 - accuracy: 0.7770\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.3686 - accuracy: 0.7748\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.3589 - accuracy: 0.7859\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.3486 - accuracy: 0.7903\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.3377 - accuracy: 0.7925\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.3287 - accuracy: 0.8013\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.3178 - accuracy: 0.8057\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.3076 - accuracy: 0.8035\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2967 - accuracy: 0.8102\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.2887 - accuracy: 0.8168\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2776 - accuracy: 0.8190\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2689 - accuracy: 0.8190\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2588 - accuracy: 0.8234\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.2499 - accuracy: 0.8212\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2392 - accuracy: 0.8234\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.2300 - accuracy: 0.8300\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.2218 - accuracy: 0.8234\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.2131 - accuracy: 0.8256\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2032 - accuracy: 0.8256\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1963 - accuracy: 0.8256\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1871 - accuracy: 0.8256\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1771 - accuracy: 0.8256\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1683 - accuracy: 0.8234\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1587 - accuracy: 0.8322\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.1499 - accuracy: 0.8366\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.1417 - accuracy: 0.8344\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.1338 - accuracy: 0.8344\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1261 - accuracy: 0.8366\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1183 - accuracy: 0.8366\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1107 - accuracy: 0.8344\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0998 - accuracy: 0.8322\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0914 - accuracy: 0.8322\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0833 - accuracy: 0.8389\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0757 - accuracy: 0.8411\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.0681 - accuracy: 0.8455\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0597 - accuracy: 0.8499\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0520 - accuracy: 0.8543\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0451 - accuracy: 0.8609\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0368 - accuracy: 0.8609\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0287 - accuracy: 0.8609\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0215 - accuracy: 0.8631\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0139 - accuracy: 0.8631\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0064 - accuracy: 0.8609\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.9997 - accuracy: 0.8609\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9926 - accuracy: 0.8675\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.9847 - accuracy: 0.8698\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9778 - accuracy: 0.8698\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9716 - accuracy: 0.8698\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.9633 - accuracy: 0.8675\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9599 - accuracy: 0.8698\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9522 - accuracy: 0.8698\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9444 - accuracy: 0.8720\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9381 - accuracy: 0.8698\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9301 - accuracy: 0.8698\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9232 - accuracy: 0.8675\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.9158 - accuracy: 0.8720\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.9105 - accuracy: 0.8742\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9038 - accuracy: 0.8720\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8967 - accuracy: 0.8720\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.8908 - accuracy: 0.8720\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8846 - accuracy: 0.8764\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8792 - accuracy: 0.8720\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8715 - accuracy: 0.8764\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8670 - accuracy: 0.8742\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8597 - accuracy: 0.8764\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8542 - accuracy: 0.8764\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8475 - accuracy: 0.8742\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8420 - accuracy: 0.8742\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8365 - accuracy: 0.8742\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8301 - accuracy: 0.8764\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8248 - accuracy: 0.8786\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8198 - accuracy: 0.8764\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8145 - accuracy: 0.8808\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8080 - accuracy: 0.8808\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8027 - accuracy: 0.8786\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7965 - accuracy: 0.8764\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7913 - accuracy: 0.8808\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7852 - accuracy: 0.8764\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7801 - accuracy: 0.8742\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7744 - accuracy: 0.8786\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.7691 - accuracy: 0.8808\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7641 - accuracy: 0.8830\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7590 - accuracy: 0.8830\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7536 - accuracy: 0.8852\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7481 - accuracy: 0.8852\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7432 - accuracy: 0.8852\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7385 - accuracy: 0.8830\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7336 - accuracy: 0.8852\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7291 - accuracy: 0.8786\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7246 - accuracy: 0.8852\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7202 - accuracy: 0.8874\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7150 - accuracy: 0.8874\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7102 - accuracy: 0.8896\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7060 - accuracy: 0.8896\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7007 - accuracy: 0.8896\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6969 - accuracy: 0.8896\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6927 - accuracy: 0.8918\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6876 - accuracy: 0.8918\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6826 - accuracy: 0.8918\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6783 - accuracy: 0.8896\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6752 - accuracy: 0.8874\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6706 - accuracy: 0.8874\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6667 - accuracy: 0.8852\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6623 - accuracy: 0.8896\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6583 - accuracy: 0.8874\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6540 - accuracy: 0.8874\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6511 - accuracy: 0.8874\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6471 - accuracy: 0.8918\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6428 - accuracy: 0.8940\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6392 - accuracy: 0.8940\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6352 - accuracy: 0.8962\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6313 - accuracy: 0.8985\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6269 - accuracy: 0.9007\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6232 - accuracy: 0.8985\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6195 - accuracy: 0.9029\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6153 - accuracy: 0.9051\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6120 - accuracy: 0.9051\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6082 - accuracy: 0.9051\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6055 - accuracy: 0.9051\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6020 - accuracy: 0.9051\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5986 - accuracy: 0.9095\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5943 - accuracy: 0.9073\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5899 - accuracy: 0.9139\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5862 - accuracy: 0.9117\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5833 - accuracy: 0.9161\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5795 - accuracy: 0.9183\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5760 - accuracy: 0.9161\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5745 - accuracy: 0.9139\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5705 - accuracy: 0.9139\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5665 - accuracy: 0.9139\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5651 - accuracy: 0.9139\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5602 - accuracy: 0.9183\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5583 - accuracy: 0.9183\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5634 - accuracy: 0.9161\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5525 - accuracy: 0.9205\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5483 - accuracy: 0.9205\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5446 - accuracy: 0.9205\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5411 - accuracy: 0.9205\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5377 - accuracy: 0.9205\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5339 - accuracy: 0.9205\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5304 - accuracy: 0.9205\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5270 - accuracy: 0.9205\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5246 - accuracy: 0.9205\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5223 - accuracy: 0.9227\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5195 - accuracy: 0.9249\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5164 - accuracy: 0.9227\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5124 - accuracy: 0.9205\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5100 - accuracy: 0.9205\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5070 - accuracy: 0.9205\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5045 - accuracy: 0.9183\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5012 - accuracy: 0.9227\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4986 - accuracy: 0.9249\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4960 - accuracy: 0.9227\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4930 - accuracy: 0.9205\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4902 - accuracy: 0.9227\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4877 - accuracy: 0.9161\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4851 - accuracy: 0.9205\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4826 - accuracy: 0.9249\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4812 - accuracy: 0.9205\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4787 - accuracy: 0.9205\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4766 - accuracy: 0.9227\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4728 - accuracy: 0.9205\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4705 - accuracy: 0.9249\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4680 - accuracy: 0.9294\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4663 - accuracy: 0.9272\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4630 - accuracy: 0.9272\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4593 - accuracy: 0.9294\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4574 - accuracy: 0.9294\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4541 - accuracy: 0.9272\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4520 - accuracy: 0.9294\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4491 - accuracy: 0.9294\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4476 - accuracy: 0.9294\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4446 - accuracy: 0.9316\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4421 - accuracy: 0.9316\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4393 - accuracy: 0.9338\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4376 - accuracy: 0.9338\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4338 - accuracy: 0.9338\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4321 - accuracy: 0.9338\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4302 - accuracy: 0.9294\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4280 - accuracy: 0.9338\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4258 - accuracy: 0.9338\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4227 - accuracy: 0.9360\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4211 - accuracy: 0.9360\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4191 - accuracy: 0.9338\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4164 - accuracy: 0.9360\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4143 - accuracy: 0.9404\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4127 - accuracy: 0.9404\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4118 - accuracy: 0.9360\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4088 - accuracy: 0.9360\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4051 - accuracy: 0.9426\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4036 - accuracy: 0.9382\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4016 - accuracy: 0.9382\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3999 - accuracy: 0.9382\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3981 - accuracy: 0.9426\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3953 - accuracy: 0.9426\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3943 - accuracy: 0.9404\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3928 - accuracy: 0.9404\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3904 - accuracy: 0.9404\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3928 - accuracy: 0.9360\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3886 - accuracy: 0.9404\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3865 - accuracy: 0.9426\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3840 - accuracy: 0.9316\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3832 - accuracy: 0.9404\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3790 - accuracy: 0.9404\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3787 - accuracy: 0.9360\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3753 - accuracy: 0.9382\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3726 - accuracy: 0.9382\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3711 - accuracy: 0.9404\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3685 - accuracy: 0.9404\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3671 - accuracy: 0.9404\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3655 - accuracy: 0.9382\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3637 - accuracy: 0.9404\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3619 - accuracy: 0.9426\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3607 - accuracy: 0.9360\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3586 - accuracy: 0.9382\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3577 - accuracy: 0.9404\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3556 - accuracy: 0.9426\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3547 - accuracy: 0.9426\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3525 - accuracy: 0.9426\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3516 - accuracy: 0.9404\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3492 - accuracy: 0.9426\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3479 - accuracy: 0.9404\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3466 - accuracy: 0.9404\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3449 - accuracy: 0.9426\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3437 - accuracy: 0.9404\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3422 - accuracy: 0.9448\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3395 - accuracy: 0.9426\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3379 - accuracy: 0.9426\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3359 - accuracy: 0.9448\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3353 - accuracy: 0.9404\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3336 - accuracy: 0.9382\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3311 - accuracy: 0.9404\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3303 - accuracy: 0.9382\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3282 - accuracy: 0.9448\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3271 - accuracy: 0.9404\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3248 - accuracy: 0.9360\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3242 - accuracy: 0.9448\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3242 - accuracy: 0.9404\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3206 - accuracy: 0.9404\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3201 - accuracy: 0.9448\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3182 - accuracy: 0.9426\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3173 - accuracy: 0.9404\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3157 - accuracy: 0.9404\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3141 - accuracy: 0.9404\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3126 - accuracy: 0.9426\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3114 - accuracy: 0.9404\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3099 - accuracy: 0.9426\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3088 - accuracy: 0.9426\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3078 - accuracy: 0.9382\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3065 - accuracy: 0.9426\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3045 - accuracy: 0.9426\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3039 - accuracy: 0.9382\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3015 - accuracy: 0.9382\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.3013 - accuracy: 0.9382\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2995 - accuracy: 0.9404\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2983 - accuracy: 0.9426\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2984 - accuracy: 0.9404\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2961 - accuracy: 0.9404\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2943 - accuracy: 0.9404\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2946 - accuracy: 0.9360\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2931 - accuracy: 0.9382\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2922 - accuracy: 0.9404\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2903 - accuracy: 0.9426\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2884 - accuracy: 0.9426\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2898 - accuracy: 0.9426\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2885 - accuracy: 0.9382\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2877 - accuracy: 0.9426\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2895 - accuracy: 0.9360\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2953 - accuracy: 0.9404\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2904 - accuracy: 0.9426\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2873 - accuracy: 0.9404\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2873 - accuracy: 0.9382\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2829 - accuracy: 0.9382\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2806 - accuracy: 0.9448\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2788 - accuracy: 0.9404\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2773 - accuracy: 0.9382\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2745 - accuracy: 0.9382\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2742 - accuracy: 0.9404\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2718 - accuracy: 0.9448\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2706 - accuracy: 0.9448\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2699 - accuracy: 0.9426\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2680 - accuracy: 0.9426\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2677 - accuracy: 0.9426\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2663 - accuracy: 0.9470\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2652 - accuracy: 0.9448\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2646 - accuracy: 0.9448\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2629 - accuracy: 0.9426\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2621 - accuracy: 0.9426\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2606 - accuracy: 0.9448\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.9448\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2592 - accuracy: 0.9470\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2575 - accuracy: 0.9492\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2566 - accuracy: 0.9426\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2556 - accuracy: 0.9448\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2552 - accuracy: 0.9426\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2569 - accuracy: 0.9404\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2541 - accuracy: 0.9426\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2528 - accuracy: 0.9492\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2511 - accuracy: 0.9426\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2502 - accuracy: 0.9448\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2491 - accuracy: 0.9448\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2486 - accuracy: 0.9492\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2488 - accuracy: 0.9448\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2466 - accuracy: 0.9448\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2474 - accuracy: 0.9426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZustMM4oKva5"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy4mzeIJKsFN"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "uD0ebEV2Kx7W",
        "outputId": "da7b38de-c1d0-476e-e11e-250a46c3d51c"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxV9Z3/8dcnO4GQEELYISxhFVmM4IpLXfGndlerrbZWWltaO7Uz6jh1Orad7rXjjG211bY6brW1ylQUcV+qQNh3CFsSCCQhZIGQ7d7v7497iQECXCAn5y7v5+ORB+d8z8nN5wuXfO73e875fM05h4iIJK4kvwMQERF/KRGIiCQ4JQIRkQSnRCAikuCUCEREElyK3wGcqLy8PFdQUOB3GCIiMWXJkiXVzrl+nR2LuURQUFBAcXGx32GIiMQUM9t+tGOaGhIRSXBKBCIiCU6JQEQkwSkRiIgkOCUCEZEEp0QgIpLglAhERBKcEoGISDc7vPx/WyDIn4vL2F3fBEDdgVaeXlRKMOhYsHY3W6v3expPzD1QJiLxrTUQJDU5ibZAkPqmNnJ7prUfawsESUnu/PNrS1uQtJQkWgNB9uxrIbtHKmaQnpJEVUMzf1lazpj8LMr2NnLz2QXUNLYwb1UFU4f2YV1FPfm909nX3MZVkwYSdBAIOuoOtNK3Zxo1jS3k9UoHoKk1QHpKEmbWHlN1+OelJBupyUk0tQbISE3mueIyzizIpSCvJ88uLmXH3gPMOn0g33x6Gdk9UvnE1CEEnOO54jJWltcB8PDnz+DlVRW8sHwnv3h1A9X7WgD4+Wcm86lpg9t/bleyWFuYpqioyOnJYpH48vbGKuatrGD6iFzue3E19109gScXlrKyvI7zC/MYmdeTpaW1bKps4KpJgzh3dF+G5mby/NIdNLcGqGxoZuHWPfzL5eN4/MNtlNUcaH/tkf16sqXq0E/U04blsLS0ttNYzi/MY/ueRkprGg9pv/3CUYzp34vvvrCGgrxMTh+Sw+UTB3D//61hc4fXH5zTg+p9zVw1aSDPL9sBwN1XjuPHL68/av/79kxjVL9eLNpWc8QxM8hMTWZ/S4B7Z43ntpkjj/8X2gkzW+KcK+r0mBKBiHSHtkCQVTvqaAs6hudmUlrTyFOLSqmsb+b9zdV09qtoxohcdtU3sX1P45EHgdRkIz0lmX3NbaQmG60Bx6DsDG6bOZI31leyafc+0lOTaAs4PjY+n9MGZfPckjIWb9t7xGsVDe9DUUEur6yuoC3oSE4yemeksmpH3SHn9UhNJr93OhV1TbS0Bcnrlc5XLxjJD15a135OSpLRFnRHvP73rpnIqh11jOrXizML+rBg7W6Sk4yLx+XT3Bbk/ZJqFm2rYcOuBr4ycxR9e6Uxpn8WwaDj8Q+28fGpg8nJTONkKBGISLdZv6ueYbmZLCutpU9mGrsbmlheWstfl5ZTvvfAIeemJBlB5+jfO4PvXDaWJxdu57KJAxiWm8nA7AymDusDwG/f3kzN/hYeeWcLALMmDWDK0ByunTKYXukprN9VT0VdE2+ur+Leq8YfMp10uEDQsXhbDRmpyeRmpnH/39dw71UTGJHX84hznXOs2VlPYf9evLOxGgNmjulHWkoSr6yu4KVVu/juVePJ753Btur9vLWhksqGZu68bCzvbKpiYHYGaclJbK7az8wxeaSnJHfdX/QJUiIQkZO2rXo/jS0BJgzqDUB9Uysry+o4d3RfzIyy8BTK0NxM3t5Yxc2PLer0dU4b3JtPTxvCkD6ZfO2ppUweks3/fG4aLW1BsjNT6Z2RetxYHliwkanDcrhwbH7XdTBBKBGIyHGt3VnPwq17DmmrO9DKb9/eTGvAcet5I8jPSuf3725lV30TPVKTmTI0hw+2hL5n/MDerKuoB+Dyif2ZMaIv5XsPMHVYDuMGZDE6v1f7hc66xlayMlJISur6C5/SuWMlAt01JJJAnHM8u7iM/36jhDsuKeSayYP49ZslPPhGyVG/5+yRfcntldY+LQNw4dh+vLWhqj0JAKyrqCc12Xhm9tmcMbzPMePIzjz+p3/pPhoRiCSI8r2NXPCztwgEj/w/3y8rnbTkJB69pYgBvTPa2w2jd48UzIz6plZWlNXS2BLg8okDaGxp4/pHPuTq0wfx+bOHk2RGclLoS6KPRgQiCcg5R2lNI62BIB9sqeG7L6wGoH/vdOZ/ayavrt1N9b5msjJS+cwZQ8hIPfaFzN4ZqZxf+NECV5lpKcydc56nfZDuoUQgEmecc2yt3s8vXt3IS6sqDjk2e+ZI/nXWeAA+WzTUj/AkCikRiMSRAy0Bvvviav6ypByAL583gslDc0gyY+aYPLIiuDNHEo8SgUgMKd5WQ/neA5w2uDej87Pa2wNBx89f3cCzi8vY29jCpMHZnDO6L3dfMc6TkgQSX5QIRKJYSeU+nisu46VVFTS1BtrrzqQlJ3Hr+SMorWmkqSVAbs80nltSzrmj+zJ75iguGNPvOK8s8hElAhEfHCye5pzDzGhuC/DMojKeXVzGtj37aWoNkJaSRFNrEIAzC/owMq8Xw/pmctHYfH42fz2/eWvzIa/5yamD+eV1U/zojsQ4JQIRD7QFgpgZRuihrJzwffMvLN/Bb9/awsbKBi4Z3581O+qYc3Ehzy4uZUV5Hf2y0rl2yiCeXlTWngRe+/bMQ6aBAP7wxelsrtrHWxuq+PS0UAXLPro3X06SniMQOQXOufAv+jTW76rnxeU7uWR8Pv/07IpDqleO7Z9FXlYa75eEHsAalptJ2d7GQwqt/famaVw+cQBmhnOOrz25lML8Xnz7srHd3S2JQyoxIdJFqvc184tXN7JnXzMtgSBVDc2s39XAhIG9j6hSedBni4ZQvH0vLW1BPnPGUMYOyOLicfls2NVA9b5mHn1vK1edPpAbpg/r5t5IItEDZSKnaEVZLRmpyfzgpbW8X1JN/94ZVNQ1MTinB8NyM1m/q56M1CR+c+MZ1De1MjKvFwHnmDwk+6h37Uwakg3AReNUQE38pUQgchTOOb717HJWlNWyrUM9/H++fCy3XzCKNTvrmTioN0lJofILdY2tDM3N9DFikZOjRCByFO+X7OHF5TuZXpDLtVMGc6A1QFVDMzedNZykJGv/RA+h8guRlFEWiUZKBCKdaAsE+fEr6+iXlc7jt04/bh0ekVimRCDSid+9u5XVO+r59Y3TlAQk7iX5HYBItFm9o44HXtvIFRMHMGvSQL/DEfGcRgSS0H73zhaSkozte/bjHHzpvBHc8MiHobVsPz7R7/BEuoUSgSSEYNCxuWofhf0/ekL3zfWV/HDeOgAO3uH5xIfbAXh69lnkZ2Uc8Toi8UiJQBLCg29s4levbeLOS8cwol9PAH65YCP9stL5+WcmMzw3k4amNn7+6gbG9O/FaYOzj/OKIvFDiUDi2vY9+5nz1LL2p35/sWDjIcdvOafgkEqdf/rS9G6NTyQaKBFI3HrozRJ+Nn8DAFnpKdx41nAG9+nBjBG5NLUGWLB2N5+bobIOIkoEEpdW76jjZ/M3MH1ELt+5bCyTh2aTnnLobaCnD8nxKTqR6OLp7aNmdoWZbTCzEjO7u5Pjw8zsTTNbZmYrzWyWl/FIYmhqDfDoe1vJTEvmd18oYvqI3COSgIh8xLMRgZklAw8BlwLlwGIzm+ucW9vhtH8D/uyc+42ZTQDmAQVexSTxb19zGx/7xVvsrm/mC2cPJ7uHyj6IHI+XI4LpQIlzbotzrgV4Brj2sHMc0Du8nQ3s9DAeSQB/fH8ru+ubyeuVzm3nj/Q7HJGY4OU1gsFAWYf9cmDGYed8D3jVzL4B9AQu6eyFzGw2MBtg2DBd3JPOOeeYu2InZxb04bmvnuN3OCIxw+8SEzcAf3TODQFmAU+Y2RExOececc4VOeeK+vXTotzSuR+/sp6Nu/dxzeRBfociElO8HBHsAIZ22B8SbuvoVuAKAOfcB2aWAeQBlR7GJXGktrGFG3+/kOvOHMrDb2/h+jOH8rkZw/0OSySmeDkiWAwUmtkIM0sDrgfmHnZOKfAxADMbD2QAVR7GJHHmpVUVrNlZz30vriHJ4O4rx5Gc1PmKYCLSOc8SgXOuDZgDzAfWEbo7aI2Z3W9m14RPuxO4zcxWAE8Dt7hYW0RZfBMMOv6ypLx9vyCvJzmZaT5GJBKbPH2gzDk3j9AtoR3b7uuwvRY418sYJD4t2V7DT17ZwLLSWr51SSHF2/Zy9WSVjBY5GXqyWGKGc44fv7ye5WW1LNxagxnceekY5lw8+qgLxIvI8SkRSNTb39zG9j2NBIKOh9/ZAsC0YTn85FOnH1JWWkROjhKBRLWm1gBX/Nc7lNUcaG/76+3ncMbwPj5GJRJflAgkqr24fEd7EhiWm8kt5xQoCYh0MSUCiUotbUFaAkEee28b4wZk8eKcc0lNSiJJt4aKdDklAok6r6zexZ1/Xs7+lgAAP/306aoeKuIhJQKJKs1tAf7thdUMyunBp88YQmZ6Cp+YOtjvsETimhKBRI11FfXMW1VB9b5mfvnZycwco7pSIt1BiUCiwtLSvXzy1/8A4MKx/Ti/MM/niEQShxKBRIXfvrWZrIwUHr7pDIoKcvWAmEg38rsMtSQ45xy/f3cLr67dzW3nj+Sc0XmkpehtKdKdNCIQ3zQ0tXLXX1cyb9UuLp3Qn69eMMrvkEQSkhKB+GJdRT1fe3IppTWN3HPlOGbPHKnpIBGfKBFIt2hqDVBa00iSwQMLNvHSqgrys9J5+razmD4i1+/wRBKaEoF0i28+vYxX1+5u309NNp7/2jkM6ZPpY1QiAkoE0g3qDrTy5oZKZk0aQEZqMsNze3L15IFKAiJRQolAPPfG+t20Bhy3nT+SqcNUME4k2ug+PfHcvFW7GJidweQhOX6HIiKdUCIQT+1rbuPtjVVccdoAVQ4ViVKaGhJPNDS18uh7W3l7YxUtbUGuPE3rCYtEKyUC8cRzxeX86rVNAEwemqPFZESimBKBeGJdRT0Af//GeZw2ONvnaETkWHSNQLpcXWMrLy7fyfmFeUoCIjFAiUC61PKyWi7/1Tu0BIJMGaq7hERigaaGpMs45/jm08tITjIevbmI8wu1sIxILNCIQLrMmp31lNY0csfHCvnY+P4qJy0SI/Q/VbpES1uQn87fQI/UZC6d0N/vcETkBCgRSJf44UtreWdjFf961Xj69EzzOxwROQFKBHLKWgNBXlyxk6smDeTzZw33OxwROUFKBHLK5i7fSW1jK1dPHuR3KCJyEnTXkJy01kCQ2x4v5t1N1UwblqNrAyIxSiMCOWnLSmt5a0MVpw3O5vc3n0myisqJxCSNCOSkvV9STZLB41+aTnaPVL/DEZGT5OmIwMyuMLMNZlZiZncf5ZzPmtlaM1tjZk95GY90nZr9LTy1qJSi4blKAiIxzrMRgZklAw8BlwLlwGIzm+ucW9vhnELgHuBc59xeM8v3Kh7pOo++t5Xv/z30z/jYzWf6HI2InCovRwTTgRLn3BbnXAvwDHDtYefcBjzknNsL4Jyr9DAe6SK/em0jAOMH9mbSEBWVE4l1XiaCwUBZh/3ycFtHY4AxZva+mX1oZld09kJmNtvMis2suKqqyqNwJRIbdjXQ0NTGDdOH8czss/wOR0S6gN93DaUAhcCFwA3A78zsiJKVzrlHnHNFzrmifv1UyMxPf/zHVtJTkviXy8fq2oBInPAyEewAhnbYHxJu66gcmOuca3XObQU2EkoMEoX27m/h+aU7+MTUwSojIRJHvEwEi4FCMxthZmnA9cDcw855gdBoADPLIzRVtMXDmOQkldU08pUnltASCPLFc0f4HY6IdCHPEoFzrg2YA8wH1gF/ds6tMbP7zeya8GnzgT1mthZ4E/hn59wer2KSk/fjl9ezaFsNcy4azdgBWX6HIyJdyJxzxz/J7HngUeBl51zQ86iOoaioyBUXF/sZQkKprG/ioTdL+NMH2/ls0RB++unJfockIifBzJY454o6OxbpiODXwOeATWb2YzMb22XRSVT7xasb+dMH20lOMm6cocqiIvEookTgnHvNOXcjMA3YBrxmZv8wsy+amW4diVPLy2p5YfkOpg3LYf63ZjJZaxCLxKWIrxGYWV/gFuDLwDLgvwglhgWeRCa+WrK9ho8/9D7NbUHumTWe0fm9/A5JRDwSUYkJM/sbMBZ4ArjaOVcRPvSsmWnCPo60BYLMfmIJtY0tACQnGWcM6+NzVCLipUhrDT3onHuzswNHu/ggsWn9rgbeWB+q9DFhYG+enn0WSSovLRLXIp0amtDxiV8z62NmX/MoJvHRstK97dt3XTlOTw+LJIBIE8FtzrnagzvhInG3eROS+KHuQCvrd9Xz1KIy+mWls/VHs7hgjMp5iCSCSKeGks3MXPihg3CJadUYiBN1ja3M+NFrNLUG6ZmWzAPXTcFM00EiiSLSRPAKoQvDD4f3vxJukziwblc9Ta1BvnrBKG6cMYyhuZl+hyQi3SjSRHAXoV/+t4f3FwC/9yQi6XabKvcBcPM5wxmY3cPnaESku0WUCMJlJX4T/pI48srqXXz3hdUADOid4XM0IuKHSJ8jKAR+BEwA2n9bOOdGehSXdIM9+5r55jPLALh4XL6uC4gkqEinhv4A/DvwAHAR8EX8X9RGTtEzi8toaQvy2rdnUtC3p9/hiIhPIv1l3sM59zqhaqXbnXPfA67yLizxWmsgyNOLSjljeB9G52eRkqy8LpKoIh0RNJtZEqHqo3MIrTSm4jMx6nfvbOHhd7ZQva9Zi8yISMQjgjuATOCbwBnATcDNXgUl3qlsaOKH89ZRva+ZcQOyuHHGML9DEhGfHXdEEH547Drn3HeAfYSuD0iMKt72UQmJOy8bS0Zqso/RiEg0OO6IwDkXAM7rhlikGyzeVgPAp6YN4fzCPJ+jEZFoEOk1gmVmNhd4Dth/sNE597wnUYlnFm+r4eyRffnFZ7XkpIiERJoIMoA9wMUd2hygRBBDGppaWbuznjkXjfY7FBGJIpE+WazrAnFg/prdBB2cM1pTQiLykUifLP4DoRHAIZxzX+ryiMQTzjkee28rY/r3YsaIXL/DEZEoEunU0N87bGcAnwB2dn044pUPtuxhbUU9P/rkJJWSEJFDRDo19NeO+2b2NPCeJxFJlwsGHf8xdy0DszP4+JTBfocjIlHmZOsKFAL5XRmIeGf9rgY27G7gW5cU0iNNzw2IyKEivUbQwKHXCHYRWqNAYsA/NlcDMFNLT4pIJyKdGsryOhDxRt2BVp74cDvjBmRp0RkR6VREU0Nm9gkzy+6wn2NmH/cuLOkKzW0Brn/kQ3bsPcD3rpnodzgiEqUivUbw7865uoM7zrlaQusTSBR78sNS1lXU8+sbp3HWyL5+hyMiUSrSRNDZeZHeeio++b+VOzltcG8umzjA71BEJIpFmgiKzeyXZjYq/PVLYImXgcmpqWxoYllpLVcoCYjIcUSaCL4BtADPAs8ATcDXvQpKTt0Hm/cAcMEY3eUrIscW6V1D+4G7PY5FutDr6yrJ7pHKhEG9/Q5FRKJcpHcNLTCznA77fcxsfgTfd4WZbTCzEjM7aiIxs0+ZmTOzosjClmP5YPMe5q7YyXVnDiU5SeUkROTYIp0aygvfKQSAc24vx3myOLyy2UPAlcAE4AYzm9DJeVmElsJcGGnQcnTOOb774mqG983kny4Z43c4IhIDIk0EQTNrX9zWzAropBrpYaYDJc65Lc65FkLXFq7t5LzvAz8hdN1BTtE7m6opqdzH7ReMUjkJEYlIpIngXuA9M3vCzP4XeBu45zjfMxgo67BfHm5rZ2bTgKHOuZcijEOOobK+iZsfWwSgW0ZFJGKRXix+JTx/PxtYBrwAHDiVH2xmScAvgVsiOHd2+GczbNiw45yduFaUh575u3fWeHJ7pvkcjYjEikiLzn2Z0Dz+EGA5cBbwAYcuXXm4HcDQDvtDwm0HZQGnAW+F6+MPAOaa2TXOueKOL+ScewR4BKCoqOh4U1IJa+3OeszgczOULEUkcpFODd0BnAlsd85dBEwFao/9LSwGCs1shJmlAdcDcw8edM7VOefynHMFzrkC4EPgiCQgkVuzs44RfXvSM10PfYtI5CJNBE3OuSYAM0t3zq0Hxh7rG5xzbcAcYD6wDvizc26Nmd1vZtecStBypGDQsXhbDVOG5Rz/ZBGRDiL96Fgefo7gBWCBme0Fth/vm5xz84B5h7Xdd5RzL4wwFunEul317G1s5dxRWpheRE5MpBeLPxHe/J6ZvQlkA694FpWcsIMlJc4ZrSqjInJiTngy2Tn3theByKl5v6SakXk9tfiMiJywk12zWKLIuop6Fm6t0WhARE6Kbi+JcSvLa7nmf94H0PUBETkpGhHEuPdKqtu3zx6lEYGInDiNCGJUMOi47pEPWLxtLwDfu3oCOZl6mlhETpwSQYzaUXugPQncf+1EvnB2gb8BiUjM0tRQjFpRHnqw+3MzhnHTjOE+RyMisUwjghgTCDreK6lmzlPLAPjny8aSpMVnROQUaEQQY574YFt7qWmAPqoyKiKnSIkgxryzKXSX0LgBWbz1nQv9DUZE4oKmhmLMztoDTBqczZO3zaB3Rqrf4YhIHNCIIIbsb26jpHIf5xXmKQmISJdRIoghzy8tpy3ouGR8f79DEZE4okQQQ15evYtxA7KYpjUHRKQLKRHEiEDQsaKslukjcgkv7Ski0iWUCGLEu5uq2N8SYKpGAyLSxZQIYkBlfRN3PLOcQdkZzCzs53c4IhJndPtoDHhyYSkNTa387Wvn0LdXut/hiEic0YggBmzY1UBBXk9G9uvldygiEoeUCGLApsoGCvOVBETEG0oEUa62sYXNVfsZrUQgIh5RIohyd/55BUkG5+sisYh4RIkgipVU7uP19ZV84+JCzhqpZShFxBtKBFHs1bW7ALhxxjCfIxGReKbbR6PQ3v0t3P38St7bVM3Y/lnk987wOyQRiWNKBFHoyYXbmb9mNwAXjcv3ORoRiXdKBFHGOcdzS8rJyUzl9gtG8cVzR/gdkojEOSWCKLNmZz3b9zTy409O4vrpujYgIt7TxeIo4pzjJ6+sp2daMpdNHOB3OCKSIJQIosjflu3g3U3V3HXlOHK1KL2IdBMlgijyfyt2MiKvJzfNGO53KCKSQJQIooRzjmVltUwvyCUpSQvPiEj3USKIEi+v3kVtYytTtPCMiHQzTxOBmV1hZhvMrMTM7u7k+LfNbK2ZrTSz180s4eZEGlvauOjnb/G1J5cybkAWV56mi8Qi0r08SwRmlgw8BFwJTABuMLMJh522DChyzp0O/AX4qVfxRKv//XA7W6v3A/D7m4vIydRFYhHpXl6OCKYDJc65Lc65FuAZ4NqOJzjn3nTONYZ3PwSGeBhP1FleVst/zlvP2SP7svVHsxjSJ9PvkEQkAXmZCAYDZR32y8NtR3Mr8HJnB8xstpkVm1lxVVVVF4borxeW7SAtJYnf3DQNM10gFhF/RMXFYjO7CSgCftbZcefcI865IudcUb9+8VGXf+GWPTy1sJSLxvbTdJCI+MrLEhM7gKEd9oeE2w5hZpcA9wIXOOeaPYwnavzunS38cN468rPS+f61p/kdjogkOC9HBIuBQjMbYWZpwPXA3I4nmNlU4GHgGudcpYexRI2lpXv54bx1AHzj4tEqMS0ivvNsROCcazOzOcB8IBl4zDm3xszuB4qdc3MJTQX1Ap4Lz5GXOueu8SomvznnuPdvqxmUncHfvn4u/ZUERCQKeFp91Dk3D5h3WNt9HbYv8fLnR5sNuxtYV1HPDz5+mpKAiESNqLhYnCheWb2LJIMr9NCYiEQRJYJu9N6maiYNziavV7rfoYiItFMi6AZtgSAL1u5mSelezhmd53c4IiKHUCLoBi+tquC2x4vJSEnmk1OP9UydiEj301KVHntx+Q7ueGY5AO/edZGmhUQk6mhE4LGDSaBHarKSgIhEJSUCj6WEF5kZ3lcF5UQkOikReGhb9X7ago7pBbn8/uYiv8MREemUEoGH7vrrStKSk7j/4xNVYlpEopYSgQdaA0G+89wKFm6t4baZIxg3oLffIYmIHJUSgQf++P42/rKkHIDzC+OjbLaIxC/dPuqBZxaXkpOZytWnD+KM4X38DkdE5JiUCLrYLX9YxOaq/Xz/2ol8/uwCv8MRETkuTQ11oRVltby1oYqpw3L4xLSEWn5ZRGKYEkEX+uvScjJSk3j8S9Ppla7BlojEBiWCLvL6ut08/sF2LhqbT1ZGqt/hiIhETB9bT0BbIMiyslpG5PVsLxexrqKevY0t/Ojl9QB8/uzhfoYoInLClAhOwB//sY0fvLSOSYOzeeC6yXzr2eWs3lHffvwrM0dyziiVmRaR2KJEcBRLtu9lR+0BzhqZyzsbqymtaeTB1zcBsGpHHdc/spDqfc189YJRrN5Rx3sl1YwfqAfHRCT2KBF04qWVFXz9qaVHtKcmG09++SzmPLWUyoZmvnzeCO6+chx797fw27c3c9nE/j5EKyJyasw553cMJ6SoqMgVFxd78toNTa28umY3zxaXUVF3gO9cNpZlpbUMyM7gsfe28tCN0zizIJe1O+v5cMsePn/2cFKTdb1dRKKfmS1xznVa/VKJAHh7YxVtgSBPLyrjtXW7AZhz0Wi+c/nY9nOcc5hZl/5cEZHucqxEoKkh4D9fWsfWPftpCwTb2w6/+0dJQETiVcIngqbWACVV+wgEQyOjx24pYkReL/r3zvA5MhGR7pFQieDXb5UwtE8mV08e1N62YVcDgaDjk9MGk5qUxEVj8/XpX0QSSkIlgp++sgGAobmZ3PWXlbQGg1TVN5Nk8O1Lx2jxGBFJSAmTCDpeFH+/pJoNuxu46vSBnDbIuHbKICUBEUlYCZMIGlsC7du76pronZHCQ5+b5mNEIiLRIWFugt/b2NK+vaK8lnxdDBYRARIpEexvbd9eWV5H/97pPkYjIhI9EiYR1HQYEQDkZ2lEICICCZQIasOJoHdG6LJIvkYEIiJAAiWCmv2hRHDPrPFcMKYfV5420OeIRESig6eJwMyuMLMNZlZiZnd3cjzdzJ4NH19oZgVexTI4pweXTejPZ4uG8qcvTTx5qHUAAAazSURBVGfK0ByvfpSISEzx7PZRM0sGHgIuBcqBxWY21zm3tsNptwJ7nXOjzex64CfAdV7Ec9nEAVw2cYAXLy0iEtO8HBFMB0qcc1uccy3AM8C1h51zLfCn8PZfgI+Z6juIiHQrLxPBYKCsw355uK3Tc5xzbUAd0PfwFzKz2WZWbGbFVVVVHoUrIpKYYuJisXPuEedckXOuqF+/fn6HIyISV7xMBDuAoR32h4TbOj3HzFKAbGCPhzGJiMhhvEwEi4FCMxthZmnA9cDcw86ZC9wc3v408IaLtSXTRERinGd3DTnn2sxsDjAfSAYec86tMbP7gWLn3FzgUeAJMysBagglCxER6UaeVh91zs0D5h3Wdl+H7SbgM17GICIixxYTF4tFRMQ7FmtT8mZWBWw/yW/PA6q7MJxYoD4nBvU5MZxKn4c75zq97TLmEsGpMLNi51yR33F0J/U5MajPicGrPmtqSEQkwSkRiIgkuERLBI/4HYAP1OfEoD4nBk/6nFDXCERE5EiJNiIQEZHDKBGIiCS4hEkEx1stLVaZ2WNmVmlmqzu05ZrZAjPbFP6zT7jdzOzB8N/BSjOb5l/kJ8/MhprZm2a21szWmNkd4fa47beZZZjZIjNbEe7zf4TbR4RX9ysJr/aXFm7vttX/vGRmyWa2zMz+Ht6P6/4CmNk2M1tlZsvNrDjc5ul7OyESQYfV0q4EJgA3mNkEf6PqMn8Erjis7W7gdedcIfB6eB9C/S8Mf80GftNNMXa1NuBO59wE4Czg6+F/z3judzNwsXNuMjAFuMLMziK0qt8DzrnRwF5Cq/5Bh9X/gAfC58WiO4B1Hfbjvb8HXeScm9LhmQFv39vOubj/As4G5nfYvwe4x++4urB/BcDqDvsbgIHh7YHAhvD2w8ANnZ0Xy1/Ai4SWRE2IfgOZwFJgBqGnTFPC7e3vc0LFHs8Ob6eEzzO/Yz/Bfg4J/9K7GPg7YPHc3w793gbkHdbm6Xs7IUYERLZaWjzp75yrCG/vAvqHt+Pu7yE8BTAVWEic9zs8TbIcqAQWAJuBWhda3Q8O7VdEq/9FuV8B/wIEw/t9ie/+HuSAV81siZnNDrd5+t72tPqo+M8558wsLu8RNrNewF+Bbznn6jsudx2P/XbOBYApZpYD/A0Y53NInjGz/wdUOueWmNmFfsfTzc5zzu0ws3xggZmt73jQi/d2oowIIlktLZ7sNrOBAOE/K8PtcfP3YGaphJLAk86558PNcd9vAOdcLfAmoamRnPDqfnBov2J99b9zgWvMbBvwDKHpof8ifvvbzjm3I/xnJaGEPx2P39uJkggiWS0tnnRc+e1mQnPoB9u/EL7T4CygrsNwM2ZY6KP/o8A659wvOxyK236bWb/wSAAz60Homsg6Qgnh0+HTDu9zzK7+55y7xzk3xDlXQOj/6xvOuRuJ0/4eZGY9zSzr4DZwGbAar9/bfl8Y6cYLMLOAjYTmVe/1O54u7NfTQAXQSmh+8FZCc6OvA5uA14Dc8LlG6O6pzcAqoMjv+E+yz+cRmkddCSwPf82K534DpwPLwn1eDdwXbh8JLAJKgOeA9HB7Rni/JHx8pN99OIW+Xwj8PRH6G+7fivDXmoO/q7x+b6vEhIhIgkuUqSERETkKJQIRkQSnRCAikuCUCEREEpwSgYhIglMiEAkzs0C44uPBry6rUmtmBdahQqxINFGJCZGPHHDOTfE7CJHuphGByHGE68P/NFwjfpGZjQ63F5jZG+E68K+b2bBwe38z+1t47YAVZnZO+KWSzex34fUEXg0/IYyZfdNCayusNLNnfOqmJDAlApGP9Dhsaui6DsfqnHOTgP8hVBUT4L+BPznnTgeeBB4Mtz8IvO1CawdMI/SEKIRqxj/knJsI1AKfCrffDUwNv85XveqcyNHoyWKRMDPb55zr1Un7NkKLwmwJF7vb5Zzra2bVhGq/t4bbK5xzeWZWBQxxzjV3eI0CYIELLSyCmd0FpDrnfmBmrwD7gBeAF5xz+zzuqsghNCIQiYw7yvaJaO6wHeCja3RXEaoXMw1Y3KG6pki3UCIQicx1Hf78ILz9D0KVMQFuBN4Nb78O3A7ti8lkH+1FzSwJGOqcexO4i1D55CNGJSJe0icPkY/0CK8AdtArzrmDt5D2MbOVhD7V3xBu+wbwBzP7Z6AK+GK4/Q7gETO7ldAn/9sJVYjtTDLwv+FkYcCDLrTegEi30TUCkeMIXyMocs5V+x2LiBc0NSQikuA0IhARSXAaEYiIJDglAhGRBKdEICKS4JQIREQSnBKBiEiC+/8bEGH06sLNbwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkXBNakSK7Ar"
      },
      "source": [
        "### Generate new text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBs7Qo-UK0Av",
        "outputId": "ef635244-ed62-42f5-a887-548ff360dc9f"
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  # in TF 2.5.x\n",
        "  # predicted = model.predict_classes(token_list, verbose=0) \n",
        "\n",
        "  # in TF 2.6 above\n",
        "  predict_x = model.predict(token_list, verbose=0)\n",
        "  predicted=np.argmax(predict_x,axis=1)\n",
        "\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laurence went to dublin all merry and i we banished their nonsense ask brothers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmEJYHuDrl1I"
      },
      "source": [
        "## Finding what the next word should be\n",
        "\n",
        "앞선 실습에서, 우리는 단일 가사가 포함된 문자열 데이터를 통해 새 텍스트를 생성하기 위해 데이터를 준비하는 방법에 대해 알아봤다. \n",
        "\n",
        "우리는 데이터를 토큰화하고 문장의 다음 단어로 표시된 하위 문장 조각들을 만들고, 라벨을 원핫 인코딩하여 신경망을 구축할 수 있도록 했다. \n",
        "\n",
        "이로 인해 생성된 문장을 살펴보면, 상당히 많은 단어가 중복적으로 나타남을 알 수 있는데, 이는 LSTM이 context를 단순히 forward 방향으로만 이동시키기 때문이라고 할 수 있다.\n",
        "\n",
        "LSTM 부분에 Bidirectional을 추가하면 조금 더 자연스러운 문장을 얻을 수 있다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDPyOzKoNlGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54fe0c4f-5f4b-4352-fb7e-db4309afefab"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=500, verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "15/15 [==============================] - 4s 11ms/step - loss: 5.5688 - accuracy: 0.0088\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 5.5443 - accuracy: 0.0552\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 5.4949 - accuracy: 0.0508\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 5.3480 - accuracy: 0.0508\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 5.1504 - accuracy: 0.0508\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 5.0681 - accuracy: 0.0508\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 5.0311 - accuracy: 0.0508\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.9965 - accuracy: 0.0508\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.9647 - accuracy: 0.0530\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.9267 - accuracy: 0.0596\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.8799 - accuracy: 0.0640\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.8227 - accuracy: 0.0706\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.7672 - accuracy: 0.0773\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.6980 - accuracy: 0.0684\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.6373 - accuracy: 0.0751\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.5730 - accuracy: 0.0795\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.5153 - accuracy: 0.0927\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.4637 - accuracy: 0.0971\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.4139 - accuracy: 0.1060\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.3743 - accuracy: 0.1170\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.3214 - accuracy: 0.1236\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.2771 - accuracy: 0.1236\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 4.2298 - accuracy: 0.1280\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.1901 - accuracy: 0.1280\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 4.1393 - accuracy: 0.1435\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 4.0950 - accuracy: 0.1435\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.0611 - accuracy: 0.1369\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 4.0138 - accuracy: 0.1391\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 3.9655 - accuracy: 0.1479\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 3.9351 - accuracy: 0.1501\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.8936 - accuracy: 0.1744\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.8526 - accuracy: 0.1788\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.8104 - accuracy: 0.1898\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 3.7665 - accuracy: 0.1832\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 3.7213 - accuracy: 0.2009\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 3.6746 - accuracy: 0.1876\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 3.6450 - accuracy: 0.2031\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.6083 - accuracy: 0.2208\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.5571 - accuracy: 0.2274\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.5121 - accuracy: 0.2362\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.4819 - accuracy: 0.2406\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 3.4442 - accuracy: 0.2472\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.4101 - accuracy: 0.2561\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 3.3726 - accuracy: 0.2517\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.3443 - accuracy: 0.2539\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 3.3008 - accuracy: 0.2759\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.2540 - accuracy: 0.2649\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.2258 - accuracy: 0.2737\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.1986 - accuracy: 0.2870\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 3.1635 - accuracy: 0.3024\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.1355 - accuracy: 0.3024\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.0952 - accuracy: 0.3091\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.0507 - accuracy: 0.3245\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 3.0163 - accuracy: 0.3377\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 3.0034 - accuracy: 0.3510\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.9656 - accuracy: 0.3598\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.9319 - accuracy: 0.3510\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.8959 - accuracy: 0.3709\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.8645 - accuracy: 0.3753\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.8244 - accuracy: 0.3863\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.8119 - accuracy: 0.3974\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.7995 - accuracy: 0.3885\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.7556 - accuracy: 0.4084\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.7178 - accuracy: 0.4238\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.6995 - accuracy: 0.4238\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.6722 - accuracy: 0.4327\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.6321 - accuracy: 0.4547\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.6090 - accuracy: 0.4570\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.5766 - accuracy: 0.4834\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.5481 - accuracy: 0.4989\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.5232 - accuracy: 0.4857\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.4910 - accuracy: 0.5188\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.4709 - accuracy: 0.5188\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.4409 - accuracy: 0.5276\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.4113 - accuracy: 0.5320\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.3827 - accuracy: 0.5607\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.3569 - accuracy: 0.5563\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.3263 - accuracy: 0.5806\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.3307 - accuracy: 0.5740\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.3086 - accuracy: 0.5806\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.2996 - accuracy: 0.5916\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.2853 - accuracy: 0.5806\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.2612 - accuracy: 0.5894\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.2452 - accuracy: 0.5938\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.2059 - accuracy: 0.5850\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.1861 - accuracy: 0.5938\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.1552 - accuracy: 0.6026\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.1234 - accuracy: 0.6115\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.1003 - accuracy: 0.6159\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.0775 - accuracy: 0.6159\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.0519 - accuracy: 0.6336\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.0620 - accuracy: 0.6225\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.0790 - accuracy: 0.6358\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 2.0424 - accuracy: 0.6313\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 2.0015 - accuracy: 0.6446\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.9701 - accuracy: 0.6468\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.9384 - accuracy: 0.6490\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.9201 - accuracy: 0.6556\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.8977 - accuracy: 0.6534\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.8739 - accuracy: 0.6733\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.8498 - accuracy: 0.6733\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.8321 - accuracy: 0.6799\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.8134 - accuracy: 0.6755\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.7927 - accuracy: 0.6821\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.7781 - accuracy: 0.6954\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.7562 - accuracy: 0.7064\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.7419 - accuracy: 0.7152\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.7227 - accuracy: 0.7174\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.7135 - accuracy: 0.7108\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.6938 - accuracy: 0.7196\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.6758 - accuracy: 0.7285\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.6586 - accuracy: 0.7263\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.6389 - accuracy: 0.7174\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.6182 - accuracy: 0.7285\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.5943 - accuracy: 0.7395\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.5774 - accuracy: 0.7528\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.5587 - accuracy: 0.7506\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.5431 - accuracy: 0.7660\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.5275 - accuracy: 0.7572\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.5127 - accuracy: 0.7616\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.5138 - accuracy: 0.7572\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.4938 - accuracy: 0.7616\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.4834 - accuracy: 0.7594\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.4620 - accuracy: 0.7748\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.4562 - accuracy: 0.7682\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.4360 - accuracy: 0.7704\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.4216 - accuracy: 0.7660\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.4178 - accuracy: 0.7726\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.3907 - accuracy: 0.7881\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.3754 - accuracy: 0.7947\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.3642 - accuracy: 0.7925\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.3389 - accuracy: 0.8013\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.3252 - accuracy: 0.8079\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.3062 - accuracy: 0.8079\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.2923 - accuracy: 0.8057\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.2786 - accuracy: 0.8079\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.2639 - accuracy: 0.8168\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.2471 - accuracy: 0.8212\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.2341 - accuracy: 0.8366\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.2207 - accuracy: 0.8389\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.2093 - accuracy: 0.8366\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1951 - accuracy: 0.8433\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1860 - accuracy: 0.8433\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.1743 - accuracy: 0.8433\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1591 - accuracy: 0.8433\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 1.1479 - accuracy: 0.8411\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1455 - accuracy: 0.8433\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1286 - accuracy: 0.8433\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1167 - accuracy: 0.8477\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1148 - accuracy: 0.8499\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1241 - accuracy: 0.8366\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1213 - accuracy: 0.8433\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1096 - accuracy: 0.8521\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1561 - accuracy: 0.8322\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.2908 - accuracy: 0.7947\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.2626 - accuracy: 0.7881\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.1636 - accuracy: 0.8234\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1366 - accuracy: 0.8278\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.1074 - accuracy: 0.8455\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.0812 - accuracy: 0.8543\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0476 - accuracy: 0.8587\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0260 - accuracy: 0.8675\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0621 - accuracy: 0.8477\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0570 - accuracy: 0.8455\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0591 - accuracy: 0.8366\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 1.0175 - accuracy: 0.8433\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.9794 - accuracy: 0.8653\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.9657 - accuracy: 0.8720\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.9483 - accuracy: 0.8830\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.9252 - accuracy: 0.8742\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.9144 - accuracy: 0.8764\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.9282 - accuracy: 0.8852\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.8889 - accuracy: 0.8940\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.8790 - accuracy: 0.8874\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.9168 - accuracy: 0.8918\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.8857 - accuracy: 0.9007\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.8466 - accuracy: 0.8962\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.8347 - accuracy: 0.9095\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.8222 - accuracy: 0.9117\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.8106 - accuracy: 0.9117\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.8015 - accuracy: 0.9139\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7903 - accuracy: 0.9183\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.7831 - accuracy: 0.9183\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7722 - accuracy: 0.9183\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7639 - accuracy: 0.9227\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7576 - accuracy: 0.9249\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7523 - accuracy: 0.9249\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.7444 - accuracy: 0.9227\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7355 - accuracy: 0.9294\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.7280 - accuracy: 0.9360\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7206 - accuracy: 0.9316\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7155 - accuracy: 0.9338\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.7058 - accuracy: 0.9338\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6984 - accuracy: 0.9360\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6921 - accuracy: 0.9404\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6848 - accuracy: 0.9404\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6788 - accuracy: 0.9404\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6710 - accuracy: 0.9404\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6644 - accuracy: 0.9404\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6574 - accuracy: 0.9382\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6512 - accuracy: 0.9404\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6447 - accuracy: 0.9404\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6382 - accuracy: 0.9360\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6320 - accuracy: 0.9426\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6261 - accuracy: 0.9404\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6211 - accuracy: 0.9448\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6184 - accuracy: 0.9448\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.6192 - accuracy: 0.9426\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6139 - accuracy: 0.9382\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.6053 - accuracy: 0.9382\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5957 - accuracy: 0.9426\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5902 - accuracy: 0.9404\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5816 - accuracy: 0.9426\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5744 - accuracy: 0.9448\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5719 - accuracy: 0.9470\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5671 - accuracy: 0.9470\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5595 - accuracy: 0.9470\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5544 - accuracy: 0.9492\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5502 - accuracy: 0.9448\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5490 - accuracy: 0.9448\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5455 - accuracy: 0.9470\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5368 - accuracy: 0.9492\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5314 - accuracy: 0.9470\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5278 - accuracy: 0.9470\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5224 - accuracy: 0.9492\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5161 - accuracy: 0.9492\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5109 - accuracy: 0.9470\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5084 - accuracy: 0.9448\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.5035 - accuracy: 0.9492\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4981 - accuracy: 0.9470\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4919 - accuracy: 0.9470\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4880 - accuracy: 0.9514\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4846 - accuracy: 0.9492\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4782 - accuracy: 0.9470\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4741 - accuracy: 0.9492\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4701 - accuracy: 0.9492\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4652 - accuracy: 0.9470\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4610 - accuracy: 0.9470\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4576 - accuracy: 0.9492\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4530 - accuracy: 0.9514\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4493 - accuracy: 0.9492\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4448 - accuracy: 0.9492\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4415 - accuracy: 0.9514\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4374 - accuracy: 0.9514\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4334 - accuracy: 0.9492\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4304 - accuracy: 0.9514\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4274 - accuracy: 0.9514\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4232 - accuracy: 0.9514\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4182 - accuracy: 0.9514\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4151 - accuracy: 0.9536\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4117 - accuracy: 0.9536\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4091 - accuracy: 0.9536\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.4058 - accuracy: 0.9514\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4029 - accuracy: 0.9536\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3988 - accuracy: 0.9536\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3949 - accuracy: 0.9536\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3914 - accuracy: 0.9536\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3879 - accuracy: 0.9514\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3839 - accuracy: 0.9514\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3801 - accuracy: 0.9514\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3767 - accuracy: 0.9536\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3733 - accuracy: 0.9514\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3701 - accuracy: 0.9492\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3673 - accuracy: 0.9536\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3640 - accuracy: 0.9536\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3619 - accuracy: 0.9536\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3583 - accuracy: 0.9514\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3555 - accuracy: 0.9514\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3522 - accuracy: 0.9536\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3495 - accuracy: 0.9536\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3467 - accuracy: 0.9536\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3449 - accuracy: 0.9514\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3422 - accuracy: 0.9514\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3408 - accuracy: 0.9536\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3374 - accuracy: 0.9536\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3362 - accuracy: 0.9536\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3349 - accuracy: 0.9514\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3332 - accuracy: 0.9536\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3285 - accuracy: 0.9536\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3261 - accuracy: 0.9514\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3239 - accuracy: 0.9536\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3219 - accuracy: 0.9492\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3199 - accuracy: 0.9514\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3217 - accuracy: 0.9514\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3454 - accuracy: 0.9426\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3418 - accuracy: 0.9470\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3316 - accuracy: 0.9448\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3461 - accuracy: 0.9404\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3482 - accuracy: 0.9404\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3488 - accuracy: 0.9470\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3364 - accuracy: 0.9426\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3315 - accuracy: 0.9382\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3432 - accuracy: 0.9404\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3527 - accuracy: 0.9404\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3911 - accuracy: 0.9316\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3887 - accuracy: 0.9316\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3417 - accuracy: 0.9470\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3389 - accuracy: 0.9448\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3333 - accuracy: 0.9448\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3499 - accuracy: 0.9382\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3228 - accuracy: 0.9492\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3137 - accuracy: 0.9470\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3128 - accuracy: 0.9426\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3051 - accuracy: 0.9426\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2991 - accuracy: 0.9492\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3067 - accuracy: 0.9470\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3439 - accuracy: 0.9316\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3142 - accuracy: 0.9426\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3241 - accuracy: 0.9426\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3361 - accuracy: 0.9382\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.3093 - accuracy: 0.9470\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2936 - accuracy: 0.9448\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2862 - accuracy: 0.9492\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2766 - accuracy: 0.9514\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2712 - accuracy: 0.9514\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2653 - accuracy: 0.9492\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2613 - accuracy: 0.9514\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2578 - accuracy: 0.9536\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2553 - accuracy: 0.9492\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2535 - accuracy: 0.9492\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2515 - accuracy: 0.9514\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2495 - accuracy: 0.9492\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2481 - accuracy: 0.9470\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2460 - accuracy: 0.9470\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2439 - accuracy: 0.9492\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2412 - accuracy: 0.9470\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2394 - accuracy: 0.9514\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2382 - accuracy: 0.9492\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2369 - accuracy: 0.9536\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2349 - accuracy: 0.9514\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2331 - accuracy: 0.9514\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2316 - accuracy: 0.9514\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2302 - accuracy: 0.9536\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2308 - accuracy: 0.9514\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2293 - accuracy: 0.9514\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2268 - accuracy: 0.9492\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2251 - accuracy: 0.9514\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2234 - accuracy: 0.9492\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2215 - accuracy: 0.9492\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2206 - accuracy: 0.9470\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2184 - accuracy: 0.9514\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2174 - accuracy: 0.9514\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2173 - accuracy: 0.9514\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2160 - accuracy: 0.9492\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2149 - accuracy: 0.9492\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2131 - accuracy: 0.9470\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2117 - accuracy: 0.9470\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2110 - accuracy: 0.9470\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2101 - accuracy: 0.9492\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2084 - accuracy: 0.9536\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2069 - accuracy: 0.9470\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2063 - accuracy: 0.9448\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2062 - accuracy: 0.9492\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2042 - accuracy: 0.9514\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2039 - accuracy: 0.9536\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2029 - accuracy: 0.9536\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2023 - accuracy: 0.9514\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2006 - accuracy: 0.9514\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1992 - accuracy: 0.9514\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1983 - accuracy: 0.9470\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1971 - accuracy: 0.9492\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1962 - accuracy: 0.9492\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1954 - accuracy: 0.9514\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1958 - accuracy: 0.9492\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1943 - accuracy: 0.9536\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1923 - accuracy: 0.9514\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1914 - accuracy: 0.9536\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1908 - accuracy: 0.9492\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1895 - accuracy: 0.9536\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1886 - accuracy: 0.9514\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1870 - accuracy: 0.9514\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1882 - accuracy: 0.9514\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1868 - accuracy: 0.9492\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1859 - accuracy: 0.9492\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1852 - accuracy: 0.9492\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1839 - accuracy: 0.9536\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1828 - accuracy: 0.9536\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1822 - accuracy: 0.9492\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1808 - accuracy: 0.9536\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1801 - accuracy: 0.9536\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1791 - accuracy: 0.9514\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1790 - accuracy: 0.9536\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1786 - accuracy: 0.9514\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1781 - accuracy: 0.9514\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1770 - accuracy: 0.9492\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1757 - accuracy: 0.9514\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1751 - accuracy: 0.9514\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1745 - accuracy: 0.9514\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1736 - accuracy: 0.9514\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1737 - accuracy: 0.9470\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1730 - accuracy: 0.9514\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1715 - accuracy: 0.9514\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1698 - accuracy: 0.9536\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1703 - accuracy: 0.9536\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1705 - accuracy: 0.9536\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1694 - accuracy: 0.9536\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1685 - accuracy: 0.9536\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1680 - accuracy: 0.9536\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1672 - accuracy: 0.9536\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1659 - accuracy: 0.9514\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1650 - accuracy: 0.9536\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1643 - accuracy: 0.9514\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1633 - accuracy: 0.9514\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1636 - accuracy: 0.9514\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1623 - accuracy: 0.9514\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1618 - accuracy: 0.9492\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1617 - accuracy: 0.9470\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1610 - accuracy: 0.9492\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1602 - accuracy: 0.9404\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1594 - accuracy: 0.9470\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1590 - accuracy: 0.9448\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1584 - accuracy: 0.9470\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1574 - accuracy: 0.9448\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1565 - accuracy: 0.9470\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1563 - accuracy: 0.9382\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1562 - accuracy: 0.9426\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1555 - accuracy: 0.9448\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1549 - accuracy: 0.9514\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1549 - accuracy: 0.9514\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1545 - accuracy: 0.9492\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1538 - accuracy: 0.9492\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1527 - accuracy: 0.9470\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1522 - accuracy: 0.9492\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1516 - accuracy: 0.9404\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1513 - accuracy: 0.9470\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1509 - accuracy: 0.9514\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1501 - accuracy: 0.9492\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1499 - accuracy: 0.9492\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1498 - accuracy: 0.9492\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1493 - accuracy: 0.9514\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1493 - accuracy: 0.9514\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1490 - accuracy: 0.9514\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1478 - accuracy: 0.9492\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1490 - accuracy: 0.9492\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1474 - accuracy: 0.9514\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1472 - accuracy: 0.9492\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1459 - accuracy: 0.9536\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1457 - accuracy: 0.9492\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1452 - accuracy: 0.9492\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1466 - accuracy: 0.9470\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1495 - accuracy: 0.9536\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1482 - accuracy: 0.9514\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1614 - accuracy: 0.9492\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1705 - accuracy: 0.9470\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1883 - accuracy: 0.9470\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1941 - accuracy: 0.9382\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2694 - accuracy: 0.9338\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.3158 - accuracy: 0.9051\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3000 - accuracy: 0.9117\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2819 - accuracy: 0.9205\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.2614 - accuracy: 0.9227\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2240 - accuracy: 0.9360\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2009 - accuracy: 0.9404\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1920 - accuracy: 0.9382\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2262 - accuracy: 0.9338\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1993 - accuracy: 0.9360\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1864 - accuracy: 0.9448\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1639 - accuracy: 0.9492\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1572 - accuracy: 0.9514\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1490 - accuracy: 0.9536\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1471 - accuracy: 0.9492\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1454 - accuracy: 0.9492\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1474 - accuracy: 0.9514\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1690 - accuracy: 0.9404\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1638 - accuracy: 0.9470\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1641 - accuracy: 0.9470\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1580 - accuracy: 0.9426\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1494 - accuracy: 0.9448\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1458 - accuracy: 0.9470\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1437 - accuracy: 0.9492\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.1415 - accuracy: 0.9470\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1416 - accuracy: 0.9404\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1413 - accuracy: 0.9448\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1400 - accuracy: 0.9514\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1394 - accuracy: 0.9448\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1385 - accuracy: 0.9470\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1394 - accuracy: 0.9470\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1381 - accuracy: 0.9492\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1375 - accuracy: 0.9492\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1369 - accuracy: 0.9448\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1361 - accuracy: 0.9426\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1355 - accuracy: 0.9470\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1345 - accuracy: 0.9514\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1346 - accuracy: 0.9470\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1339 - accuracy: 0.9448\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1335 - accuracy: 0.9448\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1328 - accuracy: 0.9470\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1325 - accuracy: 0.9492\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1333 - accuracy: 0.9492\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1324 - accuracy: 0.9470\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1322 - accuracy: 0.9470\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1317 - accuracy: 0.9492\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1310 - accuracy: 0.9536\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1299 - accuracy: 0.9492\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1296 - accuracy: 0.9514\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1293 - accuracy: 0.9470\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1288 - accuracy: 0.9514\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.1289 - accuracy: 0.9448\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1288 - accuracy: 0.9536\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1280 - accuracy: 0.9492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOVGC47cusC3",
        "outputId": "64ba0340-c343-47bd-e990-0c1b956f0100"
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predict_x = model.predict(token_list, verbose=0)\n",
        "  predicted=np.argmax(predict_x,axis=1)\n",
        "\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laurence went to dublin leg nice left the kerrigan fainted the swore strangled youd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cemn9LxbvFbM"
      },
      "source": [
        "이전보다 훨씬 나은 문장이 생성되었음을 확인할 수 있다.\n",
        "\n",
        "여전히 반복이 있고 어색한 부분이 있지만, 이는 우리가 데이터로써 넣어준 문장이 곡 가사이기 때문이라 할 수도 있다.\n",
        "\n",
        "또한, 문장 생성은 뒤로 갈 수록, 생성된 데이터에 대한 새로운 예측이기 때문에 문장이 길어질 수록 정확도가 떨어지는 문제도 있다.\n",
        "\n",
        "위 코드에서, 사용된 데이터를 바꾸어 더 나은 문장을 생성할 수 있도록 해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1UL0uOOPg8Q"
      },
      "source": [
        "## Lab02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMpVfgVfw9gF",
        "outputId": "ed089399-2ed5-42cc-fd84-65c2a9290f64"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cCYhqL1uwaP",
        "outputId": "39d99655-50e8-4c87-f6b3-15c14ff4acd3"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "path = '/content/drive/MyDrive/data'\n",
        "data = open(f'{path}/Laurences_generated_poetry.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'and': 2, 'i': 3, 'to': 4, 'a': 5, 'of': 6, 'my': 7, 'in': 8, 'me': 9, 'for': 10, 'you': 11, 'all': 12, 'was': 13, 'she': 14, 'that': 15, 'on': 16, 'with': 17, 'her': 18, 'but': 19, 'as': 20, 'when': 21, 'love': 22, 'is': 23, 'your': 24, 'it': 25, 'will': 26, 'from': 27, 'by': 28, 'they': 29, 'be': 30, 'are': 31, 'so': 32, 'he': 33, 'old': 34, 'no': 35, 'oh': 36, 'ill': 37, 'at': 38, 'one': 39, 'his': 40, 'there': 41, 'were': 42, 'heart': 43, 'down': 44, 'now': 45, 'we': 46, 'where': 47, 'young': 48, 'never': 49, 'go': 50, 'come': 51, 'then': 52, 'did': 53, 'not': 54, 'said': 55, 'away': 56, 'their': 57, 'sweet': 58, 'them': 59, 'green': 60, 'if': 61, 'take': 62, 'our': 63, 'like': 64, 'night': 65, 'day': 66, 'o': 67, 'out': 68, 'fair': 69, 'this': 70, 'town': 71, 'have': 72, 'can': 73, 'true': 74, 'its': 75, 'thou': 76, 'see': 77, 'dear': 78, 'more': 79, 'theres': 80, 'or': 81, 'had': 82, 'would': 83, 'over': 84, 'hear': 85, 'up': 86, 'ive': 87, 'through': 88, 'home': 89, 'again': 90, 'well': 91, 'oer': 92, 'land': 93, 'good': 94, 'im': 95, 'ye': 96, 'sea': 97, 'left': 98, 'still': 99, 'father': 100, 'long': 101, 'rose': 102, 'could': 103, 'morning': 104, 'wild': 105, 'who': 106, 'eyes': 107, 'came': 108, 'while': 109, 'too': 110, 'back': 111, 'little': 112, 'an': 113, 'took': 114, 'him': 115, 'bow': 116, 'first': 117, 'let': 118, 'man': 119, 'shall': 120, 'know': 121, 'get': 122, 'high': 123, 'gone': 124, 'say': 125, 'ever': 126, 'some': 127, 'mary': 128, 'hand': 129, 'till': 130, 'put': 131, 'own': 132, 'time': 133, 'heard': 134, 'dead': 135, 'may': 136, 'bright': 137, 'mountain': 138, 'early': 139, 'rosin': 140, 'gave': 141, 'thee': 142, 'only': 143, 'far': 144, 'maid': 145, 'must': 146, 'find': 147, 'girl': 148, 'sure': 149, 'round': 150, 'dublin': 151, 'once': 152, 'world': 153, 'delight': 154, 'last': 155, 'johnny': 156, 'seen': 157, 'has': 158, 'fine': 159, 'road': 160, 'mother': 161, 'tis': 162, 'what': 163, 'way': 164, 'moon': 165, 'soul': 166, 'neer': 167, 'id': 168, 'just': 169, 'thats': 170, 'days': 171, 'darling': 172, 'went': 173, 'white': 174, 'die': 175, 'than': 176, 'hair': 177, 'goes': 178, 'meet': 179, 'today': 180, 'do': 181, 'girls': 182, 'shes': 183, 'thyme': 184, 'thy': 185, 'sing': 186, 'pretty': 187, 'new': 188, 'poor': 189, 'into': 190, 'life': 191, 'irish': 192, 'give': 193, 'boy': 194, 'youre': 195, 'make': 196, 'passed': 197, 'lovely': 198, 'black': 199, 'youll': 200, 'died': 201, 'red': 202, 'smile': 203, 'keep': 204, 'loves': 205, 'free': 206, 'leave': 207, 'friends': 208, 'each': 209, 'saw': 210, 'behind': 211, 'song': 212, 'ra': 213, 'dont': 214, 'arms': 215, 'am': 216, 'sun': 217, 'saying': 218, 'made': 219, 'wish': 220, 'cold': 221, 'met': 222, 'before': 223, 'should': 224, 'rocky': 225, 'light': 226, 'wid': 227, 'boys': 228, 'best': 229, 'fields': 230, 'since': 231, 'ball': 232, 'water': 233, 'casey': 234, 'mind': 235, 'along': 236, 'loved': 237, 'place': 238, 'ireland': 239, 'next': 240, 'three': 241, 'many': 242, 'years': 243, 'door': 244, 'us': 245, 'drink': 246, 'got': 247, 'might': 248, 'live': 249, 'roses': 250, 'play': 251, 'soon': 252, 'ground': 253, 'times': 254, 'spent': 255, 'going': 256, 'tree': 257, 'barley': 258, 'grass': 259, 'kind': 260, 'twas': 261, 'bridge': 262, 'around': 263, 'blue': 264, 'tell': 265, 'row': 266, 'how': 267, 'money': 268, 'merry': 269, 'stepped': 270, 'corporal': 271, 'always': 272, 'though': 273, 'near': 274, 'taken': 275, 'ones': 276, 'daughter': 277, 'forever': 278, 'loo': 279, 'shining': 280, 'plenty': 281, 'hes': 282, 'ship': 283, 'banks': 284, 'think': 285, 'very': 286, 'stand': 287, 'heres': 288, 'snow': 289, 'mountains': 290, 'molly': 291, 'wheel': 292, 'street': 293, 'erin': 294, 'side': 295, 'feet': 296, 'star': 297, 'look': 298, 'brave': 299, 'woman': 300, 'sons': 301, 'two': 302, 'says': 303, 'asked': 304, 'lanigans': 305, 'singing': 306, 'men': 307, 'toome': 308, 'stole': 309, 'god': 310, 'hill': 311, 'lonely': 312, 'lover': 313, 'tears': 314, 'fathers': 315, 'low': 316, 'voice': 317, 'quite': 318, 'able': 319, 'nice': 320, 'laid': 321, 'comrades': 322, 'wind': 323, 'another': 324, 'sit': 325, 'face': 326, 'band': 327, 'call': 328, 'colleen': 329, 'until': 330, 'hills': 331, 'mine': 332, 'above': 333, 'upon': 334, 'eer': 335, 'youve': 336, 'fly': 337, 'been': 338, 'late': 339, 'alive': 340, 'ballyjamesduff': 341, 'looked': 342, 'great': 343, 'why': 344, 'every': 345, 'proud': 346, 'found': 347, 'bragh': 348, 'such': 349, 'birds': 350, 'wedding': 351, 'welcome': 352, 'dancing': 353, 'da': 354, 'fell': 355, 'thinking': 356, 'roddy': 357, 'mccorley': 358, 'smiling': 359, 'mallow': 360, 'blooming': 361, 'thought': 362, 'peace': 363, 'soft': 364, 'pure': 365, 'harp': 366, 'dream': 367, 'alas': 368, 'yet': 369, 'clear': 370, 'art': 371, 'off': 372, 'hope': 373, 'fought': 374, 'mothers': 375, 'shore': 376, 'ago': 377, 'fol': 378, 'de': 379, 'house': 380, 'married': 381, 'bound': 382, 'danced': 383, 'devil': 384, 'dawning': 385, 'makes': 386, 'same': 387, 'sat': 388, 'any': 389, 'glass': 390, 'gay': 391, 'relations': 392, 'evening': 393, 'watched': 394, 'right': 395, 'fellows': 396, 'whiskey': 397, 'bonnie': 398, 'grows': 399, 'women': 400, 'flowers': 401, 'beauty': 402, 'cannot': 403, 'handsome': 404, 'happy': 405, 'gold': 406, 'rover': 407, 'none': 408, 'doneen': 409, 'summers': 410, 'people': 411, 'set': 412, 'paddy': 413, 'morn': 414, 'most': 415, 'easy': 416, 'struck': 417, 'beautiful': 418, 'those': 419, 'golden': 420, 'run': 421, 'pipes': 422, 'glen': 423, 'dying': 424, 'here': 425, 'wall': 426, 'across': 427, 'fire': 428, 'eileen': 429, 'longer': 430, 'cheeks': 431, 'valley': 432, 'both': 433, 'dew': 434, 'care': 435, 'bride': 436, 'nothing': 437, 'wont': 438, 'theyre': 439, 'colonel': 440, 'maiden': 441, 'shed': 442, 'til': 443, 'brown': 444, 'breast': 445, 'corn': 446, 'sinking': 447, 'began': 448, 'name': 449, 'cruel': 450, 'sound': 451, 'spancil': 452, 'county': 453, 'lies': 454, 'color': 455, 'thing': 456, 'decay': 457, 'sleep': 458, 'hours': 459, 'loving': 460, 'weary': 461, 'ringing': 462, 'please': 463, 'forget': 464, 'lie': 465, 'ran': 466, 'tore': 467, 'country': 468, 'fear': 469, 'fortune': 470, 'kissed': 471, 'alone': 472, 'ould': 473, 'cry': 474, 'dreams': 475, 'used': 476, 'horse': 477, 'break': 478, 'bells': 479, 'didnt': 480, 'weeks': 481, 'without': 482, 'raw': 483, 'nor': 484, 'twenty': 485, 'tune': 486, 'hed': 487, 'roving': 488, 'leaves': 489, 'cant': 490, 'death': 491, 'ten': 492, 'prison': 493, 'judge': 494, 'against': 495, 'lads': 496, 'shell': 497, 'fill': 498, 'valleys': 499, 'other': 500, 'pale': 501, 'joy': 502, 'wide': 503, 'bring': 504, 'ah': 505, 'cliffs': 506, 'city': 507, 'end': 508, 'turn': 509, 'sky': 510, 'born': 511, 'knew': 512, 'smiled': 513, 'rosie': 514, 'comes': 515, 'sayin': 516, 'lord': 517, 'dungannon': 518, 'blood': 519, 'air': 520, 'danny': 521, 'calling': 522, 'sunshine': 523, 'spring': 524, 'bid': 525, 'grow': 526, 'truth': 527, 'tear': 528, 'rings': 529, 'guns': 530, 'bay': 531, 'oflynn': 532, 'och': 533, 'stick': 534, 'rest': 535, 'four': 536, 'jewel': 537, 'tried': 538, 'grief': 539, 'answer': 540, 'kathleen': 541, 'fond': 542, 'eye': 543, 'goin': 544, 'pistols': 545, 'musha': 546, 'whack': 547, 'creole': 548, 'together': 549, 'room': 550, 'fall': 551, 'swore': 552, 'being': 553, 'step': 554, 'lark': 555, 'cailã\\xadn': 556, 'deas': 557, 'crãºite': 558, 'na': 559, 'mbã³': 560, 'sir': 561, 'isle': 562, 'waiting': 563, 'magic': 564, 'skibbereen': 565, 'loud': 566, 'raise': 567, 'bent': 568, 'aged': 569, 'summer': 570, 'jenny': 571, 'excise': 572, 'rigadoo': 573, 'auld': 574, 'hearts': 575, 'nay': 576, 'stool': 577, 'farrell': 578, 'garden': 579, 'precious': 580, 'child': 581, 'slumber': 582, 'sleeping': 583, 'watch': 584, 'gently': 585, 'minstrel': 586, 'praise': 587, 'bell': 588, 'shaken': 589, 'immortal': 590, 'pray': 591, 'stay': 592, 'spoke': 593, 'cross': 594, 'brothers': 595, 'much': 596, 'past': 597, 'killarney': 598, 'sang': 599, 'tones': 600, 'ral': 601, 'wander': 602, 'cot': 603, 'feel': 604, 'yore': 605, 'answered': 606, 'divil': 607, 'middle': 608, 'bit': 609, 'led': 610, 'soldiers': 611, 'lily': 612, 'bed': 613, 'lassie': 614, 'clothes': 615, 'return': 616, 'broken': 617, 'derry': 618, 'sighed': 619, 'english': 620, 'tomorrow': 621, 'souls': 622, 'van': 623, 'diemans': 624, 'law': 625, 'neither': 626, 'winds': 627, 'rather': 628, 'doesnt': 629, 'rosy': 630, 'neatest': 631, 'hands': 632, 'whereon': 633, 'stands': 634, 'write': 635, 'thousand': 636, 'fare': 637, 'youd': 638, 'velvet': 639, 'neat': 640, 'landed': 641, 'health': 642, 'kellswater': 643, 'quiet': 644, 'stars': 645, 'beside': 646, 'warm': 647, 'sunday': 648, 'grey': 649, 'ocean': 650, 'sad': 651, 'spend': 652, 'kilkenny': 653, 'silver': 654, 'view': 655, 'west': 656, 'plain': 657, 'barrow': 658, 'broad': 659, 'narrow': 660, 'crying': 661, 'wonder': 662, 'save': 663, 'stop': 664, 'tender': 665, 'told': 666, 'lip': 667, 'dance': 668, 'foot': 669, 'kilrain': 670, 'saint': 671, 'visit': 672, 'mossy': 673, 'wexford': 674, 'irishmen': 675, 'shadow': 676, 'tho': 677, 'salley': 678, 'gardens': 679, 'foolish': 680, 'youth': 681, 'fade': 682, 'war': 683, 'believe': 684, 'which': 685, 'change': 686, 'entwine': 687, 'turns': 688, 'turned': 689, 'crown': 690, 'played': 691, 'captain': 692, 'blow': 693, 'children': 694, 'slainte': 695, 'gentle': 696, 'heavens': 697, 'bloom': 698, 'grand': 699, 'bush': 700, 'nest': 701, 'rich': 702, 'parting': 703, 'better': 704, 'window': 705, 'haste': 706, 'fresh': 707, 'stream': 708, 'rays': 709, 'ma': 710, 'ring': 711, 'lad': 712, 'athy': 713, 'drop': 714, 'hardly': 715, 'done': 716, 'arm': 717, 'leg': 718, 'beg': 719, 'drew': 720, 'bold': 721, 'drawn': 722, 'jail': 723, 'writin': 724, 'farewell': 725, 'tired': 726, 'lake': 727, 'want': 728, 'ringlets': 729, 'myself': 730, 'songs': 731, 'reel': 732, 'steps': 733, 'hearty': 734, 'fainted': 735, 'called': 736, 'under': 737, 'toe': 738, 'mairi': 739, 'fairest': 740, 'darlin': 741, 'bird': 742, 'memory': 743, 'lips': 744, 'sweetly': 745, 'morrow': 746, 'consent': 747, 'else': 748, 'sold': 749, 'stout': 750, 'pair': 751, 'drinking': 752, 'meself': 753, 'fray': 754, 'pike': 755, 'coat': 756, 'beneath': 757, 'rent': 758, 'part': 759, 'half': 760, 'head': 761, 'friend': 762, 'standing': 763, 'floor': 764, 'bare': 765, 'wed': 766, 'son': 767, 'pride': 768, 'vision': 769, 'sword': 770, 'after': 771, 'won': 772, 'farmers': 773, 'flower': 774, 'nut': 775, 'surely': 776, 'stood': 777, 'wandered': 778, 'athenry': 779, 'rising': 780, 'beating': 781, 'form': 782, 'dhu': 783, 'buy': 784, 'laughter': 785, 'wear': 786, 'raking': 787, 'rakes': 788, 'claret': 789, 'shure': 790, 'tralee': 791, 'slower': 792, 'lower': 793, 'deep': 794, 'wearin': 795, 'duram': 796, 'takes': 797, 'beware': 798, 'steal': 799, 'brings': 800, 'things': 801, 'joys': 802, 'bunch': 803, 'sailor': 804, 'chanced': 805, 'pass': 806, 'angels': 807, 'send': 808, 'drowsy': 809, 'keeping': 810, 'spirit': 811, 'stealing': 812, 'feeling': 813, 'roam': 814, 'presence': 815, 'heavenward': 816, 'dust': 817, 'dim': 818, 'journey': 819, 'waves': 820, 'frightened': 821, 'leaving': 822, 'struggle': 823, 'parents': 824, 'courage': 825, 'weeping': 826, 'pain': 827, 'mist': 828, 'felt': 829, 'roared': 830, 'making': 831, 'fever': 832, 'moment': 833, 'distance': 834, 'wailing': 835, 'oft': 836, 'held': 837, 'fast': 838, 'cabin': 839, 'honey': 840, 'diddle': 841, 'clearly': 842, 'open': 843, 'opened': 844, 'table': 845, 'wine': 846, 'lay': 847, 'shells': 848, 'sailed': 849, 'drown': 850, 'fetters': 851, 'chains': 852, 'wives': 853, 'sorrow': 854, 'thoughts': 855, 'cursed': 856, 'hell': 857, 'five': 858, 'buried': 859, 'lost': 860, 'endless': 861, 'slavery': 862, 'gun': 863, 'rain': 864, 'cares': 865, 'ghosts': 866, 'runaway': 867, 'twill': 868, 'month': 869, 'meadows': 870, 'prettiest': 871, 'winters': 872, 'satisfied': 873, 'few': 874, 'short': 875, 'lines': 876, 'shone': 877, 'shoulder': 878, 'belfast': 879, 'trade': 880, 'bad': 881, 'caused': 882, 'stray': 883, 'meaning': 884, 'damsel': 885, 'appear': 886, 'seven': 887, 'sentence': 888, 'jolly': 889, 'whenever': 890, 'wee': 891, 'wife': 892, 'lives': 893, 'martha': 894, 'courted': 895, 'bridgit': 896, 'omalley': 897, 'desolation': 898, 'thorn': 899, 'gaze': 900, 'stone': 901, 'approaching': 902, 'sets': 903, 'carrigfergus': 904, 'nights': 905, 'swim': 906, 'wings': 907, 'sober': 908, 'travel': 909, 'native': 910, 'places': 911, 'slopes': 912, 'hares': 913, 'lofty': 914, 'malone': 915, 'wheeled': 916, 'streets': 917, 'enough': 918, 'reilly': 919, 'tough': 920, 'whispers': 921, 'phil': 922, 'threw': 923, 'straight': 924, 'belles': 925, 'moor': 926, 'brand': 927, 'shapes': 928, 'work': 929, 'vow': 930, 'blarney': 931, 'paid': 932, 'bower': 933, 'remain': 934, 'charming': 935, 'storied': 936, 'chieftains': 937, 'slaughter': 938, 'bann': 939, 'boyne': 940, 'liffey': 941, 'gallant': 942, 'awake': 943, 'greet': 944, 'meadow': 945, 'sweeter': 946, 'dirty': 947, 'cats': 948, 'crossed': 949, 'field': 950, 'river': 951, 'full': 952, 'aroon': 953, 'sends': 954, 'woe': 955, 'chain': 956, 'main': 957, 'charms': 958, 'fondly': 959, 'fleet': 960, 'fairy': 961, 'thine': 962, 'known': 963, 'truly': 964, 'close': 965, 'story': 966, 'flag': 967, 'sweetest': 968, 'honor': 969, 'playing': 970, 'mauser': 971, 'music': 972, 'tom': 973, 'hurrah': 974, 'big': 975, 'lead': 976, 'south': 977, 'generation': 978, 'freedom': 979, 'agin': 980, 'creature': 981, 'dad': 982, 'venture': 983, 'word': 984, 'wonderful': 985, 'crazy': 986, 'lazy': 987, 'grave': 988, 'jest': 989, 'remark': 990, 'strangers': 991, 'strong': 992, 'shook': 993, 'walk': 994, 'north': 995, 'ours': 996, 'cease': 997, 'strife': 998, 'whats': 999, 'lilacs': 1000, 'prove': 1001, 'sweetheart': 1002, 'letters': 1003, 'sent': 1004, 'speak': 1005, 'brow': 1006, 'albert': 1007, 'mooney': 1008, 'fighting': 1009, 'fingers': 1010, 'toes': 1011, 'john': 1012, 'hurroo': 1013, 'drums': 1014, 'beguiled': 1015, 'carry': 1016, 'bone': 1017, 'havent': 1018, 'walkin': 1019, 'kilgary': 1020, 'pepper': 1021, 'countin': 1022, 'forth': 1023, 'deliver': 1024, 'daddy': 1025, 'em': 1026, 'deceive': 1027, 'between': 1028, 'even': 1029, 'prisoner': 1030, 'fists': 1031, 'knocked': 1032, 'carriages': 1033, 'rollin': 1034, 'juice': 1035, 'courtin': 1036, 'ponchartrain': 1037, 'does': 1038, 'stranger': 1039, 'marry': 1040, 'adieu': 1041, 'ask': 1042, 'tipped': 1043, 'arrived': 1044, 'ladies': 1045, 'potatoes': 1046, 'courting': 1047, 'miss': 1048, 'small': 1049, 'ned': 1050, 'ribbons': 1051, 'heel': 1052, 'bonny': 1053, 'pipe': 1054, 'thrush': 1055, 'sweethearts': 1056, 'unto': 1057, 'rise': 1058, 'softly': 1059, 'milking': 1060, 'rare': 1061, 'pity': 1062, 'treasure': 1063, 'noon': 1064, 'sailing': 1065, 'banish': 1066, 'riches': 1067, 'comfort': 1068, 'yonder': 1069, 'flows': 1070, 'fairer': 1071, 'lass': 1072, 'woods': 1073, 'strayed': 1074, 'locks': 1075, 'breaking': 1076, 'june': 1077, 'started': 1078, 'hearted': 1079, 'beer': 1080, 'daylight': 1081, 'among': 1082, 'bundle': 1083, 'connaught': 1084, 'quay': 1085, 'erins': 1086, 'galway': 1087, 'fearless': 1088, 'bravely': 1089, 'marches': 1090, 'fate': 1091, 'neck': 1092, 'trod': 1093, 'marched': 1094, 'antrim': 1095, 'sash': 1096, 'flashed': 1097, 'hath': 1098, 'foemans': 1099, 'fight': 1100, 'heavy': 1101, 'bore': 1102, 'mans': 1103, 'counter': 1104, 'dozen': 1105, 'gallon': 1106, 'bottles': 1107, 'diamond': 1108, 'resemble': 1109, 'tiny': 1110, 'friendly': 1111, 'weather': 1112, 'inside': 1113, 'remember': 1114, 'someone': 1115, 'hat': 1116, 'body': 1117, 'dancers': 1118, 'hanging': 1119, 'empty': 1120, 'shoes': 1121, 'broke': 1122, 'december': 1123, 'move': 1124, 'reason': 1125, 'roof': 1126, 'naught': 1127, 'tower': 1128, 'power': 1129, 'king': 1130, 'dreaming': 1131, 'crew': 1132, 'whos': 1133, 'mccann': 1134, 'smoke': 1135, 'notes': 1136, 'yeoman': 1137, 'cavalry': 1138, 'guard': 1139, 'forced': 1140, 'brother': 1141, 'cousin': 1142, 'blame': 1143, 'croppy': 1144, 'dressed': 1145, 'trees': 1146, 'wore': 1147, 'words': 1148, 'swiftly': 1149, 'dawn': 1150, 'lovd': 1151, 'voices': 1152, 'moaning': 1153, 'dark': 1154, 'gather': 1155, 'tay': 1156, 'swinging': 1157, 'drinkin': 1158, 'sitting': 1159, 'stile': 1160, 'springing': 1161, 'yours': 1162, 'kept': 1163, 'aisey': 1164, 'rub': 1165, 'dub': 1166, 'dow': 1167, 'shelah': 1168, 'fairly': 1169, 'beggarman': 1170, 'begging': 1171, 'slept': 1172, 'holes': 1173, 'coming': 1174, 'thru': 1175, 'boo': 1176, 'lady': 1177, 'kerry': 1178, 'pipers': 1179, 'laugh': 1180, 'beaming': 1181, 'guineas': 1182, 'least': 1183, 'diggin': 1184, 'mourne': 1185, 'spending': 1186, 'mellow': 1187, 'plying': 1188, 'slowly': 1189, 'mooncoin': 1190, 'flow': 1191, 'sounds': 1192, 'shine': 1193, 'cool': 1194, 'crystal': 1195, 'fountain': 1196, 'moonlight': 1197, 'grandmother': 1198, 'crooning': 1199, 'merrily': 1200, 'spins': 1201, 'lightly': 1202, 'moving': 1203, 'lattice': 1204, 'grove': 1205, 'swings': 1206, 'finger': 1207, 'shamrock': 1208, 'pocket': 1209, 'springtime': 1210, 'gilgarra': 1211, 'rapier': 1212, 'ringum': 1213, 'mornin': 1214, 'heather': 1215, 'build': 1216, 'maidens': 1217, 'prime': 1218, 'nlyme': 1219, 'flavours': 1220, 'lusty': 1221, 'reminded': 1222, 'attend': 1223, 'guardian': 1224, 'creeping': 1225, 'dale': 1226, 'vigil': 1227, 'visions': 1228, 'revealing': 1229, 'breathes': 1230, 'holy': 1231, 'strains': 1232, 'hover': 1233, 'hark': 1234, 'solemn': 1235, 'winging': 1236, 'earthly': 1237, 'shalt': 1238, 'awaken': 1239, 'destiny': 1240, 'emigrants': 1241, 'amid': 1242, 'longing': 1243, 'parted': 1244, 'townland': 1245, 'vessel': 1246, 'crowded': 1247, 'disquieted': 1248, 'folk': 1249, 'escape': 1250, 'hardship': 1251, 'sustaining': 1252, 'glimpse': 1253, 'faded': 1254, 'strangely': 1255, 'seas': 1256, 'anger': 1257, 'desperate': 1258, 'plight': 1259, 'worsened': 1260, 'delirium': 1261, 'possessed': 1262, 'clouded': 1263, 'prayers': 1264, 'begged': 1265, 'forgiveness': 1266, 'seeking': 1267, 'distant': 1268, 'mither': 1269, 'simple': 1270, 'ditty': 1271, 'ld': 1272, 'li': 1273, 'hush': 1274, 'lullaby': 1275, 'huggin': 1276, 'hummin': 1277, 'rock': 1278, 'asleep': 1279, 'outside': 1280, 'modestly': 1281, 'ry': 1282, 'ay': 1283, 'di': 1284, 're': 1285, 'dai': 1286, 'rie': 1287, 'shc': 1288, 'bridle': 1289, 'stable': 1290, 'oats': 1291, 'eat': 1292, 'soldier': 1293, 'aisy': 1294, 'arose': 1295, 'christmas': 1296, '1803': 1297, 'australia': 1298, 'marks': 1299, 'carried': 1300, 'rusty': 1301, 'iron': 1302, 'wains': 1303, 'mainsails': 1304, 'unfurled': 1305, 'curses': 1306, 'hurled': 1307, 'swell': 1308, 'moth': 1309, 'firelights': 1310, 'horses': 1311, 'rode': 1312, 'taking': 1313, 'hades': 1314, 'twilight': 1315, 'forty': 1316, 'slime': 1317, 'climate': 1318, 'bravery': 1319, 'ended': 1320, 'bond': 1321, 'rebel': 1322, 'iii': 1323, 'violin': 1324, 'clay': 1325, 'sooner': 1326, 'sport': 1327, 'colour': 1328, 'knows': 1329, 'earth': 1330, 'serve': 1331, 'clyde': 1332, 'mourn': 1333, 'weep': 1334, 'suffer': 1335, 'diamonds': 1336, 'queen': 1337, 'hung': 1338, 'tied': 1339, 'apprenticed': 1340, 'happiness': 1341, 'misfortune': 1342, 'follow': 1343, 'strolling': 1344, 'selling': 1345, 'bar': 1346, 'customer': 1347, 'slipped': 1348, 'luck': 1349, 'jury': 1350, 'trial': 1351, 'case': 1352, 'warning': 1353, 'liquor': 1354, 'porter': 1355, 'pleasures': 1356, 'fishing': 1357, 'farming': 1358, 'glens': 1359, 'softest': 1360, 'dripping': 1361, 'snare': 1362, 'lose': 1363, 'court': 1364, 'primrose': 1365, 'bee': 1366, 'hopeless': 1367, 'wonders': 1368, 'admiration': 1369, 'haunt': 1370, 'wherever': 1371, 'sands': 1372, 'purer': 1373, 'within': 1374, 'grieve': 1375, 'drumslieve': 1376, 'ballygrant': 1377, 'deepest': 1378, 'boatsman': 1379, 'ferry': 1380, 'childhood': 1381, 'reflections': 1382, 'boyhood': 1383, 'melting': 1384, 'roaming': 1385, 'reported': 1386, 'marble': 1387, 'stones': 1388, 'ink': 1389, 'support': 1390, 'drunk': 1391, 'seldom': 1392, 'sick': 1393, 'numbered': 1394, 'foam': 1395, 'compare': 1396, 'sights': 1397, 'coast': 1398, 'clare': 1399, 'kilkee': 1400, 'kilrush': 1401, 'watching': 1402, 'pheasants': 1403, 'homes': 1404, 'streams': 1405, 'dublins': 1406, 'cockles': 1407, 'mussels': 1408, 'fish': 1409, 'monger': 1410, 'ghost': 1411, 'wheels': 1412, 'eden': 1413, 'vanished': 1414, 'finea': 1415, 'halfway': 1416, 'cootehill': 1417, 'gruff': 1418, 'whispering': 1419, 'crow': 1420, 'newborn': 1421, 'babies': 1422, 'huff': 1423, 'start': 1424, 'sorrowful': 1425, 'squall': 1426, 'babys': 1427, 'toil': 1428, 'worn': 1429, 'fore': 1430, 'flute': 1431, 'yer': 1432, 'boot': 1433, 'magee': 1434, 'scruff': 1435, 'slanderin': 1436, 'marchin': 1437, 'assisted': 1438, 'drain': 1439, 'dudeen': 1440, 'puff': 1441, 'whisperings': 1442, 'barrin': 1443, 'chocolate': 1444, 'feegee': 1445, 'sort': 1446, 'moonshiny': 1447, 'stuff': 1448, 'addle': 1449, 'brain': 1450, 'ringin': 1451, 'glamour': 1452, 'gas': 1453, 'guff': 1454, 'whisper': 1455, 'oil': 1456, 'remarkable': 1457, 'policeman': 1458, 'bluff': 1459, 'maintain': 1460, 'guril': 1461, 'sic': 1462, 'passage': 1463, 'rough': 1464, 'borne': 1465, 'breeze': 1466, 'boundless': 1467, 'stupendous': 1468, 'roll': 1469, 'thundering': 1470, 'motion': 1471, 'mermaids': 1472, 'fierce': 1473, 'tempest': 1474, 'gathers': 1475, 'oneill': 1476, 'odonnell': 1477, 'lucan': 1478, 'oconnell': 1479, 'brian': 1480, 'drove': 1481, 'danes': 1482, 'patrick': 1483, 'vermin': 1484, 'whose': 1485, 'benburb': 1486, 'blackwater': 1487, 'owen': 1488, 'roe': 1489, 'munroe': 1490, 'lambs': 1491, 'skip': 1492, 'views': 1493, 'enchanting': 1494, 'rostrevor': 1495, 'groves': 1496, 'lakes': 1497, 'ride': 1498, 'tide': 1499, 'majestic': 1500, 'shannon': 1501, 'sail': 1502, 'loch': 1503, 'neagh': 1504, 'ross': 1505, 'gorey': 1506, 'saxon': 1507, 'tory': 1508, 'soil': 1509, 'sanctified': 1510, 'enemies': 1511, 'links': 1512, 'encumbered': 1513, 'resound': 1514, 'hosannahs': 1515, 'bide': 1516, 'hushed': 1517, 'lying': 1518, 'kneel': 1519, 'ave': 1520, 'tread': 1521, 'fail': 1522, 'simply': 1523, 'gasworks': 1524, 'croft': 1525, 'dreamed': 1526, 'canal': 1527, 'factory': 1528, 'clouds': 1529, 'drifting': 1530, 'prowling': 1531, 'beat': 1532, 'springs': 1533, 'siren': 1534, 'docks': 1535, 'train': 1536, 'smelled': 1537, 'smokey': 1538, 'sharp': 1539, 'axe': 1540, 'steel': 1541, 'tempered': 1542, 'chop': 1543, 't': 1544, 'agree': 1545, 'leaning': 1546, 'weirs': 1547, 'ray': 1548, 'glow': 1549, 'changeless': 1550, 'constant': 1551, 'bounding': 1552, 'castles': 1553, 'sacked': 1554, 'scattered': 1555, 'fixed': 1556, 'endearing': 1557, 'gifts': 1558, 'fading': 1559, 'wouldst': 1560, 'adored': 1561, 'loveliness': 1562, 'ruin': 1563, 'itself': 1564, 'verdantly': 1565, 'unprofaned': 1566, 'fervor': 1567, 'faith': 1568, 'forgets': 1569, 'sunflower': 1570, 'rag': 1571, 'games': 1572, 'hold': 1573, 'defend': 1574, 'veteran': 1575, 'volunteers': 1576, 'pat': 1577, 'pearse': 1578, 'clark': 1579, 'macdonagh': 1580, 'macdiarmada': 1581, 'mcbryde': 1582, 'james': 1583, 'connolly': 1584, 'placed': 1585, 'machine': 1586, 'ranting': 1587, 'hour': 1588, 'bullet': 1589, 'stuck': 1590, 'craw': 1591, 'poisoning': 1592, 'ceannt': 1593, 'lions': 1594, 'union': 1595, 'poured': 1596, 'dismay': 1597, 'horror': 1598, 'englishmen': 1599, 'khaki': 1600, 'renown': 1601, 'fame': 1602, 'forefathers': 1603, 'blaze': 1604, 'priests': 1605, 'offer': 1606, 'charmin': 1607, 'variety': 1608, 'renownd': 1609, 'learnin': 1610, 'piety': 1611, 'advance': 1612, 'widout': 1613, 'impropriety': 1614, 'flowr': 1615, 'cho': 1616, 'powrfulest': 1617, 'preacher': 1618, 'tenderest': 1619, 'teacher': 1620, 'kindliest': 1621, 'donegal': 1622, 'talk': 1623, 'provost': 1624, 'trinity': 1625, 'famous': 1626, 'greek': 1627, 'latinity': 1628, 'divils': 1629, 'divinity': 1630, 'd': 1631, 'likes': 1632, 'logic': 1633, 'mythology': 1634, 'thayology': 1635, 'conchology': 1636, 'sinners': 1637, 'wishful': 1638, 'childer': 1639, 'avick': 1640, 'gad': 1641, 'flock': 1642, 'grandest': 1643, 'control': 1644, 'checking': 1645, 'coaxin': 1646, 'onaisy': 1647, 'lifting': 1648, 'avoidin': 1649, 'frivolity': 1650, 'seasons': 1651, 'innocent': 1652, 'jollity': 1653, 'playboy': 1654, 'claim': 1655, 'equality': 1656, 'comicality': 1657, 'bishop': 1658, 'lave': 1659, 'gaiety': 1660, 'laity': 1661, 'clergy': 1662, 'jewels': 1663, 'plundering': 1664, 'pillage': 1665, 'starved': 1666, 'cries': 1667, 'thems': 1668, 'bondage': 1669, 'fourth': 1670, 'tabhair': 1671, 'dom': 1672, 'lã¡mh': 1673, 'harmony': 1674, 'east': 1675, 'destroy': 1676, 'command': 1677, 'gesture': 1678, 'troubles': 1679, 'weak': 1680, 'peoples': 1681, 'creeds': 1682, 'lets': 1683, 'needs': 1684, 'passion': 1685, 'fashion': 1686, 'guide': 1687, 'share': 1688, 'sparkling': 1689, 'meeting': 1690, 'iull': 1691, 'contented': 1692, 'ache': 1693, 'painful': 1694, 'wrote': 1695, 'twisted': 1696, 'twined': 1697, 'cheek': 1698, 'bedim': 1699, 'holds': 1700, 'smiles': 1701, 'scarcely': 1702, 'darkning': 1703, 'beyond': 1704, 'yearn': 1705, 'laughs': 1706, 'humble': 1707, 'brightest': 1708, 'gleam': 1709, 'forgot': 1710, 'pulled': 1711, 'comb': 1712, 'counting': 1713, 'knock': 1714, 'murray': 1715, 'fellow': 1716, 'hail': 1717, 'tumblin': 1718, 'apple': 1719, 'pie': 1720, 'gets': 1721, 'doleful': 1722, 'enemy': 1723, 'nearly': 1724, 'slew': 1725, 'queer': 1726, 'mild': 1727, 'legs': 1728, 'indeed': 1729, 'island': 1730, 'sulloon': 1731, 'flesh': 1732, 'yere': 1733, 'armless': 1734, 'boneless': 1735, 'chickenless': 1736, 'egg': 1737, 'yell': 1738, 'bowl': 1739, 'rolling': 1740, 'swearing': 1741, 'rattled': 1742, 'saber': 1743, 'deceiver': 1744, 'rig': 1745, 'um': 1746, 'du': 1747, 'rum': 1748, 'jar': 1749, 'shinin': 1750, 'coins': 1751, 'promised': 1752, 'vowed': 1753, 'devils': 1754, 'awakened': 1755, 'six': 1756, 'guards': 1757, 'numbers': 1758, 'odd': 1759, 'flew': 1760, 'mistaken': 1761, 'mollys': 1762, 'robbing': 1763, 'sentry': 1764, 'sligo': 1765, 'fishin': 1766, 'bowlin': 1767, 'others': 1768, 'railroad': 1769, 'ties': 1770, 'crossings': 1771, 'swamps': 1772, 'elevations': 1773, 'resolved': 1774, 'sunset': 1775, 'higher': 1776, 'win': 1777, 'allegators': 1778, 'wood': 1779, 'treated': 1780, 'shoulders': 1781, 'paint': 1782, 'picture': 1783, 'vain': 1784, 'returned': 1785, 'cottage': 1786, 'sociable': 1787, 'foaming': 1788, 'n': 1789, 'jeremy': 1790, 'lanigan': 1791, 'battered': 1792, 'hadnt': 1793, 'pound': 1794, 'farm': 1795, 'acres': 1796, 'party': 1797, 'listen': 1798, 'glisten': 1799, 'rows': 1800, 'ructions': 1801, 'invitation': 1802, 'minute': 1803, 'bees': 1804, 'cask': 1805, 'judy': 1806, 'odaly': 1807, 'milliner': 1808, 'wink': 1809, 'peggy': 1810, 'mcgilligan': 1811, 'lashings': 1812, 'punch': 1813, 'cakes': 1814, 'bacon': 1815, 'tea': 1816, 'nolans': 1817, 'dolans': 1818, 'ogradys': 1819, 'sounded': 1820, 'taras': 1821, 'hall': 1822, 'nelly': 1823, 'gray': 1824, 'rat': 1825, 'catchers': 1826, 'doing': 1827, 'kinds': 1828, 'nonsensical': 1829, 'polkas': 1830, 'whirligig': 1831, 'julia': 1832, 'banished': 1833, 'nonsense': 1834, 'twist': 1835, 'jig': 1836, 'mavrone': 1837, 'mad': 1838, 'ceiling': 1839, 'brooks': 1840, 'academy': 1841, 'learning': 1842, 'learn': 1843, 'couples': 1844, 'groups': 1845, 'accident': 1846, 'happened': 1847, 'terrance': 1848, 'mccarthy': 1849, 'finnertys': 1850, 'hoops': 1851, 'cried': 1852, 'meelia': 1853, 'murther': 1854, 'gathered': 1855, 'carmody': 1856, 'further': 1857, 'satisfaction': 1858, 'midst': 1859, 'kerrigan': 1860, 'declared': 1861, 'painted': 1862, 'suppose': 1863, 'morgan': 1864, 'powerful': 1865, 'stretched': 1866, 'smashed': 1867, 'chaneys': 1868, 'runctions': 1869, 'lick': 1870, 'phelim': 1871, 'mchugh': 1872, 'replied': 1873, 'introduction': 1874, 'kicked': 1875, 'terrible': 1876, 'hullabaloo': 1877, 'piper': 1878, 'strangled': 1879, 'squeezed': 1880, 'bellows': 1881, 'chanters': 1882, 'entangled': 1883, 'gaily': 1884, 'mairis': 1885, 'hillways': 1886, 'myrtle': 1887, 'bracken': 1888, 'sheilings': 1889, 'sake': 1890, 'rowans': 1891, 'herring': 1892, 'meal': 1893, 'peat': 1894, 'creel': 1895, 'bairns': 1896, 'weel': 1897, 'toast': 1898, 'soar': 1899, 'blackbird': 1900, 'note': 1901, 'linnet': 1902, 'lure': 1903, 'cozy': 1904, 'catch': 1905, 'company': 1906, 'harm': 1907, 'wit': 1908, 'recall': 1909, 'leisure': 1910, 'awhile': 1911, 'sorely': 1912, 'ruby': 1913, 'enthralled': 1914, 'sorry': 1915, 'theyd': 1916, 'falls': 1917, 'lot': 1918, 'tuned': 1919, 'bough': 1920, 'cow': 1921, 'chanting': 1922, 'melodious': 1923, 'scarce': 1924, 'soothed': 1925, 'solace': 1926, 'courtesy': 1927, 'salute': 1928, 'amiable': 1929, 'captive': 1930, 'slave': 1931, 'future': 1932, 'banter': 1933, 'enamour': 1934, 'indies': 1935, 'afford': 1936, 'transparently': 1937, 'flame': 1938, 'add': 1939, 'fuel': 1940, 'grant': 1941, 'desire': 1942, 'expire': 1943, 'wealth': 1944, 'damer': 1945, 'african': 1946, 'devonshire': 1947, 'lamp': 1948, 'alladin': 1949, 'genie': 1950, 'also': 1951, 'withdraw': 1952, 'tease': 1953, 'single': 1954, 'airy': 1955, 'embarrass': 1956, 'besides': 1957, 'almanack': 1958, 'useless': 1959, 'date': 1960, 'ware': 1961, 'rate': 1962, 'fragrance': 1963, 'loses': 1964, 'consumed': 1965, 'october': 1966, 'knowing': 1967, 'steer': 1968, 'blast': 1969, 'danger': 1970, 'farthing': 1971, 'affection': 1972, 'enjoy': 1973, 'choose': 1974, 'killarneys': 1975, 'sister': 1976, 'pains': 1977, 'loss': 1978, 'tuam': 1979, 'saluted': 1980, 'drank': 1981, 'pint': 1982, 'smother': 1983, 'reap': 1984, 'cut': 1985, 'goblins': 1986, 'bought': 1987, 'brogues': 1988, 'rattling': 1989, 'bogs': 1990, 'frightning': 1991, 'dogs': 1992, 'hunt': 1993, 'hare': 1994, 'follol': 1995, 'rah': 1996, 'mullingar': 1997, 'rested': 1998, 'limbs': 1999, 'blithe': 2000, 'heartfrom': 2001, 'paddys': 2002, 'cure': 2003, 'lassies': 2004, 'laughing': 2005, 'curious': 2006, 'style': 2007, 'twould': 2008, 'bubblin': 2009, 'hired': 2010, 'wages': 2011, 'required': 2012, 'almost': 2013, 'deprived': 2014, 'stroll': 2015, 'quality': 2016, 'locality': 2017, 'something': 2018, 'wobblin': 2019, 'enquiring': 2020, 'rogue': 2021, 'brogue': 2022, 'wasnt': 2023, 'vogue': 2024, 'spirits': 2025, 'falling': 2026, 'jumped': 2027, 'aboard': 2028, 'pigs': 2029, 'rigs': 2030, 'jigs': 2031, 'bubbling': 2032, 'holyhead': 2033, 'wished': 2034, 'instead': 2035, 'bouys': 2036, 'liverpool': 2037, 'safely': 2038, 'fool': 2039, 'boil': 2040, 'temper': 2041, 'losing': 2042, 'abusing': 2043, 'shillelagh': 2044, 'nigh': 2045, 'hobble': 2046, 'load': 2047, 'hurray': 2048, 'joined': 2049, 'affray': 2050, 'quitely': 2051, 'cleared': 2052, 'host': 2053, 'march': 2054, 'faces': 2055, 'farmstead': 2056, 'fishers': 2057, 'ban': 2058, 'vengeance': 2059, 'hapless': 2060, 'about': 2061, 'hemp': 2062, 'rope': 2063, 'clung': 2064, 'grim': 2065, 'array': 2066, 'earnest': 2067, 'stalwart': 2068, 'stainless': 2069, 'banner': 2070, 'marching': 2071, 'torn': 2072, 'furious': 2073, 'odds': 2074, 'keen': 2075, 'toomebridge': 2076, 'treads': 2077, 'upwards': 2078, 'traveled': 2079, 'quarters': 2080, 'below': 2081, 'hogshead': 2082, 'stack': 2083, 'stagger': 2084, 'dig': 2085, 'hole': 2086, 'couple': 2087, 'scratch': 2088, 'consolation': 2089, 'tyrant': 2090, 'remorseless': 2091, 'foe': 2092, 'lift': 2093, 'stranded': 2094, 'prince': 2095, 'edward': 2096, 'coffee': 2097, 'trace': 2098, 'fiddlin': 2099, 'dime': 2100, 'shy': 2101, 'hello': 2102, 'wintry': 2103, 'yellow': 2104, 'somewhere': 2105, 'written': 2106, 'begin': 2107, 'tap': 2108, 'caught': 2109, 'leap': 2110, 'clumsy': 2111, 'graceful': 2112, 'fiddlers': 2113, 'everywhere': 2114, 'boots': 2115, 'laughtcr': 2116, 'suits': 2117, 'easter': 2118, 'gowns': 2119, 'sailors': 2120, 'pianos': 2121, 'setting': 2122, 'someones': 2123, 'hats': 2124, 'rack': 2125, 'chair': 2126, 'wooden': 2127, 'feels': 2128, 'touch': 2129, 'awaitin': 2130, 'thc': 2131, 'fiddles': 2132, 'closet': 2133, 'strings': 2134, 'tbe': 2135, 'covers': 2136, 'buttoned': 2137, 'sometimes': 2138, 'melody': 2139, 'passes': 2140, 'slight': 2141, 'lack': 2142, 'moved': 2143, 'homeward': 2144, 'swan': 2145, 'moves': 2146, 'goods': 2147, 'gear': 2148, 'din': 2149, 'rude': 2150, 'wherein': 2151, 'dwell': 2152, 'abandon': 2153, 'energy': 2154, 'blight': 2155, 'praties': 2156, 'sheep': 2157, 'cattle': 2158, 'taxes': 2159, 'unpaid': 2160, 'redeem': 2161, 'bleak': 2162, 'landlord': 2163, 'sheriff': 2164, 'spleen': 2165, 'heaved': 2166, 'sigh': 2167, 'bade': 2168, 'goodbye': 2169, 'stony': 2170, 'anguish': 2171, 'seeing': 2172, 'feeble': 2173, 'frame': 2174, 'wrapped': 2175, 'cï¿½ta': 2176, 'mï¿½r': 2177, 'unseen': 2178, 'stern': 2179, 'rally': 2180, 'cheer': 2181, 'revenge': 2182, 'waking': 2183, 'wisdom': 2184, 'dwelling': 2185, 'battleshield': 2186, 'dignity': 2187, 'shelter': 2188, 'heed': 2189, 'inheritance': 2190, 'heavem': 2191, 'heaven': 2192, 'victory': 2193, 'reach': 2194, 'whatever': 2195, 'befall': 2196, 'ruler': 2197, 'pleasant': 2198, 'rambling': 2199, 'board': 2200, 'followed': 2201, 'shortly': 2202, 'anchor': 2203, '23rd': 2204, 'lrelands': 2205, 'daughters': 2206, 'crowds': 2207, 'assembled': 2208, 'fulfill': 2209, 'jovial': 2210, 'conversations': 2211, 'neighbors': 2212, 'turning': 2213, 'tailor': 2214, 'quigley': 2215, 'bould': 2216, 'britches': 2217, 'lived': 2218, 'flying': 2219, 'dove': 2220, 'hiii': 2221, 'dreamt': 2222, 'joking': 2223, 'manys': 2224, 'cock': 2225, 'shrill': 2226, 'awoke': 2227, 'california': 2228, 'miles': 2229, 'banbridge': 2230, 'july': 2231, 'boreen': 2232, 'sheen': 2233, 'coaxing': 2234, 'elf': 2235, 'shake': 2236, 'bantry': 2237, 'onward': 2238, 'sped': 2239, 'gazed': 2240, 'passerby': 2241, 'gem': 2242, 'irelands': 2243, 'travelled': 2244, 'hit': 2245, 'career': 2246, 'square': 2247, 'surrendered': 2248, 'tenant': 2249, 'shawl': 2250, 'gown': 2251, 'crossroads': 2252, 'dress': 2253, 'try': 2254, 'sheeps': 2255, 'deludhering': 2256, 'yoke': 2257, 'rust': 2258, 'plow': 2259, 'fireside': 2260, 'sits': 2261, 'whistle': 2262, 'changing': 2263, 'fright': 2264, 'downfall': 2265, 'cornwall': 2266, 'parlour': 2267, 'passing': 2268, 'william': 2269, 'betray': 2270, 'guinea': 2271, 'walking': 2272, 'mounted': 2273, 'platform': 2274, 'deny': 2275, 'walked': 2276, 'margin': 2277, 'lough': 2278, 'leane': 2279, 'bloomed': 2280, 'whom': 2281, 'cap': 2282, 'cloak': 2283, 'glossy': 2284, 'pail': 2285, 'palm': 2286, 'venus': 2287, 'bank': 2288, 'travelians': 2289, 'babes': 2290, 'freebirds': 2291, 'grew': 2292, 'matters': 2293, 'famine': 2294, 'rebelled': 2295, 'windswept': 2296, 'harbour': 2297, 'botany': 2298, 'whilst': 2299, 'wan': 2300, 'cloud': 2301, 'shannons': 2302, 'returnd': 2303, 'doubts': 2304, 'fears': 2305, 'aching': 2306, 'seemd': 2307, 'mingling': 2308, 'flood': 2309, 'path': 2310, 'wrath': 2311, 'lamenting': 2312, 'sudden': 2313, 'kissd': 2314, 'showrs': 2315, 'flowing': 2316, 'laughd': 2317, 'beam': 2318, 'soared': 2319, 'aloft': 2320, 'phantom': 2321, 'outspread': 2322, 'throbbing': 2323, 'hid': 2324, 'treasures': 2325, 'pots': 2326, 'tin': 2327, 'cans': 2328, 'mash': 2329, 'bran': 2330, 'barney': 2331, 'peeled': 2332, 'searching': 2333, 'connemara': 2334, 'butcher': 2335, 'quart': 2336, 'bottle': 2337, 'help': 2338, 'gate': 2339, 'glory': 2340, 'lane': 2341, 'village': 2342, 'church': 2343, 'spire': 2344, 'graveyard': 2345, 'baby': 2346, 'blessing': 2347, 'hoping': 2348, 'trust': 2349, 'strength': 2350, 'thank': 2351, 'bidding': 2352, 'bread': 2353, 'shines': 2354, 'fifty': 2355, 'often': 2356, 'shut': 2357, 'frisky': 2358, 'pig': 2359, 'whisky': 2360, 'uncle': 2361, 'enlisted': 2362, 'trudged': 2363, 'bosom': 2364, 'daisy': 2365, 'drubbing': 2366, 'shirts': 2367, 'battle': 2368, 'blows': 2369, 'pate': 2370, 'bothered': 2371, 'rarely': 2372, 'dropped': 2373, 'honest': 2374, 'thinks': 2375, 'eight': 2376, 'score': 2377, 'basin': 2378, 'zoo': 2379, 'everybody': 2380, 'calls': 2381, 'trades': 2382, 'dinner': 2383, 'slip': 2384, 'corner': 2385, 'barn': 2386, 'currabawn': 2387, 'shocking': 2388, 'wet': 2389, 'raindrops': 2390, 'rats': 2391, 'peek': 2392, 'waken': 2393, 'spotted': 2394, 'apron': 2395, 'calico': 2396, 'blouse': 2397, 'frighten': 2398, 'afraid': 2399, 'flaxen': 2400, 'haired': 2401, 'rags': 2402, 'tags': 2403, 'leggins': 2404, 'collar': 2405, 'tie': 2406, 'goggles': 2407, 'fashioned': 2408, 'bag': 2409, 'bulging': 2410, 'sack': 2411, 'peeping': 2412, 'skin': 2413, 'rink': 2414, 'doodle': 2415, 'getting': 2416, 'raked': 2417, 'gladness': 2418, 'tuning': 2419, 'fills': 2420, 'eily': 2421, 'prouder': 2422, 'thady': 2423, 'boldly': 2424, 'lasses': 2425, 'fled': 2426, 'silent': 2427, 'glad': 2428, 'echo': 2429, 'companions': 2430, 'soars': 2431, 'enchanted': 2432, 'granted': 2433, 'adoration': 2434, 'gives': 2435, 'joyous': 2436, 'elation': 2437, 'covered': 2438, 'winter': 2439, 'riding': 2440, 'cherry': 2441, 'coal': 2442, 'falter': 2443, 'bowed': 2444, 'bonnet': 2445, 'courteous': 2446, 'looks': 2447, 'engaging': 2448, 'sell': 2449, 'purse': 2450, 'yearly': 2451, 'need': 2452, 'market': 2453, 'gain': 2454, 'dearly': 2455, 'tarry': 2456, 'although': 2457, 'parlay': 2458, 'ranks': 2459, 'girded': 2460, 'slung': 2461, 'warrior': 2462, 'bard': 2463, 'betrays': 2464, 'rights': 2465, 'faithful': 2466, 'chords': 2467, 'asunder': 2468, 'sully': 2469, 'bravry': 2470, 'londons': 2471, 'sight': 2472, 'workin': 2473, 'sow': 2474, 'wheat': 2475, 'gangs': 2476, 'sweep': 2477, 'expressed': 2478, 'london': 2479, 'top': 2480, 'dresses': 2481, 'bath': 2482, 'startin': 2483, 'fashions': 2484, 'mccree': 2485, 'nature': 2486, 'designed': 2487, 'complexions': 2488, 'cream': 2489, 'regard': 2490, 'sip': 2491, 'colors': 2492, 'wait': 2493, 'waitin': 2494, 'sweeps': 2495, 'beauing': 2496, 'belling': 2497, 'windows': 2498, 'cursing': 2499, 'faster': 2500, 'waiters': 2501, 'bailiffs': 2502, 'duns': 2503, 'bacchus': 2504, 'begotten': 2505, 'politicians': 2506, 'funds': 2507, 'dadda': 2508, 'living': 2509, 'drives': 2510, 'having': 2511, 'racking': 2512, 'tenants': 2513, 'stewards': 2514, 'teasing': 2515, 'raising': 2516, 'wishing': 2517, 'sunny': 2518, 'doves': 2519, 'coo': 2520, 'neath': 2521, 'sunbeam': 2522, 'robin': 2523, 'waters': 2524, 'larks': 2525, 'join': 2526, 'breaks': 2527, 'oftimes': 2528, 'lilies': 2529, 'declining': 2530, 'vale': 2531, 'shades': 2532, 'mantle': 2533, 'spreading': 2534, 'listening': 2535, 'shedding': 2536, 'beginning': 2537, 'spinning': 2538, 'blind': 2539, 'drowsily': 2540, 'knitting': 2541, 'cheerily': 2542, 'noiselessly': 2543, 'whirring': 2544, 'foots': 2545, 'stirring': 2546, 'sprightly': 2547, 'chara': 2548, 'tapping': 2549, 'ivy': 2550, 'flapping': 2551, 'somebody': 2552, 'sighing': 2553, 'autumn': 2554, 'noise': 2555, 'chirping': 2556, 'holly': 2557, 'shoving': 2558, 'wrong': 2559, 'coolin': 2560, 'casement': 2561, 'rove': 2562, 'moons': 2563, 'brightly': 2564, 'shakes': 2565, 'lays': 2566, 'longs': 2567, 'lingers': 2568, 'glance': 2569, 'puts': 2570, 'lazily': 2571, 'easily': 2572, 'lowly': 2573, 'reels': 2574, 'noiseless': 2575, 'leaps': 2576, 'ere': 2577, 'lovers': 2578, 'roved': 2579, 'verdant': 2580, 'braes': 2581, 'skreen': 2582, 'countrie': 2583, 'foreign': 2584, 'strand': 2585, 'dewy': 2586, 'climb': 2587, 'rob': 2588, 'boat': 2589, 'sails': 2590, 'loaded': 2591, 'sink': 2592, 'leaned': 2593, 'oak': 2594, 'trusty': 2595, 'false': 2596, 'reached': 2597, 'pricked': 2598, 'waxes': 2599, 'fades': 2600, 'wholl': 2601, 'cockle': 2602, 'gloom': 2603, 'news': 2604, 'forbid': 2605, 'patricks': 2606, 'napper': 2607, 'tandy': 2608, 'hows': 2609, 'distressful': 2610, 'englands': 2611, 'remind': 2612, 'pull': 2613, 'throw': 2614, 'sod': 2615, 'root': 2616, 'underfoot': 2617, 'laws': 2618, 'blades': 2619, 'growin': 2620, 'dare': 2621, 'show': 2622, 'caubeen': 2623, 'year': 2624, 'returning': 2625, 'store': 2626, 'ale': 2627, 'frequent': 2628, 'landlady': 2629, 'credit': 2630, 'custom': 2631, 'sovereigns': 2632, 'landladys': 2633, 'wines': 2634, 'confess': 2635, 'pardon': 2636, 'prodigal': 2637, 'caress': 2638, 'forgive': 2639, 'ofttimes': 2640, 'wondering': 2641, 'powr': 2642, 'beguile': 2643, 'teardrop': 2644, 'lilting': 2645, 'laughters': 2646, 'twinkle': 2647, 'lilt': 2648, 'seems': 2649, 'linnets': 2650, 'real': 2651, 'regret': 2652, 'throughout': 2653, 'youths': 2654, 'chance': 2655, 'spied': 2656, 'receiver': 2657, 'counted': 2658, 'penny': 2659, 'bu': 2660, 'rungum': 2661, 'chamber': 2662, 'course': 2663, 'charges': 2664, 'filled': 2665, 'ready': 2666, 'footmen': 2667, 'likewise': 2668, 'draw': 2669, 'pistol': 2670, 'couldnt': 2671, 'shoot': 2672, 'robbin': 2673, 'jailer': 2674, 'tight': 2675, 'fisted': 2676, 'army': 2677, 'stationed': 2678, 'cork': 2679, 'roamin': 2680, 'swear': 2681, 'treat': 2682, 'sportin': 2683, 'hurley': 2684, 'bollin': 2685, 'maids': 2686, 'summertime': 2687, 'pluck': 2688, 'yon': 2689}\n",
            "2690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90vOGwS21eRV"
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-OAkYPlxb6N",
        "outputId": "ae251af7-f904-465b-f228-151708e188e9"
      },
      "source": [
        "# large corpus에 대해 더 잘 동작할 수 있도록 model 수정\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150))) # increase units\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam(learning_rate = 0.01) # use own optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=500, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "377/377 [==============================] - 11s 19ms/step - loss: 6.6500 - accuracy: 0.0714\n",
            "Epoch 2/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 5.7676 - accuracy: 0.1141\n",
            "Epoch 3/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 4.8700 - accuracy: 0.1699\n",
            "Epoch 4/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 3.9243 - accuracy: 0.2446\n",
            "Epoch 5/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 3.0836 - accuracy: 0.3530\n",
            "Epoch 6/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 2.4078 - accuracy: 0.4531\n",
            "Epoch 7/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.9189 - accuracy: 0.5515\n",
            "Epoch 8/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.5388 - accuracy: 0.6359\n",
            "Epoch 9/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.3045 - accuracy: 0.6927\n",
            "Epoch 10/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.1599 - accuracy: 0.7233\n",
            "Epoch 11/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0549 - accuracy: 0.7446\n",
            "Epoch 12/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0275 - accuracy: 0.7496\n",
            "Epoch 13/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.0652 - accuracy: 0.7314\n",
            "Epoch 14/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.1541 - accuracy: 0.7067\n",
            "Epoch 15/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.1644 - accuracy: 0.7019\n",
            "Epoch 16/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0899 - accuracy: 0.7207\n",
            "Epoch 17/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9582 - accuracy: 0.7508\n",
            "Epoch 18/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8574 - accuracy: 0.7843\n",
            "Epoch 19/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8230 - accuracy: 0.7895\n",
            "Epoch 20/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8040 - accuracy: 0.7956\n",
            "Epoch 21/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8769 - accuracy: 0.7704\n",
            "Epoch 22/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0168 - accuracy: 0.7357\n",
            "Epoch 23/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.2224 - accuracy: 0.6831\n",
            "Epoch 24/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.1042 - accuracy: 0.7133\n",
            "Epoch 25/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.9928 - accuracy: 0.7385\n",
            "Epoch 26/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8915 - accuracy: 0.7697\n",
            "Epoch 27/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8454 - accuracy: 0.7797\n",
            "Epoch 28/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8114 - accuracy: 0.7878\n",
            "Epoch 29/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8416 - accuracy: 0.7795\n",
            "Epoch 30/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8975 - accuracy: 0.7619\n",
            "Epoch 31/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9475 - accuracy: 0.7481\n",
            "Epoch 32/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0157 - accuracy: 0.7360\n",
            "Epoch 33/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0201 - accuracy: 0.7313\n",
            "Epoch 34/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0743 - accuracy: 0.7187\n",
            "Epoch 35/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9669 - accuracy: 0.7412\n",
            "Epoch 36/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8779 - accuracy: 0.7670\n",
            "Epoch 37/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8064 - accuracy: 0.7873\n",
            "Epoch 38/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7413 - accuracy: 0.8076\n",
            "Epoch 39/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7257 - accuracy: 0.8091\n",
            "Epoch 40/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7833 - accuracy: 0.7948\n",
            "Epoch 41/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8764 - accuracy: 0.7663\n",
            "Epoch 42/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0665 - accuracy: 0.7177\n",
            "Epoch 43/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.1245 - accuracy: 0.7094\n",
            "Epoch 44/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 1.0109 - accuracy: 0.7274\n",
            "Epoch 45/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9073 - accuracy: 0.7565\n",
            "Epoch 46/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8240 - accuracy: 0.7800\n",
            "Epoch 47/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7653 - accuracy: 0.7938\n",
            "Epoch 48/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7344 - accuracy: 0.8063\n",
            "Epoch 49/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7816 - accuracy: 0.7933\n",
            "Epoch 50/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8169 - accuracy: 0.7814\n",
            "Epoch 51/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9261 - accuracy: 0.7524\n",
            "Epoch 52/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9637 - accuracy: 0.7450\n",
            "Epoch 53/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9760 - accuracy: 0.7396\n",
            "Epoch 54/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9797 - accuracy: 0.7448\n",
            "Epoch 55/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8628 - accuracy: 0.7675\n",
            "Epoch 56/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8052 - accuracy: 0.7835\n",
            "Epoch 57/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7833 - accuracy: 0.7902\n",
            "Epoch 58/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8397 - accuracy: 0.7776\n",
            "Epoch 59/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8037 - accuracy: 0.7826\n",
            "Epoch 60/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8001 - accuracy: 0.7853\n",
            "Epoch 61/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8922 - accuracy: 0.7640\n",
            "Epoch 62/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8675 - accuracy: 0.7636\n",
            "Epoch 63/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8849 - accuracy: 0.7601\n",
            "Epoch 64/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8849 - accuracy: 0.7629\n",
            "Epoch 65/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8463 - accuracy: 0.7724\n",
            "Epoch 66/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8360 - accuracy: 0.7738\n",
            "Epoch 67/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8219 - accuracy: 0.7755\n",
            "Epoch 68/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8157 - accuracy: 0.7805\n",
            "Epoch 69/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8463 - accuracy: 0.7727\n",
            "Epoch 70/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8260 - accuracy: 0.7756\n",
            "Epoch 71/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7920 - accuracy: 0.7843\n",
            "Epoch 72/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7569 - accuracy: 0.7970\n",
            "Epoch 73/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7687 - accuracy: 0.7905\n",
            "Epoch 74/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8598 - accuracy: 0.7682\n",
            "Epoch 75/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9459 - accuracy: 0.7510\n",
            "Epoch 76/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9252 - accuracy: 0.7527\n",
            "Epoch 77/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8735 - accuracy: 0.7637\n",
            "Epoch 78/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8140 - accuracy: 0.7814\n",
            "Epoch 79/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7818 - accuracy: 0.7883\n",
            "Epoch 80/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7949 - accuracy: 0.7855\n",
            "Epoch 81/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8032 - accuracy: 0.7804\n",
            "Epoch 82/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9119 - accuracy: 0.7583\n",
            "Epoch 83/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.9141 - accuracy: 0.7590\n",
            "Epoch 84/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8967 - accuracy: 0.7597\n",
            "Epoch 85/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8272 - accuracy: 0.7737\n",
            "Epoch 86/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8064 - accuracy: 0.7799\n",
            "Epoch 87/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7786 - accuracy: 0.7895\n",
            "Epoch 88/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7958 - accuracy: 0.7864\n",
            "Epoch 89/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8229 - accuracy: 0.7781\n",
            "Epoch 90/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8616 - accuracy: 0.7679\n",
            "Epoch 91/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8253 - accuracy: 0.7800\n",
            "Epoch 92/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8022 - accuracy: 0.7808\n",
            "Epoch 93/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7721 - accuracy: 0.7886\n",
            "Epoch 94/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7690 - accuracy: 0.7896\n",
            "Epoch 95/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8089 - accuracy: 0.7809\n",
            "Epoch 96/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8850 - accuracy: 0.7583\n",
            "Epoch 97/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8461 - accuracy: 0.7710\n",
            "Epoch 98/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8053 - accuracy: 0.7822\n",
            "Epoch 99/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7725 - accuracy: 0.7939\n",
            "Epoch 100/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7587 - accuracy: 0.7920\n",
            "Epoch 101/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7453 - accuracy: 0.7973\n",
            "Epoch 102/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7723 - accuracy: 0.7899\n",
            "Epoch 103/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7858 - accuracy: 0.7873\n",
            "Epoch 104/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7888 - accuracy: 0.7848\n",
            "Epoch 105/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8109 - accuracy: 0.7798\n",
            "Epoch 106/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8117 - accuracy: 0.7787\n",
            "Epoch 107/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7856 - accuracy: 0.7874\n",
            "Epoch 108/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8219 - accuracy: 0.7799\n",
            "Epoch 109/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7860 - accuracy: 0.7876\n",
            "Epoch 110/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7609 - accuracy: 0.7960\n",
            "Epoch 111/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7425 - accuracy: 0.7975\n",
            "Epoch 112/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7342 - accuracy: 0.8005\n",
            "Epoch 113/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7092 - accuracy: 0.8066\n",
            "Epoch 114/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7562 - accuracy: 0.7935\n",
            "Epoch 115/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7964 - accuracy: 0.7837\n",
            "Epoch 116/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8007 - accuracy: 0.7834\n",
            "Epoch 117/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8579 - accuracy: 0.7686\n",
            "Epoch 118/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8258 - accuracy: 0.7788\n",
            "Epoch 119/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8164 - accuracy: 0.7829\n",
            "Epoch 120/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7617 - accuracy: 0.7902\n",
            "Epoch 121/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7312 - accuracy: 0.8023\n",
            "Epoch 122/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7349 - accuracy: 0.8020\n",
            "Epoch 123/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7408 - accuracy: 0.7989\n",
            "Epoch 124/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7326 - accuracy: 0.8040\n",
            "Epoch 125/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7298 - accuracy: 0.8005\n",
            "Epoch 126/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7479 - accuracy: 0.7980\n",
            "Epoch 127/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7780 - accuracy: 0.7907\n",
            "Epoch 128/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8262 - accuracy: 0.7789\n",
            "Epoch 129/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8378 - accuracy: 0.7789\n",
            "Epoch 130/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8483 - accuracy: 0.7749\n",
            "Epoch 131/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8516 - accuracy: 0.7732\n",
            "Epoch 132/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7552 - accuracy: 0.7958\n",
            "Epoch 133/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7348 - accuracy: 0.8010\n",
            "Epoch 134/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7379 - accuracy: 0.8028\n",
            "Epoch 135/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7401 - accuracy: 0.7980\n",
            "Epoch 136/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8259 - accuracy: 0.7783\n",
            "Epoch 137/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8524 - accuracy: 0.7729\n",
            "Epoch 138/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8214 - accuracy: 0.7794\n",
            "Epoch 139/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8218 - accuracy: 0.7842\n",
            "Epoch 140/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8815 - accuracy: 0.7637\n",
            "Epoch 141/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8158 - accuracy: 0.7819\n",
            "Epoch 142/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7481 - accuracy: 0.7966\n",
            "Epoch 143/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7246 - accuracy: 0.8057\n",
            "Epoch 144/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7327 - accuracy: 0.8067\n",
            "Epoch 145/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7795 - accuracy: 0.7922\n",
            "Epoch 146/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8251 - accuracy: 0.7834\n",
            "Epoch 147/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8337 - accuracy: 0.7791\n",
            "Epoch 148/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8244 - accuracy: 0.7823\n",
            "Epoch 149/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7798 - accuracy: 0.7896\n",
            "Epoch 150/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7605 - accuracy: 0.7983\n",
            "Epoch 151/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7253 - accuracy: 0.8055\n",
            "Epoch 152/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7347 - accuracy: 0.8050\n",
            "Epoch 153/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7082 - accuracy: 0.8084\n",
            "Epoch 154/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7450 - accuracy: 0.8026\n",
            "Epoch 155/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7458 - accuracy: 0.8008\n",
            "Epoch 156/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7148 - accuracy: 0.8061\n",
            "Epoch 157/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7368 - accuracy: 0.8003\n",
            "Epoch 158/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7757 - accuracy: 0.7903\n",
            "Epoch 159/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8403 - accuracy: 0.7759\n",
            "Epoch 160/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8670 - accuracy: 0.7739\n",
            "Epoch 161/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8292 - accuracy: 0.7787\n",
            "Epoch 162/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7887 - accuracy: 0.7932\n",
            "Epoch 163/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7453 - accuracy: 0.8029\n",
            "Epoch 164/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7254 - accuracy: 0.8041\n",
            "Epoch 165/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7393 - accuracy: 0.8021\n",
            "Epoch 166/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7225 - accuracy: 0.8052\n",
            "Epoch 167/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7240 - accuracy: 0.8015\n",
            "Epoch 168/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7440 - accuracy: 0.8007\n",
            "Epoch 169/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7380 - accuracy: 0.8017\n",
            "Epoch 170/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7272 - accuracy: 0.8054\n",
            "Epoch 171/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7666 - accuracy: 0.7974\n",
            "Epoch 172/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7360 - accuracy: 0.8001\n",
            "Epoch 173/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7481 - accuracy: 0.7961\n",
            "Epoch 174/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7540 - accuracy: 0.7962\n",
            "Epoch 175/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7371 - accuracy: 0.8011\n",
            "Epoch 176/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7754 - accuracy: 0.7927\n",
            "Epoch 177/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7939 - accuracy: 0.7853\n",
            "Epoch 178/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7879 - accuracy: 0.7843\n",
            "Epoch 179/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7638 - accuracy: 0.7929\n",
            "Epoch 180/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7232 - accuracy: 0.8052\n",
            "Epoch 181/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6992 - accuracy: 0.8120\n",
            "Epoch 182/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7131 - accuracy: 0.8084\n",
            "Epoch 183/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6948 - accuracy: 0.8097\n",
            "Epoch 184/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7013 - accuracy: 0.8134\n",
            "Epoch 185/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7007 - accuracy: 0.8113\n",
            "Epoch 186/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7102 - accuracy: 0.8068\n",
            "Epoch 187/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7063 - accuracy: 0.8107\n",
            "Epoch 188/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7183 - accuracy: 0.8041\n",
            "Epoch 189/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7472 - accuracy: 0.8015\n",
            "Epoch 190/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7861 - accuracy: 0.7960\n",
            "Epoch 191/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7950 - accuracy: 0.7922\n",
            "Epoch 192/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7835 - accuracy: 0.7934\n",
            "Epoch 193/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7568 - accuracy: 0.7981\n",
            "Epoch 194/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7343 - accuracy: 0.7981\n",
            "Epoch 195/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7324 - accuracy: 0.8028\n",
            "Epoch 196/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7333 - accuracy: 0.8020\n",
            "Epoch 197/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6798 - accuracy: 0.8122\n",
            "Epoch 198/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7036 - accuracy: 0.8085\n",
            "Epoch 199/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7067 - accuracy: 0.8062\n",
            "Epoch 200/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7163 - accuracy: 0.8050\n",
            "Epoch 201/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7064 - accuracy: 0.8071\n",
            "Epoch 202/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7107 - accuracy: 0.8105\n",
            "Epoch 203/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6734 - accuracy: 0.8158\n",
            "Epoch 204/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6775 - accuracy: 0.8162\n",
            "Epoch 205/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7019 - accuracy: 0.8145\n",
            "Epoch 206/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7176 - accuracy: 0.8060\n",
            "Epoch 207/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7138 - accuracy: 0.8035\n",
            "Epoch 208/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7294 - accuracy: 0.8005\n",
            "Epoch 209/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7353 - accuracy: 0.8032\n",
            "Epoch 210/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7249 - accuracy: 0.8060\n",
            "Epoch 211/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7283 - accuracy: 0.8016\n",
            "Epoch 212/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8481 - accuracy: 0.7916\n",
            "Epoch 213/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.9548 - accuracy: 0.7726\n",
            "Epoch 214/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8085 - accuracy: 0.7889\n",
            "Epoch 215/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7228 - accuracy: 0.8076\n",
            "Epoch 216/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8369 - accuracy: 0.7864\n",
            "Epoch 217/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7631 - accuracy: 0.7983\n",
            "Epoch 218/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7540 - accuracy: 0.8012\n",
            "Epoch 219/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7328 - accuracy: 0.8050\n",
            "Epoch 220/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7040 - accuracy: 0.8148\n",
            "Epoch 221/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7014 - accuracy: 0.8143\n",
            "Epoch 222/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6911 - accuracy: 0.8160\n",
            "Epoch 223/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6963 - accuracy: 0.8128\n",
            "Epoch 224/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6967 - accuracy: 0.8155\n",
            "Epoch 225/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7140 - accuracy: 0.8102\n",
            "Epoch 226/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7356 - accuracy: 0.8043\n",
            "Epoch 227/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7585 - accuracy: 0.7963\n",
            "Epoch 228/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7348 - accuracy: 0.8044\n",
            "Epoch 229/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7122 - accuracy: 0.8091\n",
            "Epoch 230/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6990 - accuracy: 0.8120\n",
            "Epoch 231/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7225 - accuracy: 0.8038\n",
            "Epoch 232/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7456 - accuracy: 0.8021\n",
            "Epoch 233/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7234 - accuracy: 0.8047\n",
            "Epoch 234/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7145 - accuracy: 0.8083\n",
            "Epoch 235/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7117 - accuracy: 0.8086\n",
            "Epoch 236/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7059 - accuracy: 0.8102\n",
            "Epoch 237/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6887 - accuracy: 0.8136\n",
            "Epoch 238/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6854 - accuracy: 0.8144\n",
            "Epoch 239/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6843 - accuracy: 0.8152\n",
            "Epoch 240/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6917 - accuracy: 0.8167\n",
            "Epoch 241/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7172 - accuracy: 0.8106\n",
            "Epoch 242/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7201 - accuracy: 0.8084\n",
            "Epoch 243/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7010 - accuracy: 0.8083\n",
            "Epoch 244/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7322 - accuracy: 0.8067\n",
            "Epoch 245/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7259 - accuracy: 0.8037\n",
            "Epoch 246/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7540 - accuracy: 0.8037\n",
            "Epoch 247/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8108 - accuracy: 0.7929\n",
            "Epoch 248/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7902 - accuracy: 0.7932\n",
            "Epoch 249/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7811 - accuracy: 0.7974\n",
            "Epoch 250/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7671 - accuracy: 0.7958\n",
            "Epoch 251/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7420 - accuracy: 0.7991\n",
            "Epoch 252/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7092 - accuracy: 0.8076\n",
            "Epoch 253/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6946 - accuracy: 0.8144\n",
            "Epoch 254/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6823 - accuracy: 0.8158\n",
            "Epoch 255/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6677 - accuracy: 0.8199\n",
            "Epoch 256/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6735 - accuracy: 0.8179\n",
            "Epoch 257/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6892 - accuracy: 0.8143\n",
            "Epoch 258/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6914 - accuracy: 0.8123\n",
            "Epoch 259/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7060 - accuracy: 0.8159\n",
            "Epoch 260/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7655 - accuracy: 0.8032\n",
            "Epoch 261/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7920 - accuracy: 0.7930\n",
            "Epoch 262/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7816 - accuracy: 0.7944\n",
            "Epoch 263/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7418 - accuracy: 0.8039\n",
            "Epoch 264/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6979 - accuracy: 0.8147\n",
            "Epoch 265/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.4026 - accuracy: 0.7255\n",
            "Epoch 266/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 1.1196 - accuracy: 0.7431\n",
            "Epoch 267/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.9649 - accuracy: 0.7714\n",
            "Epoch 268/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8237 - accuracy: 0.7927\n",
            "Epoch 269/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7763 - accuracy: 0.8064\n",
            "Epoch 270/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7370 - accuracy: 0.8150\n",
            "Epoch 271/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7237 - accuracy: 0.8174\n",
            "Epoch 272/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7317 - accuracy: 0.8124\n",
            "Epoch 273/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7092 - accuracy: 0.8179\n",
            "Epoch 274/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7301 - accuracy: 0.8176\n",
            "Epoch 275/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7464 - accuracy: 0.8121\n",
            "Epoch 276/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8678 - accuracy: 0.7882\n",
            "Epoch 277/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8368 - accuracy: 0.7936\n",
            "Epoch 278/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8344 - accuracy: 0.7883\n",
            "Epoch 279/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.8018 - accuracy: 0.7988\n",
            "Epoch 280/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7785 - accuracy: 0.8037\n",
            "Epoch 281/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7909 - accuracy: 0.8005\n",
            "Epoch 282/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8132 - accuracy: 0.8026\n",
            "Epoch 283/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7904 - accuracy: 0.7998\n",
            "Epoch 284/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7852 - accuracy: 0.8032\n",
            "Epoch 285/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7585 - accuracy: 0.8047\n",
            "Epoch 286/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7635 - accuracy: 0.8098\n",
            "Epoch 287/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7595 - accuracy: 0.8115\n",
            "Epoch 288/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7458 - accuracy: 0.8084\n",
            "Epoch 289/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7232 - accuracy: 0.8133\n",
            "Epoch 290/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7421 - accuracy: 0.8127\n",
            "Epoch 291/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7344 - accuracy: 0.8152\n",
            "Epoch 292/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7460 - accuracy: 0.8113\n",
            "Epoch 293/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7497 - accuracy: 0.8092\n",
            "Epoch 294/500\n",
            "377/377 [==============================] - 7s 18ms/step - loss: 0.7593 - accuracy: 0.8074\n",
            "Epoch 295/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7724 - accuracy: 0.8059\n",
            "Epoch 296/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7650 - accuracy: 0.8054\n",
            "Epoch 297/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7457 - accuracy: 0.8090\n",
            "Epoch 298/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7663 - accuracy: 0.8051\n",
            "Epoch 299/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7733 - accuracy: 0.8075\n",
            "Epoch 300/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7801 - accuracy: 0.8026\n",
            "Epoch 301/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7355 - accuracy: 0.8149\n",
            "Epoch 302/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7055 - accuracy: 0.8205\n",
            "Epoch 303/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7063 - accuracy: 0.8192\n",
            "Epoch 304/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7089 - accuracy: 0.8202\n",
            "Epoch 305/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7089 - accuracy: 0.8177\n",
            "Epoch 306/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7563 - accuracy: 0.8130\n",
            "Epoch 307/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7534 - accuracy: 0.8095\n",
            "Epoch 308/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7616 - accuracy: 0.8090\n",
            "Epoch 309/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8483 - accuracy: 0.7947\n",
            "Epoch 310/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8212 - accuracy: 0.7964\n",
            "Epoch 311/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8025 - accuracy: 0.8004\n",
            "Epoch 312/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7849 - accuracy: 0.8039\n",
            "Epoch 313/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8110 - accuracy: 0.8017\n",
            "Epoch 314/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7573 - accuracy: 0.8114\n",
            "Epoch 315/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7279 - accuracy: 0.8186\n",
            "Epoch 316/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7246 - accuracy: 0.8177\n",
            "Epoch 317/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7150 - accuracy: 0.8202\n",
            "Epoch 318/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7119 - accuracy: 0.8197\n",
            "Epoch 319/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7384 - accuracy: 0.8143\n",
            "Epoch 320/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7551 - accuracy: 0.8083\n",
            "Epoch 321/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7691 - accuracy: 0.8086\n",
            "Epoch 322/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7525 - accuracy: 0.8104\n",
            "Epoch 323/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7765 - accuracy: 0.8036\n",
            "Epoch 324/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7476 - accuracy: 0.8107\n",
            "Epoch 325/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7375 - accuracy: 0.8149\n",
            "Epoch 326/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7610 - accuracy: 0.8095\n",
            "Epoch 327/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7404 - accuracy: 0.8128\n",
            "Epoch 328/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7514 - accuracy: 0.8100\n",
            "Epoch 329/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8000 - accuracy: 0.8020\n",
            "Epoch 330/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7854 - accuracy: 0.8029\n",
            "Epoch 331/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7519 - accuracy: 0.8093\n",
            "Epoch 332/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7295 - accuracy: 0.8170\n",
            "Epoch 333/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7134 - accuracy: 0.8216\n",
            "Epoch 334/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7315 - accuracy: 0.8190\n",
            "Epoch 335/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7476 - accuracy: 0.8131\n",
            "Epoch 336/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7296 - accuracy: 0.8177\n",
            "Epoch 337/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7415 - accuracy: 0.8138\n",
            "Epoch 338/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7354 - accuracy: 0.8142\n",
            "Epoch 339/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7400 - accuracy: 0.8133\n",
            "Epoch 340/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7357 - accuracy: 0.8136\n",
            "Epoch 341/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7534 - accuracy: 0.8107\n",
            "Epoch 342/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8245 - accuracy: 0.7948\n",
            "Epoch 343/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8534 - accuracy: 0.7923\n",
            "Epoch 344/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8209 - accuracy: 0.7958\n",
            "Epoch 345/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7916 - accuracy: 0.8029\n",
            "Epoch 346/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7488 - accuracy: 0.8122\n",
            "Epoch 347/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7443 - accuracy: 0.8160\n",
            "Epoch 348/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7225 - accuracy: 0.8213\n",
            "Epoch 349/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7107 - accuracy: 0.8209\n",
            "Epoch 350/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7079 - accuracy: 0.8225\n",
            "Epoch 351/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7056 - accuracy: 0.8197\n",
            "Epoch 352/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7397 - accuracy: 0.8128\n",
            "Epoch 353/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7473 - accuracy: 0.8120\n",
            "Epoch 354/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7758 - accuracy: 0.8090\n",
            "Epoch 355/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7924 - accuracy: 0.8040\n",
            "Epoch 356/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7466 - accuracy: 0.8094\n",
            "Epoch 357/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7569 - accuracy: 0.8103\n",
            "Epoch 358/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7450 - accuracy: 0.8121\n",
            "Epoch 359/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7380 - accuracy: 0.8133\n",
            "Epoch 360/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7312 - accuracy: 0.8170\n",
            "Epoch 361/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7170 - accuracy: 0.8179\n",
            "Epoch 362/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7186 - accuracy: 0.8180\n",
            "Epoch 363/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7136 - accuracy: 0.8166\n",
            "Epoch 364/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7217 - accuracy: 0.8175\n",
            "Epoch 365/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7361 - accuracy: 0.8183\n",
            "Epoch 366/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7402 - accuracy: 0.8155\n",
            "Epoch 367/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7613 - accuracy: 0.8143\n",
            "Epoch 368/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7558 - accuracy: 0.8133\n",
            "Epoch 369/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7476 - accuracy: 0.8161\n",
            "Epoch 370/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7246 - accuracy: 0.8171\n",
            "Epoch 371/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7187 - accuracy: 0.8169\n",
            "Epoch 372/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7011 - accuracy: 0.8226\n",
            "Epoch 373/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7089 - accuracy: 0.8241\n",
            "Epoch 374/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7128 - accuracy: 0.8198\n",
            "Epoch 375/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7524 - accuracy: 0.8133\n",
            "Epoch 376/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7763 - accuracy: 0.8094\n",
            "Epoch 377/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7904 - accuracy: 0.8062\n",
            "Epoch 378/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8060 - accuracy: 0.8036\n",
            "Epoch 379/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7935 - accuracy: 0.8034\n",
            "Epoch 380/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7823 - accuracy: 0.8050\n",
            "Epoch 381/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7771 - accuracy: 0.8050\n",
            "Epoch 382/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7644 - accuracy: 0.8084\n",
            "Epoch 383/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7710 - accuracy: 0.8142\n",
            "Epoch 384/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7601 - accuracy: 0.8125\n",
            "Epoch 385/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7301 - accuracy: 0.8188\n",
            "Epoch 386/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7155 - accuracy: 0.8231\n",
            "Epoch 387/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7099 - accuracy: 0.8220\n",
            "Epoch 388/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7615 - accuracy: 0.8140\n",
            "Epoch 389/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7771 - accuracy: 0.8107\n",
            "Epoch 390/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7929 - accuracy: 0.8079\n",
            "Epoch 391/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7893 - accuracy: 0.8082\n",
            "Epoch 392/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 1.0503 - accuracy: 0.7745\n",
            "Epoch 393/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8968 - accuracy: 0.7867\n",
            "Epoch 394/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8358 - accuracy: 0.7977\n",
            "Epoch 395/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7966 - accuracy: 0.8040\n",
            "Epoch 396/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7485 - accuracy: 0.8135\n",
            "Epoch 397/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7460 - accuracy: 0.8158\n",
            "Epoch 398/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7378 - accuracy: 0.8149\n",
            "Epoch 399/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7191 - accuracy: 0.8178\n",
            "Epoch 400/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7098 - accuracy: 0.8208\n",
            "Epoch 401/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.6971 - accuracy: 0.8252\n",
            "Epoch 402/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7174 - accuracy: 0.8231\n",
            "Epoch 403/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7285 - accuracy: 0.8215\n",
            "Epoch 404/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8118 - accuracy: 0.8110\n",
            "Epoch 405/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7935 - accuracy: 0.8093\n",
            "Epoch 406/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8205 - accuracy: 0.8028\n",
            "Epoch 407/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.8102 - accuracy: 0.8015\n",
            "Epoch 408/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8449 - accuracy: 0.8013\n",
            "Epoch 409/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8086 - accuracy: 0.8042\n",
            "Epoch 410/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7837 - accuracy: 0.8124\n",
            "Epoch 411/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7602 - accuracy: 0.8191\n",
            "Epoch 412/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7449 - accuracy: 0.8179\n",
            "Epoch 413/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7084 - accuracy: 0.8231\n",
            "Epoch 414/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7410 - accuracy: 0.8221\n",
            "Epoch 415/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7885 - accuracy: 0.8138\n",
            "Epoch 416/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7821 - accuracy: 0.8089\n",
            "Epoch 417/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7993 - accuracy: 0.8040\n",
            "Epoch 418/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7797 - accuracy: 0.8077\n",
            "Epoch 419/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7667 - accuracy: 0.8138\n",
            "Epoch 420/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7510 - accuracy: 0.8155\n",
            "Epoch 421/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7301 - accuracy: 0.8175\n",
            "Epoch 422/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7146 - accuracy: 0.8221\n",
            "Epoch 423/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7065 - accuracy: 0.8229\n",
            "Epoch 424/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7330 - accuracy: 0.8211\n",
            "Epoch 425/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7221 - accuracy: 0.8231\n",
            "Epoch 426/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7134 - accuracy: 0.8222\n",
            "Epoch 427/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8112 - accuracy: 0.8090\n",
            "Epoch 428/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8250 - accuracy: 0.7998\n",
            "Epoch 429/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8046 - accuracy: 0.8053\n",
            "Epoch 430/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7844 - accuracy: 0.8087\n",
            "Epoch 431/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7335 - accuracy: 0.8197\n",
            "Epoch 432/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7213 - accuracy: 0.8233\n",
            "Epoch 433/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7075 - accuracy: 0.8248\n",
            "Epoch 434/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7111 - accuracy: 0.8261\n",
            "Epoch 435/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7113 - accuracy: 0.8251\n",
            "Epoch 436/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7216 - accuracy: 0.8252\n",
            "Epoch 437/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7217 - accuracy: 0.8218\n",
            "Epoch 438/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7187 - accuracy: 0.8240\n",
            "Epoch 439/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7431 - accuracy: 0.8211\n",
            "Epoch 440/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7444 - accuracy: 0.8201\n",
            "Epoch 441/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7516 - accuracy: 0.8152\n",
            "Epoch 442/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7777 - accuracy: 0.8079\n",
            "Epoch 443/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7648 - accuracy: 0.8118\n",
            "Epoch 444/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7675 - accuracy: 0.8115\n",
            "Epoch 445/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7663 - accuracy: 0.8114\n",
            "Epoch 446/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7352 - accuracy: 0.8165\n",
            "Epoch 447/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7453 - accuracy: 0.8177\n",
            "Epoch 448/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7253 - accuracy: 0.8215\n",
            "Epoch 449/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7168 - accuracy: 0.8236\n",
            "Epoch 450/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7010 - accuracy: 0.8288\n",
            "Epoch 451/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7075 - accuracy: 0.8261\n",
            "Epoch 452/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6995 - accuracy: 0.8266\n",
            "Epoch 453/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7318 - accuracy: 0.8219\n",
            "Epoch 454/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7169 - accuracy: 0.8260\n",
            "Epoch 455/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7212 - accuracy: 0.8214\n",
            "Epoch 456/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7368 - accuracy: 0.8194\n",
            "Epoch 457/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7813 - accuracy: 0.8138\n",
            "Epoch 458/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7726 - accuracy: 0.8154\n",
            "Epoch 459/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7610 - accuracy: 0.8154\n",
            "Epoch 460/500\n",
            "377/377 [==============================] - 7s 19ms/step - loss: 0.7754 - accuracy: 0.8145\n",
            "Epoch 461/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7828 - accuracy: 0.8163\n",
            "Epoch 462/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7686 - accuracy: 0.8139\n",
            "Epoch 463/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7435 - accuracy: 0.8181\n",
            "Epoch 464/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7261 - accuracy: 0.8208\n",
            "Epoch 465/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8133 - accuracy: 0.8109\n",
            "Epoch 466/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7845 - accuracy: 0.8111\n",
            "Epoch 467/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7406 - accuracy: 0.8181\n",
            "Epoch 468/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7495 - accuracy: 0.8205\n",
            "Epoch 469/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7633 - accuracy: 0.8202\n",
            "Epoch 470/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7407 - accuracy: 0.8177\n",
            "Epoch 471/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7410 - accuracy: 0.8182\n",
            "Epoch 472/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7358 - accuracy: 0.8191\n",
            "Epoch 473/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7115 - accuracy: 0.8263\n",
            "Epoch 474/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7046 - accuracy: 0.8261\n",
            "Epoch 475/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7026 - accuracy: 0.8324\n",
            "Epoch 476/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6931 - accuracy: 0.8310\n",
            "Epoch 477/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6960 - accuracy: 0.8329\n",
            "Epoch 478/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6973 - accuracy: 0.8320\n",
            "Epoch 479/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7319 - accuracy: 0.8236\n",
            "Epoch 480/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7442 - accuracy: 0.8184\n",
            "Epoch 481/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.8829 - accuracy: 0.7972\n",
            "Epoch 482/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.8235 - accuracy: 0.7980\n",
            "Epoch 483/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7976 - accuracy: 0.8073\n",
            "Epoch 484/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7641 - accuracy: 0.8111\n",
            "Epoch 485/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7651 - accuracy: 0.8144\n",
            "Epoch 486/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7478 - accuracy: 0.8167\n",
            "Epoch 487/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7632 - accuracy: 0.8190\n",
            "Epoch 488/500\n",
            "377/377 [==============================] - 8s 20ms/step - loss: 0.7301 - accuracy: 0.8206\n",
            "Epoch 489/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7207 - accuracy: 0.8226\n",
            "Epoch 490/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7134 - accuracy: 0.8207\n",
            "Epoch 491/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7047 - accuracy: 0.8247\n",
            "Epoch 492/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7390 - accuracy: 0.8267\n",
            "Epoch 493/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7041 - accuracy: 0.8279\n",
            "Epoch 494/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.6972 - accuracy: 0.8303\n",
            "Epoch 495/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7114 - accuracy: 0.8270\n",
            "Epoch 496/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7195 - accuracy: 0.8246\n",
            "Epoch 497/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7865 - accuracy: 0.8156\n",
            "Epoch 498/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7683 - accuracy: 0.8148\n",
            "Epoch 499/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7417 - accuracy: 0.8203\n",
            "Epoch 500/500\n",
            "377/377 [==============================] - 7s 20ms/step - loss: 0.7396 - accuracy: 0.8218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxtZBDJj1lTn"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHa9WTAa1n8K"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jDhahzoQ1piq",
        "outputId": "d1483de8-3d1d-4443-8415-180370648ab7"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyX7d5lw0v9d",
        "outputId": "380fa0fc-eebf-4904-ae0e-65a457a6d567"
      },
      "source": [
        "seed_text = \"I've got a bad feeling about this\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predict_x = model.predict(token_list, verbose=0)\n",
        "  predicted=np.argmax(predict_x,axis=1)\n",
        "\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I've got a bad feeling about this cruel law agin there at night i so i gone at the bower oer gone alas like love sends short your eyes glisten eyes far eily stealing by a art damsel windows heaven rosie kilrain december talk of athy one jeremy lanigan sanctified by the five hunt five hunt hoping i town stuck from fishers gone gone alas i jig taxes murray forbid love wandered on now on sober shannon gone i glens with regard the village church stands near mairis died without bellows chanters and kilrush right law clare o gone distant blame pure crystal fountain gone your eyes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkRK_Kh3GcHu"
      },
      "source": [
        "위 작업은 데이터의 양이 클수록 좋은 성능을 낼 것이라고 기대할 수 있다.\n",
        "\n",
        "하지만 위와 같이 원핫 인코딩 방식으로는 데이터가 굉장히 커질 경우 메모리가 부족해지는 현상이 일어날 수 있어, 이를 방지할 수 있는 다른 방법을 찾는 것이 필요해보인다.\n",
        "\n",
        "그 방법을 다음 실습에서 진행해보도록 하겠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA6K_A3YHFVK"
      },
      "source": [
        "## Additional: Generating text using a character-based RNN\n",
        "[공식문서 링크](https://www.tensorflow.org/text/tutorials/text_generation)\n",
        "\n",
        "</br>\n",
        "\n",
        "여기서 우리는, character-based RNN을 이용해서 text를 생성하는 방법을 알아볼 것이다.\n",
        "\n",
        "데이터셋으로는, Andrej Karpathy의 [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)에서 사용된 셰익스피어의 글을 이용할 것이다.\n",
        "\n",
        "문자 시퀀스의 다음 '문자'를 예측하도록 모델을 학습시키는 것이 해당 실습의 메인 아이디어이다. 모델을 반복적으로 호출하면 더 긴 텍스트 시퀀스를 생성할 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eajoXZGIteA"
      },
      "source": [
        "다음은 해당 모델을 30epochs로 학습시킨 후 Q 문자를 넣어주어 생성한 예시문이다.\n",
        "\n",
        "    QUEENE:\n",
        "    I had thought thou hadst a Roman; for the oracle,\n",
        "    Thus by All bids the man against the word,\n",
        "    Which are so weak of care, by old care done;\n",
        "    Your children were in your holy love,\n",
        "    And the precipitation through the bleeding throne.\n",
        "\n",
        "    BISHOP OF ELY:\n",
        "    Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "    Yet now I was adopted heir\n",
        "    Of the world's lamentable day,\n",
        "    To watch the next way with his father with his face?\n",
        "\n",
        "    ESCALUS:\n",
        "    The cause why then we are all resolved more sons.\n",
        "\n",
        "    VOLUMNIA:\n",
        "    O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "    And love and pale as any will to that word.\n",
        "\n",
        "    QUEEN ELIZABETH:\n",
        "    But how long have I heard the soul for this world,\n",
        "    And show his hands of life be proved to stand.\n",
        "\n",
        "    PETRUCHIO:\n",
        "    I say he look'd on, if I must be content\n",
        "    To stay him from the fatal of our country's bliss.\n",
        "    His lordship pluck'd from this sentence then for prey,\n",
        "    And then let us twain, being the moon,\n",
        "    were she such a case as fills m\n",
        "\n",
        "물론 모델이 단어의 의미를 학습한 것은 아니기 때문에 대부분의 문장이 문법적으로는 올바르지 않다. 하지만 모델이 학습을 시작할 때, 모델은 해당 영단어를 발음하는 방법은 커녕, 만들어진 문자집합이 단어를 이루는지조차 알지 못하는 것을 고려할 때, 굉장히 완성도 있음을 확인할 수 있다.\n",
        "\n",
        "또한, 셰익스피어의 연극대본 형태의 글을 학습하였기에 각 블록은 대문자로 표시된 화자의 이름 뒤에 위치하고 있는 것을 확인할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz4NCh2TKBeI"
      },
      "source": [
        "### Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHMKxJyPHDlF"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgoIK7iqKFpE"
      },
      "source": [
        "### Prepare for the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKl17-JRKEpT",
        "outputId": "3f835283-7c6b-49e9-f764-c2de7a6a0c15"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QrDHmPsKOB_",
        "outputId": "debe3ed6-129f-4c18-a6b4-11ed630a9fb2"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkGaVbYyKTQ1",
        "outputId": "3a180c96-2046-41df-f18b-649ba756e3e0"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtDJkw43KUwS",
        "outputId": "22f13113-48bd-4f9c-b54b-be572b66cc16"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5aPBQvlKX-G"
      },
      "source": [
        "### Process the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoCnV_LwKWTd",
        "outputId": "f86fd791-7c92-4882-98e7-c4285edf3c8b"
      },
      "source": [
        "# Vectorize the text: String -> Numerical representation\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTFwZXBeKvGz"
      },
      "source": [
        "# The preprocessing.StringLookup layer can convert each character into a numeric ID\n",
        "ids_from_chars = preprocessing.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl3Idmx0K4-e",
        "outputId": "e1f28699-013c-4f88-e2e6-dbb90a24fc24"
      },
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f02STO5LFjf"
      },
      "source": [
        "# representation to human=readable strings\n",
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5261_WsLIee",
        "outputId": "73ad5ab2-da4e-4fd6-ed63-f56a86e57172"
      },
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjCVa-3lLU_u",
        "outputId": "068d90e7-203f-43a2-a4f8-087a746da0e6"
      },
      "source": [
        "# join the characters back into strings\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwxS4e55LZxE"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3TXYDNELb7m"
      },
      "source": [
        "### Prediction\n",
        "\n",
        "모델에 대한 입력은 문자 시퀀스가 되고, 모델은 다음에 올 문자를 예측하게 된다. 위에서와 마찬가지로 일종의 예측 문제라고 볼 수 있는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQUV_9VLLbLs",
        "outputId": "50b6abd1-759f-46a6-a5c2-19ff211d77ac"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9GDes6nUqxj"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIBHtssFUr7l",
        "outputId": "6560aa12-b7dc-455e-fdea-812460120710"
      },
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFb8e3o5UtHN"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2IDz9NzUz62"
      },
      "source": [
        "batch 메서드는 개별 문자를 특정 사이즈의 sequence로 바꾸는 작업을 수월하게 만들어준다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63crmql0Uvic",
        "outputId": "ddc027c1-d5e7-4b6e-a93c-4218a65ee8bf"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsUyUGUyU70X",
        "outputId": "e44bd50f-257a-47c2-cc3b-1c700440eae0"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYEwc1WqVB7c"
      },
      "source": [
        "모델 학습을 위해, 우리는 input과 label을 가지는 데이터셋을 필요로 한다. 여기서의 input과 label은 sequences이다.\n",
        "\n",
        "각 단계마다 input은 현재 문자이고 label은 다음 문자가 된다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2ZDg3ATU96V"
      },
      "source": [
        "# shifts input to align with label for each time step\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZaS__R0VQVP",
        "outputId": "2acd79a2-bc9b-4f98-d008-eca7307d336d"
      },
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHWVgXNXVRlQ"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TwJe2u2Vbov",
        "outputId": "718bc1b1-bd2b-47d8-dca5-80e4fb3f408c"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zHIGIbyVeBx"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "`tf.data`를 이용해서 text를 manageable한 sequence로 바꾸도록 하자.\n",
        "\n",
        "data를 모델에 feeding하기 전에, data를 shuffle하고 이를 batch에 pack하는 과정이 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBgqa56WVcpY",
        "outputId": "4a00eaa5-ac03-4d96-8d15-f363281eaeff"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDyYkrLsV9-b"
      },
      "source": [
        "### Build the Model\n",
        "\n",
        "해당 모델은 3개의 레이어로 이루어진다.\n",
        "\n",
        "- `tf.keras.layers.Embedding`: input layer. 각 character-ID를 `embedding_dim` 차원의 벡터로 매핑하는 trainable lookup table\n",
        "\n",
        "- `tf.keras.layers.GRU`: `units=rnn_units` 사이즈의 RNN의 일종 (대신 LSTM 사용 가능)\n",
        "\n",
        "- `tf.keras.layers.Dense`: output layer. `vocab_size`의 output 출력. character 당 하나의 logit을 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJEZYaNvV8ZM"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf8MXp9TXa2I"
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4ByQXscXjUe"
      },
      "source": [
        "~~모델을 class로 정의하는 방법 기억할 것~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HOIv6eOXeTL"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qeXcwfRXwUO"
      },
      "source": [
        "각 character는 모델에 의해 embedding되고, 해당 embedding을 input으로 timestep마다 GRU를 실행시킨다. 이것은 Dense layer에 apply되어 다음 문자의 log-likelihood를 예측하는 logit을 출력한다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/76294398/141275287-5460b3f1-f137-40eb-8d59-e909393cb38c.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgOpYOwmYeNf"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "여기서, loss function은 prediction의 마지막 차원을 거쳐 적용될 것이므로, `tf.keras.losses.sparse_categorical_crossentropy`가 loss function으로써 사용될 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C68Os1M5XgQ7",
        "outputId": "fc0c6048-d97c-4107-ec11-f016820c44b3"
      },
      "source": [
        "# check the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwBIkP4YYkhX",
        "outputId": "0f880bcf-caf5-4425-e255-427001b7900c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense_3 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGbrs1_GYnQG"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mz4TuS2ZHYS",
        "outputId": "acff3fd0-c87e-48e6-d324-963ee9ec1e30"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.1894884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNRZYWbGZI7W",
        "outputId": "178183dc-07da-4e08-d307-98d866eb8f03"
      },
      "source": [
        "tf.exp(mean_loss).numpy()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.98902"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVF8OcvFZJqt"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8htsJ9MZOxZ"
      },
      "source": [
        "#### Configure Checkpoints\n",
        "\n",
        "`tf.keras.callbacks.ModelCheckpoint`를 이용해 training 과정 중 checkpoint가 save되도록 하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9Z6DcgMZLCQ"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sry-awXOZb8Q",
        "outputId": "4b347514-b1d3-48ca-dd80-f381c7cc676a"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 27s 136ms/step - loss: 2.7273\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 25s 134ms/step - loss: 1.9907\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.7102\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.5478\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.4487\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.3807\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 25s 135ms/step - loss: 1.3276\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 25s 135ms/step - loss: 1.2824\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 25s 135ms/step - loss: 1.2412\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 25s 135ms/step - loss: 1.2007\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.1612\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.1192\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 1.0751\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 25s 135ms/step - loss: 1.0288\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 25s 135ms/step - loss: 0.9807\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 0.9293\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 0.8762\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 0.8240\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 0.7730\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 25s 136ms/step - loss: 0.7253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGwqxte-ZiOw"
      },
      "source": [
        "### Generate text\n",
        "\n",
        "해당 모델을 통해 text를 생성하는 가장 간단한 방법은 loop를 실행시키고 그 전반에 internal state가 전파되게 하는 것이다.\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/76294398/141276770-ba93cb7c-0aeb-4542-bf2e-0f099d990177.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP2KSiPgZgxb"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7c23kDDaA0k"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mauVGE-caDQN",
        "outputId": "df52ed9e-982d-47de-a053-f9124a1fd45f"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Nay, then I see him soon look'd fate, nothing\n",
            "should see me an imagine.\n",
            "\n",
            "Servant:\n",
            "He hearly yet unborn! vent-hold death.\n",
            "\n",
            "ABHORSON:\n",
            "Come on: warlike spenct, as the dost now, sweet Kate, one ground\n",
            "And bear thee treth, and I can none with her.\n",
            "Upon this son shall lie in peradain?\n",
            "Most mighty, you have left unquisadors,\n",
            "Nor though for Rome, may less thee,\n",
            "To move my business, and all this linely\n",
            "letter us here for Edward; and their souls--\n",
            "As you have early bough as approaches\n",
            "As if they who, any chance to sleep shows more,\n",
            "Such as she is not four thressed hit himself.\n",
            "\n",
            "YORK:\n",
            "Think you you think I can tell you alone? let's peru\n",
            "With this excellent blood for both, frightlongs, beseech\n",
            "you.\n",
            "\n",
            "CORIOLANUS:\n",
            "Let me give what thou queen, you know; for I\n",
            "will secret me to swear courtier.\n",
            "\n",
            "Second Lady:\n",
            "Blue, here comes the world's threats?\n",
            "\n",
            "TYBALT:\n",
            "Look I heart is seated, and after players\n",
            "All in reversion her consent,\n",
            "As Priuse of Careshul with a tormour of a woman\n",
            "Cass'd it will a serpther-'do  \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.8103272914886475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5whdAixSaEY9",
        "outputId": "cce0e82a-9a34-4385-ada8-03de36820846"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThat is that thinks you have in dried; for they\\nnot pick'd in blood out at mine honest house.\\n\\nESCALUS:\\nI am the lark, that know me now:\\nGo thou to Full ase you lore's toad.\\nThis cannot guls fortune both to claim?\\nI saw, tempt with me in her issiee,\\nThe matter out o' dost thou wast before 's.\\n\\nJULIET:\\nIt told me not, whick, thou repliest! a mighty power,\\nEre we let no other heir to answer.\\n\\nPETRUCHIO:\\nWhere bite an uneapant give?\\n\\nJULIET:\\nNature till now Elbow her that did never lead each carp,\\nThat getted us in pomp, distrain'd here,\\nOur strength as white until death waste griefs,\\nLike wom, and surge the church to the roof of a\\nsweeter's eye, from his own goodness ir he,\\nCass on the hardle old and not upon my hands,\\nNeing to be soon look on no way of beasts!\\n\\nRICHARD:\\nBreak of our mind, teach me how to the cast.\\n\\nEDWARD:\\nI am a peril to incert them to\\nHortensio to ask him to the world.\\n\\nVIRGILIA:\\nI think he were to tell him thy\\nabsence; and that I may slaught erroin:\\nImell prisoners \"\n",
            " b\"ROMEO:\\nTo lose his bonds to-day! Dignity as quick.\\n\\nQUEEN MARGARET:\\nWhen every griefs, the queen, did remember\\nThe dukest begin in print opposite,\\nAnd one be glory bounded at thy fire,\\nFour kindling herm to citizens to be;\\nUnquick was an ergly through her friends,\\nAnd beat him safes and presently. Beseech your highness,\\nSuch as the glandess dimpate had left upon,\\nCorry unless that dip deserves doe, are, come,\\nWas never gentle law aside him as his duke,\\nWhich was too cured to take in off his head.\\nMy chokes young Norfolk, far wold me not.\\n\\nHASTINGS:\\nO royal time of execute, the kiss of the\\ncushering: this earth should guide thee, for you.\\n\\nLord Mayor:\\nHow now, sir, have you rather?\\n\\nSecond Senator:\\nFarewell.\\n\\nVALERIA:\\nBy my hand, and cannot speak like a careless.\\n\\nQUEEN ELIZABETH:\\nThanks, gentle Paris, though some prince of day;\\nFor one beign great plain'd alike as from\\nTheir fings: and in the hollow groath is down,\\nOr by alive be hull'd a fresh army.\\n\\nCATESBY:\\nResaugher.\\nThis do are slain, w\"\n",
            " b\"ROMEO:\\nNow, sir, is your brack friends, I'll knock your highness\\nAnd charged them woeing that which they kill with legs,\\nAnd scorn upon us.\\n\\nMARCIUS:\\nCome, come, we Curset it be so, out with\\na glarse he pains him: 'brow either, for thou wast born.\\n\\nAUTOLYCUS:\\nWhat, unhappy thou! A father, Bapures,\\nYet Abode the king, of Care, we have ta'en\\nAt else have manyment his stones,\\nWhich, to the regal titin of humours\\nGives so the enemies of our sword,\\nNow comes the master of our aid; and the extreme-day\\ndiscerse hath catched in thee as the liarable;\\nTo lock strice and bloody as the eastern plunts:\\n'Tis since he did bear the breath of greatest thing:\\nAs for a clamour of it, and come to Mantua\\nMyself thy harido she will wear by the last,\\nBut did you rather patient. New strattle not,\\nTherefore, good Camillo, but he was not the charpels,\\nScratch'd, as to you by meaning, down king, the earth,\\nOr how it here, this sharp-wern strong sunstine show\\nnothing: the people did accompe wound\\nIn John of the wildest\"\n",
            " b\"ROMEO:\\nThat Marcius shall be yours, and here I think\\nI'll not to have her horrors like a knet comfort,\\nAnd set myself any judgment Kate,\\nFor our affections which he substined yours.\\nNow, with standing every knot,\\nSo many lies all unto the house these\\nbehalfs; yet your honours say as life\\nStaziolds, and devils the eye may less\\nThan thou hast sleep, her brokery corceive\\nHis powerful dusicy and to Rosaline:\\nLet me pluck my business with the hungry pant\\nWhiles it were the continue. Lord came,\\nLet them home:\\nAnd, in the bloody mind of England's pare Iny--torthy though,\\nThat now peaceeding in the field of all,\\nTo bloody Juliet in the city of '\\nThird, I'll give my captive army, and with\\nHisterning.\\n\\nPOLIXENES:\\nThe gods be sorrows benn thy brother body\\nToo fair and tild George's years alone!\\n\\nBAPTISTA:\\nAn't arm'd\\nWith all at home! command these times of slespass\\nThat Any long hour.\\n\\nANTONIO:\\nI wish you, sir, he must apparent. I'll tell thee thine,\\nShould grieve thee first with me to be wish.\\n\\nAUTOLY\"\n",
            " b\"ROMEO:\\nNot so fismin; alack, alack, which bore mocuery in\\nthis face,\\nAnd slew the Takings of his sister?\\nBut if she can scratch a little tame.\\n\\nQUEEN ELIZABETH:\\nO ho, yet she chance to have the lips made me\\nTo have left unap idle, that line heart\\nTo foes my husband on horsed 'What I am.\\n\\nQUEEN ELIZABETH:\\nMadam, I have brought you do.\\nI am dead and that taught it becomes\\nAnd take it, and reports sharps of true.\\n\\nISABELLA:\\nYou have not been in every town, or more removes;\\nBut for the controversy of his good will,\\nWould brave their flowers or breath, makes thee to the sun\\nAnd destance contemplation.\\n\\nMISTRESS OVERDONE:\\nWhich was the ordering her which he is?\\n\\nQUEEN MARCAUUMERLE:\\nMadam, I'll prove with her.\\n\\nTRANIO:\\nNot I, behind the king's, lords?\\n\\nLADY GREY:\\nWhere is my lady Bianca move\\nTo save his grief?\\nShe, Coriolanus! Mireshood I power the kind\\nNature divines o'er his thunder on the hardly:\\nMinesay's strength morning.\\n\\nCAPULET:\\nWhile she hongstly and will nurse with you,\\nSo to rest with Go\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.1571972370147705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PY5damFaKO5"
      },
      "source": [
        "### Export the Generator\n",
        "\n",
        "이 single-step model은 `tf.saved_model`을 통해 손쉽게 save되고 restore될 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWoEzEr1aJXn",
        "outputId": "13ab30b0-aca3-478f-e7c2-6215234b8118"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f6c64f9a490>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiGdxYpMaZof",
        "outputId": "0a78023e-a644-4855-bbd0-2e629c2ad8a2"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "This widow'st two lady stroke and loves to hear\n",
            "Chatering arguments of pity, you have\n",
            "must sea nor \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBYWyG0uadx5"
      },
      "source": [
        "### Advanced: Customized Training\n",
        "\n",
        "위의 훈련과정은 간단하지만, 높은 자율성을 제공하지는 않는다. \n",
        "\n",
        "잘못된 예측이 모델에 피드백되는 것을 방지하는 teacher-forcing 기능을 사용하기 때문에 모델은 실수로부터 복구하는 법을 배우지 못한다.\n",
        "\n",
        "이제 모델을 수동으로 실행하는 방법에 대해 알아보았으니 training loop를 구현할 것이다. \n",
        "\n",
        "Customized training loop의 가장 중요한 부분은 train 부분 함수입니다.\n",
        "\n",
        "`tf.GradientTape`를 사용하여 그라디언트를 추적한다. \n",
        "\n",
        "기본 절차는 다음과 같다.\n",
        "\n",
        "1. 모델을 실행하고 `tf.GradientTape`를 이용해 loss를 계산\n",
        "2. 업데이트를 계산하고 Optimizer를 사용하여 모델에 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZX_tUf0aatO"
      },
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aahq1xI9bW8A"
      },
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk_u3iOzbYHG"
      },
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4cwbrP-bZJI",
        "outputId": "a418089c-8f8c-4689-f4af-e762b3b7521f"
      },
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 27s 136ms/step - loss: 2.7414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6cd0274890>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJVNKiOKbaRl",
        "outputId": "0b855143-bbe0-4110-ea9e-0a5725f88498"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1506\n",
            "Epoch 1 Batch 50 Loss 2.0763\n",
            "Epoch 1 Batch 100 Loss 1.9530\n",
            "Epoch 1 Batch 150 Loss 1.8628\n",
            "\n",
            "Epoch 1 Loss: 2.0064\n",
            "Time taken for 1 epoch 25.39 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8157\n",
            "Epoch 2 Batch 50 Loss 1.7655\n",
            "Epoch 2 Batch 100 Loss 1.7070\n",
            "Epoch 2 Batch 150 Loss 1.6554\n",
            "\n",
            "Epoch 2 Loss: 1.7280\n",
            "Time taken for 1 epoch 24.43 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6172\n",
            "Epoch 3 Batch 50 Loss 1.5649\n",
            "Epoch 3 Batch 100 Loss 1.5220\n",
            "Epoch 3 Batch 150 Loss 1.5328\n",
            "\n",
            "Epoch 3 Loss: 1.5651\n",
            "Time taken for 1 epoch 24.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4930\n",
            "Epoch 4 Batch 50 Loss 1.4155\n",
            "Epoch 4 Batch 100 Loss 1.4574\n",
            "Epoch 4 Batch 150 Loss 1.4524\n",
            "\n",
            "Epoch 4 Loss: 1.4627\n",
            "Time taken for 1 epoch 24.42 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3875\n",
            "Epoch 5 Batch 50 Loss 1.3706\n",
            "Epoch 5 Batch 100 Loss 1.3670\n",
            "Epoch 5 Batch 150 Loss 1.3710\n",
            "\n",
            "Epoch 5 Loss: 1.3935\n",
            "Time taken for 1 epoch 24.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3520\n",
            "Epoch 6 Batch 50 Loss 1.3948\n",
            "Epoch 6 Batch 100 Loss 1.3732\n",
            "Epoch 6 Batch 150 Loss 1.3269\n",
            "\n",
            "Epoch 6 Loss: 1.3395\n",
            "Time taken for 1 epoch 24.53 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2801\n",
            "Epoch 7 Batch 50 Loss 1.3354\n",
            "Epoch 7 Batch 100 Loss 1.2668\n",
            "Epoch 7 Batch 150 Loss 1.2882\n",
            "\n",
            "Epoch 7 Loss: 1.2948\n",
            "Time taken for 1 epoch 24.49 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2257\n",
            "Epoch 8 Batch 50 Loss 1.2924\n",
            "Epoch 8 Batch 100 Loss 1.2398\n",
            "Epoch 8 Batch 150 Loss 1.2459\n",
            "\n",
            "Epoch 8 Loss: 1.2548\n",
            "Time taken for 1 epoch 24.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1857\n",
            "Epoch 9 Batch 50 Loss 1.2140\n",
            "Epoch 9 Batch 100 Loss 1.1975\n",
            "Epoch 9 Batch 150 Loss 1.2541\n",
            "\n",
            "Epoch 9 Loss: 1.2171\n",
            "Time taken for 1 epoch 24.53 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1891\n",
            "Epoch 10 Batch 50 Loss 1.1500\n",
            "Epoch 10 Batch 100 Loss 1.1486\n",
            "Epoch 10 Batch 150 Loss 1.2030\n",
            "\n",
            "Epoch 10 Loss: 1.1770\n",
            "Time taken for 1 epoch 24.62 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}