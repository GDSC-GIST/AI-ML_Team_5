{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9691591",
   "metadata": {},
   "source": [
    "Natural Language Processing in Tensorflow\n",
    "---------------------------------------------------------------\n",
    "1. **Sentiment in text**\n",
    "1. Word Embeddings\n",
    "1. Sequence model\n",
    "1. Sequence models and literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.5.0\n",
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b110e",
   "metadata": {},
   "source": [
    "- 아래 예제에 필요한 tensorflow와 gdown 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb9d36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1540e0",
   "metadata": {},
   "source": [
    "First Example: **Tokenizer**를 이용한 word-encoding\n",
    "\n",
    "- sentences: 위 예제에서 사용할 문장 배열\n",
    "- tokenizer: Tokenizer의 instance, 여기서 num_word는 저장할 token의 개수로 사용해야 할 token이 (num_word = 100)보다 많다면 빈도수를 기준으로 상위 100개의 단어를 인코딩 하겠다는 의미이다. (많은 양의 데이터를 처리하는데 유용함.)\n",
    "- fit_on_texts(): input의 데이터를 가져와 encoding. \n",
    "- word_index: encoding한 data를 Dictionary 형태로 return. 대소문자를 구분하지 않고, 문장부호를 제거함으로써 단어에 대한 의미 보존.\n",
    "\n",
    "Result: sentence에 있는 단어를 indexing한 결과를 확인가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda62ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "Padded Sequences:\n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n",
      "(4, 5)\n",
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence: \n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \" , word_index)\n",
    "print(\"\\nSequences = \" , sequences)\n",
    "print(\"\\nPadded Sequences:\")\n",
    "print(padded)\n",
    "print(padded.shape)\n",
    "\n",
    "# Try with words that the tokenizer wasn't fit to\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen=10)\n",
    "print(\"\\nPadded Test Sequence: \")\n",
    "print(padded)\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7086d0",
   "metadata": {},
   "source": [
    "Second Example: Text to Sequence & **Padding**\n",
    "\n",
    "- sentences: 위 예제에서 사용할 문장 배열\n",
    "\n",
    "- oov_token: 편의상 out of vocab의 약자로 encoding할 data에서 encoding하지 못한 data를 대체한다. 이로써 후에 text_to_sequence 메소드를 호출할 때 encoding하지 못한 데이터를 대신하여 sequence를 만들 수 있도록 도우는 역할을 한다.\n",
    "    * 실제로 word_index 결과를 보면 OOV가 1번으로 encoding된 결과를 확인할 수 있다.\n",
    "    * Test Sequence에서 'i really love my dog'를 [5, 1(OOV), 3, 2, 4]로 변환된 모습을 확인가능\n",
    "    \n",
    "- sequence: tokenzier의 **texts_to_sequence** 메소드를 이용해 sentence를 token의 list로 return한다. 예컨대, 'I love my dog'과 같은 문장에는 각 단어에 대응하는 token을 이용해서 [5, 3, 2, 4]가 return된다.\n",
    "\n",
    "- padded: pad_sequence 메소드를 이용하여 sequence data를 max_len에 맞추어 padding\n",
    "    * ML을 위해 input data의 size를 통일시켜주는 수단\n",
    "    * 예제 결과에서 max_len인 길이 5에 맞추기 위해서 첫 번째 sentence가 [0 5 3 2 4]로 변환한 모습을 확인 가능\n",
    "    * max_len이 sentence 길이보다 짧으면 앞에서부터 데이터가 소실됨\n",
    "        + 예컨대, max_len이 3이면 첫 번째 sentence가 [3 2 4]로 출력됨\n",
    "    * parameter를 사용해 0이 추가되는 방향과 데이터가 소실되는 방향을 정할 수 있음(default 값은 pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9f0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n",
      "To: C:\\Users\\sykan\\sarcasm.json\n",
      "\n",
      "  0%|          | 0.00/5.64M [00:00<?, ?B/s]\n",
      "  9%|9         | 524k/5.64M [00:00<00:01, 3.33MB/s]\n",
      " 37%|###7      | 2.10M/5.64M [00:00<00:00, 8.83MB/s]\n",
      "100%|##########| 5.64M/5.64M [00:00<00:00, 8.30MB/s]\n",
      "100%|##########| 5.64M/5.64M [00:00<00:00, 8.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29657\n",
      "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "(26709, 40)\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n",
    "  \n",
    "import json\n",
    "\n",
    "with open(\"./sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "\n",
    "sentences = [] \n",
    "labels = []\n",
    "urls = []\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])\n",
    "    urls.append(item['article_link'])\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post')\n",
    "print(padded[0])\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39228c45",
   "metadata": {},
   "source": [
    "Third Example: **Sarcasm Example**\n",
    "\n",
    "- 실제 데이터에서 위에서 배운 방법을 적용시키는 것을 보여주는 예제\n",
    "\n",
    "- sarcasm.json data는 headline, is_sarcastic, article_link의 data를 가진 객체로 이를 각각 sentences, labels, urls에 append 해준다.\n",
    "\n",
    "- Tokenizer를 이용해 data의 sentences를 indexing하고, padding한다.\n",
    "\n",
    "- pad_sequences(sequences, padding='post'): padding parameter를 'post'로 설정함으로써 뒤에서부터 0이 추가되도록 padding하겠다는 의미\n",
    "\n",
    "- print(padded[0]): 첫 번째 sentence의 sequencing 결과를 출력\n",
    "\n",
    "- padded.shape의 결과인 (26709, 40)은 data에는 26709개의 문장과, 최대 40개의 단어로 이루어진 문장이 있다는 것을 의미함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
