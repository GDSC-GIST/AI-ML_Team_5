{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b071ad1",
   "metadata": {},
   "source": [
    "Natural Language Processing in Tensorflow\n",
    "---------------------------------------------------------------\n",
    "1. Sentiment in text\n",
    "1. Word Embeddings\n",
    "1. Sequence model\n",
    "1. **Sequence models and literature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd2555",
   "metadata": {},
   "source": [
    "Week4 Quiz (받은 성적 100%)\n",
    "-----------------\n",
    "\n",
    "#### 1. What is the name of the method used to tokenize a list of sentences?\n",
    "- **fit_on_tests(sentences)**\n",
    "- tokenize(sentences)\n",
    "- fit_to_test(sentences)\n",
    "- tokenize_on_text(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41111efa",
   "metadata": {},
   "source": [
    "#### 2. If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, what’s the output shape?\n",
    "- (None, 120, 124)\n",
    "- (None, 116, 124)\n",
    "- (None, 120, 128)\n",
    "- **(None, 116, 128)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a96789",
   "metadata": {},
   "source": [
    "#### 3. What is the purpose of the embedding dimension?\n",
    "- **It is the number of dimensions for the vector representing the word encoding**\n",
    "- It is the number of dimensions required to encode every word in the corpus\n",
    "- It is the number of words to encode in the embedding\n",
    "- It is the number of letters in the word, denoting the size of the encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1c6e06",
   "metadata": {},
   "source": [
    "#### 4. IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?\n",
    "- **Binary crossentropy**\n",
    "- Categorical crossentropy\n",
    "- Binary Gradient descent\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a0643",
   "metadata": {},
   "source": [
    "#### 5. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?\n",
    "- Make sure that they are all the same length using the pad_sequences method of the tokenizer\n",
    "- **Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace**\n",
    "- Process them on the input layer of the Neural Network using the pad_sequences property\n",
    "- Specify the input layer of the Neural Network to expect different sizes with dynamic_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12f438",
   "metadata": {},
   "source": [
    "#### 6. When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?\n",
    "- **Because the probability that each word matches an existing phrase goes down the more words you create**\n",
    "- Because the probability of prediction compounds, and thus increases overall\n",
    "- Because you are more likely to hit words not in the training set\n",
    "- It doesn't, the likelihood of gibberish doesn't change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf36868",
   "metadata": {},
   "source": [
    "#### 7. What is a major drawback of word-based training for text generation instead of character-based generation?\n",
    "- There is no major drawback, it's always better to do word-based training\n",
    "- **Because there are far more words in a typical corpus than characters, it is much more memory intensive**\n",
    "- Character based generation is more accurate because there are less characters to predict\n",
    "- Word based generation is more accurate because there is a larger body of words to draw from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431159d",
   "metadata": {},
   "source": [
    "#### 8. How does an LSTM help understand meaning when words that qualify each other aren’t necessarily beside each other in a sentence?\n",
    "- They shuffle the words randomly\n",
    "- They load all words into a cell state\n",
    "- They don't\n",
    "- **Values from earlier words can be carried to later ones via a cell state**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
