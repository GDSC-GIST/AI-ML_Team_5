{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Week_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRNqCD-SHLwh"
      },
      "source": [
        "# Text Preprocessing\n",
        "- **Tokenizing**\n",
        "- Cleaning\n",
        "- Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXKRS70O4zYT"
      },
      "source": [
        "!pip install tensorflow==2.5.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1O42ZZ-5eVf",
        "outputId": "179baf67-ef64-4a96-d009-f1af138fb2ba"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkvvZue66PUD"
      },
      "source": [
        "### Tokeninzer\n",
        "> tokenizing: 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의함. \n",
        ">\n",
        "generating the dictionary of word encodings and creating vectors out of the sentences.. \\\n",
        "=> 단어를 토큰화하고 숫자에 대응, 딕셔너리 생성 \\\n",
        "- `num_words`: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n",
        "- `fit_on_texts`: encodes (숫자 부여)\n",
        "- `word_index`: returns dictionary with word&index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux9zItHi_uTq",
        "outputId": "affd4a66-c4a7-4650-8f0e-434e58e0b322"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 0  5  3  2  4]\n",
            " [ 0  5  3  2  7]\n",
            " [ 0  6  3  2  4]\n",
            " [ 9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRFFfTZyF0KZ"
      },
      "source": [
        "- `oov_token`: replace out-of-vocabulary words during text_to_sequence calls, 즉 test data에서 encoding 안 된 단어 대체하는 용도\n",
        "- `text_to_sequences`: transforms a string of text into a list of words\n",
        "### Sequence\n",
        "- `sequence`: 정수의 시퀀스로 변환된 텍스트 문장\n",
        "- `pad_sequences`: with 0, pads sequences to the same length.\n",
        "> '패딩'한다: '고르다', 즉 일종의 통일성을 주는 것? 위에서 `pad_sequences`를 거쳐 나오는 리스트는 길이가 모두 같아짐\n",
        ">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHBERM4vFqEU",
        "outputId": "c2d006f1-d1e6-4f51-cef6-9ce96a37eae8"
      },
      "source": [
        "test_data = [\n",
        "    'i really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print(\"\\nPadded Test Sequence: \")\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            "[[0 0 0 0 0 5 1 3 2 4]\n",
            " [0 0 0 0 0 2 4 1 2 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7KMcqSoLh2P",
        "outputId": "68b03327-c3e7-41fd-c248-f653ee907f6c"
      },
      "source": [
        "!gdown --id 1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xRU3xY5-tkiPGvlz5xBJ18_pHWSRzI4v\n",
            "To: /content/sarcasm.json\n",
            "\r  0% 0.00/5.64M [00:00<?, ?B/s]\r100% 5.64M/5.64M [00:00<00:00, 81.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7kgyGx-Lqa3"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"./sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "\n",
        "sentences = [] \n",
        "labels = []\n",
        "urls = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "    urls.append(item['article_link'])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON5-oS_0Lu_H"
      },
      "source": [
        "### `.json` (JavaScript Object Notation)\n",
        "- Javascript에서 객체를 만들 때 사용하는 표현식\n",
        "- 경량(Lightweight)의 DATA-교환 형식\n",
        "- { String key : String Value } 형태\n",
        "\n",
        "```\n",
        "{\n",
        "  \"firstName\": \"Kwon\",\n",
        "  \"lastName\": \"YoungJae\",\n",
        "  \"email\": \"kyoje11@gmail.com\"\n",
        "}\n",
        "```\n",
        "출처: https://nesoy.github.io/articles/2017-02/JSON \\\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8Z_mUJsL19y",
        "outputId": "c266f0a5-8c10-4af2-a007-636694184f47"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "#print(word_index)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, padding='post')\n",
        "print(padded[0])\n",
        "print(padded.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29657\n",
            "[  308 15115   679  3337  2298    48   382  2576 15116     6  2577  8434\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "(26709, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MKFlq6sN0iM"
      },
      "source": [
        "- `padding` :String, 'pre' or 'post' (optional, defaults to 'pre'): pad either before or after each sequence, i.e., 패딩할 때 0이 앞으로 올지 뒤로 올지 결정해주는 것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Rx9bSFO5LQ"
      },
      "source": [
        "# Week 1 Quiz \n",
        "총점 8점 \n",
        "1. What is the name of the object used to tokenize sentences?\n",
        "- **Tokenizer**\n",
        "- WordTokenizer\n",
        "- TextTokenizer\n",
        "- CharacterTokenizer\n",
        "\n",
        "2. What is the name of the method used to tokenize a list of sentences?\n",
        "- fit_to_text(sentences)\n",
        "- tokenize_on_text(sentences)\n",
        "- **fit_on_texts(sentences)**\n",
        "- tokenize(sentences)\n",
        "\n",
        "3. Once you have the corpus tokenized, what’s the method used to encode a list of sentences to use those tokens?\n",
        "- texts_to_tokens(sentences)\n",
        "- text_to_sequences(sentences)\n",
        "- **texts_to_sequences(sentences)**\n",
        "- text_to_tokens(sentences)\n",
        "\n",
        "4. When initializing the tokenizer, how to you specify a token to use for unknown words?\n",
        "- unknown_token=`<Token>`\n",
        "- **oov_token=`<Token>`**\n",
        "- unknown_word=`<Token>`\n",
        "- out_of_vocab=`<Token>`\n",
        "\n",
        "5. If you don’t use a token for out of vocabulary words, what happens at encoding?\n",
        "- The word isn’t encoded, and is replaced by a zero in the sequence\n",
        "- The word isn’t encoded, and the sequencing ends\n",
        "- The word is replaced by the most common token\n",
        "- **The word isn’t encoded, and is skipped in the sequence**\n",
        "\n",
        "6. If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?\n",
        "- **Make sure that they are all the same length using the pad_sequences method of the tokenizer**\n",
        "- Use the pad_sequences object from the tensorflow.keras.preprocessing.sequence namespace\n",
        "- Process them on the input layer of the Neural Netword using the pad_sequences property\n",
        "- Specify the input layer of the Neural Network to expect different sizes with dynamic_length\n",
        "\n",
        "7. If you have a number of sequences of different length, and call pad_sequences on them, what’s the default result?\n",
        "- **They’ll get padded to the length of the longest sequence by adding zeros to the beginning of shorter ones**\n",
        "- They’ll get cropped to the length of the shortest sequence\n",
        "- Nothing, they’ll remain unchanged\n",
        "- They’ll get padded to the length of the longest sequence by adding zeros to the end of shorter ones\n",
        "\n",
        "8. When padding sequences, if you want the padding to be at the end of the sequence, how do you do it?\n",
        "- Call the padding method of the pad_sequences object, passing it ‘after’\n",
        "- **Pass padding=’post’ to pad_sequences when initializing it**\n",
        "- Pass padding=’after’ to pad_sequences when initializing it\n",
        "- Call the padding method of the pad_sequences object, passing it ‘post’\n",
        "\n",
        "\n"
      ]
    }
  ]
}